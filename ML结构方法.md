## 模型文件保存

**模型保存与调用：**

**1.pickle与joblib：**

* 原理：通过函数保存对象在文件中，保存对象即指将传入类的**self**当前状态保存在文件中，以便于在另一个文件将该状态赋予另一个对象中

* 保存模型

  >   joblib.dump(object, "modelname.m")  or   pickle.dump(obj, file)

* 调用本地模型

  >  object = joblib.load("modelname.m")   or   pickle.load(file)    

​       tip: file means the file object and the way to open it

## 数据增强

[数据增强(Data Augmentation) - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/41679153)

做数据增强的前提是对现有数据的全面了解和对模型不足的初步考究

#### SMOTE算法

随机样本增强即是直接复制样本，这样更可能会造成过拟合，随机生成与不平衡样本近似但不同的样本这一方法，称为SMOTE算法

* 

## 损失函数

简单定义：已知的标准与自动拟合的标准相差的程度的定量表达，用于比较两个模型差距有多少

损失函数在机器学习中的每一种算法中都很重要，因为训练模型的过程就是优化损失函数的过程，在学习相关算法的过程中，对损失函数的理解也在不断加深

（从结论上讲，损失函数与代价函数是同一种东西，目标函数是一个与它们相关但是更广的概念，对于目标函数来说在有约束条件下的最小化就是损失函数）

一般来讲，损失函数具有以下性质

* 损失函数是算法参数θ的函数
* 最好是对参数θ可微分的函数

一般来说，损失函数越小，就代表模型对样本拟合的越好，但是，并不是损失函数越小越好

这个时候还有一个概念叫风险函数(risk function)。设为f(x)，风险函数是损失函数的期望。

f(x)关于训练集的平均损失就称为经验风险，降低经验风险是我们的目标之一。但是经验风险过低的话就会造成模型过拟合。过拟合的数学描述即是模型函数过于复杂了，于是，我们的另一个目标就是降低模型复杂程度，我们定义一个函数J(x)来描述它，称这个函数为结构风险。

两种风险函数组合，就成为了我们最终要优化的目标函数

根据优化函数不同，可将损失函数分为四种思路，分别是经验风险最小化、结构风险最小化、最大化似然估计、最大化后验概率

模型 ![[公式]](https://www.zhihu.com/equation?tex=f%28X%29) 关于训练数据集的平均损失称为经验风险。经验风险最小化（Empirical Risk Minimization，ERM）认为经验风险最小的模型是最优的模型，公式如下：
$$
\min _{f \in b} \frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)
$$
结构风险在经验风险上加上表示模型复杂度的正则化项或罚项。结构风险最小化（Structural Risk Minimization，SRM）是为了防止过拟合而提出来的， ![[公式]](https://www.zhihu.com/equation?tex=J%28f%29) 是模型的复杂度，公式如下：
$$
\min _{f \in b} \frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)+\lambda J(f)
$$
最大化似然估计（Maximum Likelihood Estimation，MLE）是一种**参数**估计方法，利用已知的样本分布，找到最有可能（即最大概率）**导致这种分布的参数值**；或者说什么样的参数才能使我们观测到目前这组数据的概率最大

最大后验估计（Maximum A Posteriori estimation, MAP）也是一种参数估计方法，与MLE类似。最大区别是，MAP在其中融入了要估计量的先验分布，故最大后验估计可以看做规则化的最大似然估计

两种关系：

* 当模型是**条件概率**分布、损失函数是**对数损失函数**时，经验风险最小化等价于最大似然估计
* 当模型是**条件概率**分布、损失函数是**对数损失函数**、模型复杂度有模型的**先验概率**表示时，结构风险最小化等价于最大后验概率估计

#### **简单损失函数**

**最小二乘法：**直觉上，只要能够表示两个模型的输出相差程度的大小就能够满足我们的要求，于是有
$$
loss=\sum_{0}^{all}|y_{i}-y|
$$
但是，绝对值并非任何情况下都可导，于是，可以用平方去替代它
$$
loss=\sum_{0}^{all}(y_{i}-y)^2
$$
但是最小二乘法在反向求梯度的时候非常麻烦，因此一般不采用最小二乘法作为损失函数

**极大似然估计：**

似然值：真实的情况已经发生，我们假设对应现实的有多种概率模型，通过这些概率模型算出的结果就叫似然值，有几个模型就有几个似然值。从数学层面可以看做是以下公式
$$
P(C_{1},C_{2},C_{3},...,C_{n}|\theta)=\prod_{1}^{n}P(C_{i}|\theta)
$$

对应在神经网络中，每一个神经网络都有独特的权重与偏重，可以看做是一种理想模型，它对于某种输入有相应似然值结果。从数学层面可以看做是以下公式。
$$
P(x_{1},x_{2},x_{3},...,x_{n}|W,b)=\prod_{1}^{n}P(x_{i}|W,b)
$$
极大似然估计：就是对某个特定W,b的神经网络的似然值求最大，通过极大似然值大小来表示模型与标准相差的程度大小，因此，肯定不能只看一个神经网络的输出，因此有
$$
\prod_{1}^{n}P(x_{i}|W,b)=\prod_{1}^{n}P(x_{i}|y_{i})
$$
伯努利分布：是一种**离散分布**，又称为 “0-1 分布” 或 “两点分布”。例如抛硬币的正面或反面，物品有缺陷或没缺陷，病人康复或未康复，此类满足「**只有两种可能，试验结果相互独立且对立**」的随机变量通常称为伯努利随机变量

x,y就分别是一种伯努利随机变量，概率分布方程为
$$
f(x)=p^x*(1-p)^{1-x}
$$
由此，我们表示出了神经网络模型似然值的可迭代式

$$
\prod_{1}^{n}P(x_{i}|y_{i})=\prod_{1}^{n}y_{i}^{x_{i}}*(1-y_{i})^{1-x_{i}}
$$
因为对比连乘，连加更加便于计算，而log运算具有单调不变性，因此我们对似然值做一次log运算
$$
log(\prod_{1}^{n}y_{i}^{x_{i}}*(1-y_{i})^{1-x_{i}})=\sum_{1}^{n}log(y_{i}^{x_{i}}*(1-y_{i})^{1-x_{i}})=\sum_{1}^{n}(x_{i}*log(y_{i})+(1-x_{i})*log(1-y_{i}))\\极大似然值=max(\sum_{1}^{n}(x_{i}*log(y_{i})+(1-x_{i})*log(1-y_{i})))
$$
至此，我们将似然值这个概率概念设计成可以用于表示模型相差程度的函数，即把它变成为损失函数了

**交叉熵损失：**

先将模型换成熵，再用

信息量：我们设定一个函数，可以用信息量这个抽象概念来定量，即
$$
f(x):=信息量
$$
接下来的衍生概念都要使得这个定义能够在体系中自洽

接下来通过现实例子来尝试能不能推出f(x)的自洽的数学表达式，抽象的讲，对于两件能够连续发生的事情，（信息量的概念只能衡量这种例子吗？）例如小明在桂电踢球，踢到决赛的概率是a，踢到冠军的概率是b，而直接踢到冠军的概率是a*b，因为两件事的信息量是一样的，那么需要有
$$
f(a*b)=f(a)+f(b)
$$
这是一个非常明显的log运算，即
$$
f(x):=log_{n}x
$$
明显的，x这个概率越低，信息量越大。而n是一种量纲，事件的信息量数值大小随n的变化而变化，但是实际含义不变，故我们把n定义为2，方便思考，即实际上：
$$
f(x)=-log_{2}x
$$
熵：衡量一个概率模型（系统）的不确定程度（直接看出结果的可能性），越大概率的事件对系统不确定程度的贡献越小，由此，可以把熵定义为
$$
H_{熵}(P):=E_{期望}(P_{f})=\sum_{i=1}^{m} p_{i} \cdot f\left(p_{i}\right)=\sum_{i=1}^{m} p_{i}\left(-\log _{2} p_{i}\right)=-\sum_{i=1}^{m} p_{i} \cdot \log _{2} p_{i}
\
$$
至此，我们把熵这个概念量化了。而我们的目标是将概率模型和人脑内模型做比较，是否可以直接比较两个模型的熵呢。答案是不行的，因为人脑模型熵无法计算。因此，我们需要一种能够不直接求熵而能够定量比较两个模型的熵差距的方法

KL散度：Kullback-Leibler Divergence，即 K-L散度 ，是一种量化两种概率分布P和Q之间差异的方式，又叫**相对熵** KL散度能帮助我们度量使用一个分布来近似另一个分布时所损失的信息量，具体定义是：
$$
\begin{array}{l}
\boldsymbol{D}_{K L}(\boldsymbol{P} \| \boldsymbol{Q}) \\
:=\sum_{i=1}^{m} p_{i} \cdot\left(f_{Q}\left(\boldsymbol{q}_{i}\right)-f_{p}\left(p_{i}\right)\right) \\
=\sum_{i=1}^{m}{p}_{i} \cdot\left(\left(-\log _{2} \boldsymbol{q}_{i}\right)-\left(-\log _{2} p_{i}\right)\right) \\
=\sum_{i=1}^{m}{p}_{i} \cdot\left(-\log _{2} \boldsymbol{q}_{i}\right)-\sum_{i=1}^{m} p_{i} \cdot\left(-\log _{2} p_{i}\right)
\end{array}
$$
这一段定义的文字描述是，以P为基准，P的信息量是确定但未知的，求Q与P之间所有事件的熵的差值的在P系统内的期望，这里比较绕，但是KL散度就是这个概念，由此可以得到Q去拟合P的话会损失多少信息量。而通过吉布斯不等式我们可以得知，KL散度一直是大于0的

交叉熵：从上面我们可以看出，交叉熵就是KL散度左边的部分。而将这个数学中交叉熵的定义应用于概率模型中需要一些变换，首先，m是在P模型中所有事件的总数，Pi可以直接用输入的训练集标签，值为0、1。而Qi需要考虑一下，因为Qi的值是一个浮点数值，代表对输入的置信度的大小，它与训练集标签是不对应的，因此我们需要把Qi进行展开，分为训练集标签是1和训练集标签是0的情况，具体公式为：
$$
\begin{array}{l}
\boldsymbol{H}(\boldsymbol{P}, \boldsymbol{Q}) \\
=\sum_{i=1}^{m} p_{i} \cdot\left(-\log _{2} q_{i}\right) \\
=\sum_{i=1}^{n} x_{i} \cdot\left(-\log _{2} q_{i}\right) \\
=-\sum_{i=1}^{n}\left(x_{i} \cdot \log _{2} y_{i}+\left(1-x_{i}\right) \cdot \log _{2}\left(1-y_{i}\right)\right)
\end{array}
$$
至此，H成为了可以定量描述P，Q两个模型的差异的表达式

（在这里，交叉熵的函数形式与极大似然估计的函数形式非常像，但是极大似然估计的函数形式是通过数学直觉进行推导的，本身没有定义支撑，而交叉熵损失函数是通过定义一步步推导得到，具有量纲，更加严谨）

## 模型方法

#### 交叉验证

[N折交叉验证的作用（如何使用交叉验证） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/113623623)

(我个人感觉如果用交叉验证来选择模型是一种手工过拟合的过程，对提升模型的泛化能力有害)

交叉验证是在机器学习**建立模型**和**验证模型参数**时常用的办法。交叉验证，顾名思义，就是重复的使用数据，把得到的样本数据进行切分，组合为不同的训练集和测试集，用训练集来训练模型，用测试集来评估模型预测的好坏。

**简单交叉验证**

对于总数据集D，随机挑选固定比例的训练集和验证集，对一个或多个模型进行训练。得到挑选出效果更好的模型和相应的那组参数

**S折交叉验证**

和第一种方法不同，S折交叉验证会把样本数据随机的分成S份，每次随机的选择S-1份作为训练集，剩下的1份做测试集。当这一轮完成后，重新随机选择S-1份来训练数据。若干轮（小于S）之后，选择损失函数评估最优的模型和参数

#### 正则化

**内核理解**

模型在训练过程中，因为种种因素，难免会出现过拟合的情况。在以梯度和矩阵为核心的算法上，过拟合的本质即是：在训练的某一时间点上，模型的参数过于复杂，导致出现模型对训练集拟合很好，对测试集拟合程度则不work

那么在线性模型上，正则化的方法就能够起到很好的降低模型复杂度的作用，其原理即是，通过在优化函数上增加结构风险项（也叫惩罚项），减小参数的变化范围（据证明可得，线性模型中函数的复杂度大小和参数的变化范围大小具有相当大的关系），最后达到减小函数复杂度的效果

**L1、L2、L-R正则化**

关于L1，L2正则化的公式、对参数的作用原理、性质、相同与不同在博客中都能找到答案

[为什么L1和L2正则化可防止过拟合 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/85630046)（这篇不严谨，但是很直观）

[(3 封私信 / 81 条消息) l1 相比于 l2 为什么容易获得稀疏解？ - 知乎 (zhihu.com)](https://www.zhihu.com/question/37096933)

而L-R正则化即是在特征空间上W的限定范围不一致，原理是相同的，没有什么太大的实用性

#### 调参

**网格搜索**

#### 批规范化

[Batch Normalization详解 - shine-lee - 博客园 (cnblogs.com)](https://www.cnblogs.com/shine-lee/p/11989612.html)

[论文阅读笔记：看完也许能进一步了解Batch Normalization - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/340219662)

在神经网络某一层中，其前面的层可以看作是对输入的处理，后面的层可以看作是损失函数

但是由于反向传播，其输入的特征只是长度不变，分布却一直在变

分布变化，代表这一层的拟合线性模型需要重新拟合数据，一来一回的就像模型和分布的“追逐游戏”

对于浅层模型，比如SVM，输入特征的分布是固定的，即使拆分成不同的batch，每个batch的统计特性也是相近的，因此只需调整直线位置来适应输入分布

而深层模型，每层输入的分布和权重在同时变化，训练相对困难

多层的情况则是，在反向传播过程中，每一层都向损失函数降低的方向调整自己，导致层间配合“缺乏默契”

为了避免过于震荡，学习率不得不设置得足够小，足够小就意味着学习缓慢。

这个时候我们引入Batch Normalization，其操作就是将上一层输出的m(batch size)个输出都做standardization，再做scale and shift

这两个过程合在一起就是
$$
y_{i}^{(b)}=B N\left(x_{i}\right)^{(b)}=\gamma \cdot\left(\frac{x_{i}^{(b)}-\mu\left(x_{i}\right)}{\sqrt{\sigma\left(x_{i}\right)^{2}+\epsilon}}\right)+\beta
$$
其中

* μ和σ为当前行的统计量，不可学习。
* γ和β为待学习的scale和shift参数，用于控制yi的方差和均值。
* BN层中，xi和xj之间不存在信息交流(i≠j)

无论原本均值方差是多少，通过BN后其均值和方差分别变为待学习的β和γ

使用Batch Normalization，可以获得如下好处

* 可以使用更大的学习率
* 可以将bias置为0
* 对权重初始化不再敏感
* 对权重的尺度不再敏感
* 深层网络可以使用sigmoid和tanh了
* Batch Normalization具有某种正则化作用，减少过拟合

卷积层如何使用BatchNorm？没有scale and shift过程可不可以？BN层放在ReLU前面还是后面？BN层为什么有效？
