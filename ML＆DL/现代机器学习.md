# 集成学习-ensemble learning

**集成学习**（**ensemble learning**）通过构建并结合多个学习器来完成学习任务。如下图所示，其工作原理是先产生一组“个体学习器”，再用某种策略将它们结合起来得到最终输出

集成学习通过将多个学习器进行结合，常常可获得比单一学习器显著优越的泛化性能。这对“弱学习器”尤为明显，因此集成学习的很多理论研究都是针对弱学习器进行的。但是实践中处于种种考虑，例如希望使用较少的个体学习器，或是重用关于常见学习器的一些经验等，人们往往会使用比较强的学习器

关于集成学习效果为什么那么好，可以做个简单的分析，对于**二分类**问题，假如基分类器的错误率为ε，则有
$$
P(h_{i}(x)\neq{f(x)})=ε
$$

$$
H(x)=sign(\sum_{i=1}^{T}h_{i}(x))
$$

$$
P(H(x)\neq{f(x)})=\sum^{[T/2]}_{k=0}\tbinom{T}{k}(1-ε)^{k}ε^{T-k}\leq{exp(-\frac{1}{2}T(1-2ε)^{2})}
$$

可知，随着分类器数量T的增大，集成的错误率将知指数下降，最终趋向于0

如何产生并结合“好而不同”的个体学习器，是集成学习研究的核心

集成学习结合多个学习器的效果直观展示

![](http://1.14.100.228:8002/images/2022/01/11/ensemble-learning-example-1.png)

Bagging的训练过程旨在降低方差，而Boosting的训练过程旨在降低偏差，过程影响类（学习率、子模型数量等）的调整会对模型产生巨大影响

## Bagging

Bagging就是通过Bootstrap aggregating来生成众多不同的分类器的一种集成学习方法

Bootstrap：是一种有放回的统计学上的采样方法，因为该方法充分利用了给定的观测信息，不需要模型其他的假设和增加新的观测，并且具有稳健性和效率高的特点0通过重采样（re-sample）避免了Cross-Validation造成的样本减少问题，Bootstrap也可以用于创造数据的随机性，以此来训练同一类型模型的不同分类器。

#### Random Forest

总体上，就是生成大量不同的决策树，如果是分类树，就以voting来产生结果，如果是回归树，则以average来产生结果。（前置概念：决策树，bagging方法，voting、average combiner方法）

优点：1.本身采用bootstrap方法，不需要交叉验证2.精度很高，但是参数却很少3.训练特征是随机的，不需要做特征属性选择

**特征重要性**



## Boosting

Boosting是一族可以将弱学习器提升为强学习器的算法，这族算法的工作机制类似：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本进行调整，使得先前基小学期做错的训练样本在后续受到更多关注，然后基于调整后的样本分布来训练下一个基学习器；如此反复进行，直至学习器数目达到了事前指定的值T

#### Adaptive Boosting

[（十三）通俗易懂理解——Adaboost算法原理 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/41536315)

AdaBoost是英文"Adaptive Boosting"（自适应增强）的缩写，它的自适应在于：前一个基本分类器被错误分类的样本的权值会增大，而正确分类的样本的权值会减小，并再次用来训练下一个基本分类器。同时，在每一轮迭代中，加入一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数才确定最终的强分类器，其算法流程为

![](http://1.14.100.228:8002/images/2022/01/11/adaboost.png)

当分类任务为二分类，分类样本为D={-1，+1}时，样本分对与分错的权重更新方式可以进行化简

![](http://1.14.100.228:8002/images/2022/01/11/adaboosting.png)

Adaboost的优点

* Adaboost提供一种框架，在框架内可以使用各种方法构建子分类器。可以使用简单的弱分类器，不用对特征进行筛选，也**不存在过拟合的现象**
* Adaboost算法不需要弱分类器的先验知识，最后得到的强分类器的分类精度依赖于所有弱分类器。无论是应用于人造数据还是真实数据，Adaboost都能显著的提高学习精度
* Adaboost算法不需要预先知道弱分类器的错误率上限，且最后得到的强分类器的分类精度依赖于所有弱分类器的分类精度，可以深挖分类器的能力。Adaboost可以根据弱分类器的反馈，自适应地调整假定的错误率，执行的效率高
* Adaboost对同一个训练样本集训练不同的弱分类器，按照一定的方法把这些弱分类器集合起来，构造一个分类能力很强的强分类器，即“三个臭皮匠赛过一个诸葛亮”

而Adaboost的缺点也十分明显，在训练过程中，难以分类的样本的权重会指数上升，训练将会过于偏向这类困难的样本，导致Adaboost算法易受噪声干扰

假如说Adaboost的优点有一些是其它集成学习方法也具备的，那么有两个特性是Adaboost需要重点说明的

* 训练的错误率上界，随着迭代次数的增加，会逐渐下降
* Adaboost算法即使训练次数很多，也不会出现过拟合的问题

[(2条消息) Adaboost 算法的原理与推导_结构之法 算法之道-CSDN博客_adaboost算法](https://blog.csdn.net/v_july_v/article/details/40718799)

#### Gradient Boost Decision Tree

[GBDT算法原理以及实例理解 | 技术闲谈 (ranmaosong.github.io)](https://ranmaosong.github.io/2019/04/27/ML-GBDT/)

梯度提升树（Grandient Boosting）是提升树（Boosting Tree）的一种改进算法，上一个adaboost是在参数空间上的的加法模型

GBDT最早由Friedman提出，以CART回归树为基学习器，其核心是拟合前面迭代所留下来的残差，使其达到最小，将Regression Decision Tree 和 Gradient Boosting结合起来就是现在一般而言的GBDT算法

注意：这里的GBDT是指传统的梯度提升树实现，而实际上的GBDT泛指一切梯度提升树，包括XGB，lightgbm、Catboost等

**GBDT树的算法描述：**

<img src="http://1.14.100.228:8002/images/2022/01/11/f03a3ee954302a2bac2ed9585ab42d8f.png" style="zoom:50%;" />

**函数空间**

[从参数空间到函数空间理解GBDT+XGBoost - 简书 (jianshu.com)](https://www.jianshu.com/p/6075922a8bc3)

真正的函数空间的概念是泛函分析的基础，我不是专业学数学的，这里只简述一下函数空间在boosting算法中的表达。

一般的，boosting算法的实现模型都可以看作是各类加法模型。从GBDT开始，算法优化的主体从参数转化为了函数（树模型），也就是说，模型的描述空间由参数空间转为了函数空间。而在空间上用梯度下降法进行优化的是GBDT，用牛顿法进行优化的则是XGBoost。

引入函数空间这个概念后，就可以方便的使用损失函数的导数等概念并借助常规的优化算法来学习弱学习器



**残差**

这里残差就是当前轮次要生成的树的拟合对象，就是样本标签与之前轮次训练产生的强学习器的输出结果之差
$$
r=y-f_{t-1}(x)
$$
当损失函数是平方损失和指数损失时，提升书的每一步优化是很简单的，但是对于一般损失函数而言，往往每一步优化起来不那么容易，但是对于一般损失函数而言，往往每一步优化起来不那么容易，由此，梯度提升树提出了**负梯度**的概念作为残差的近似值

**一阶泰勒函数展开（梯度下降法）**

机器学习中需要最小化损失函数E(θ)，这个θ就是要求解的模型参数。梯度下降法常用于求解无约束最优化问题，是一种迭代方法。初始化θ为θ^0，，不断迭代来更新θ的值，进行损失函数的极小化

<img src="http://1.14.100.228:8002/images/2022/01/11/98247a724877478c48a2dbeeedb008dc.png" style="zoom:50%;" />

此时我们发现，当GBDT损失函数是平方损失时，**负梯度就是残差**

**GBDT算法详细流程**

![](http://1.14.100.228:8002/images/2022/01/11/GBDT.png)

#### XGBoost

XGBoost与GBDT的差别主要体现在两点上，一是算法对数据的处理以及损失函数的定义不同，二是工程实现上有具体方法

**二阶泰勒展开（牛顿法）**



#### LightGBM

#### CatBoost

## Others

**combiner**

Average

对于数值型输出，最常见的结合策略是平均法

* simple average
  $$
  H(x)=\frac{1}{T}\sum_{i=1}^{T}h_{i}(x)
  $$

* weighted average
  $$
  H(x)=\sum_{i=1}^{T}W_{i}h_{i}(x)
  $$

* 

事实上，加权平均法可认为是集成学习研究的基本出发点，不同的集成学习方法可视为通过不同的方式来确定加权平均法中的基学习器权重

现实任务中的训练样本通常不充分或存在噪声，这将使学出的权重不完全可靠。尤其是对规模比较大的集成来说，要学习的权重比较多，容易过拟合

Voting

对分类任务来说，学习器从类别标记集合{c1,c2,c3,...}中预测出一个标记，最常用的结合策略是投票法

* majority voting（硬投票）
  $$
  H(x)=c_{argmax}\sum_{i=1}^{T}h_{i}^{j}(x)
  $$

* weighted voting（软投票）
  $$
  H(x)=c_{argmax}\sum_{i=1}^{T}w_{i}h_{i}^{j}(x)
  $$

在现实任务中，不同类型的个体学习器可能产生不同类型的输出值

* 类标记：h(x)={0,1,....}
* 类概率：h(x)=[0,1]，相当于对后验概率的一个估计

不同类型的输出值需要在代码层面上对算法进行更改，对一些能在预测出类别标记的同时产生分类置信度的学习器，其分类置信度可转化为类概率来使用。若基学习器的类型不同，则其类概率不能直接进行比较；在这种情况下，通常可以把类概率输出转化为类标记输出

**Blending**

Blending是把原始的数据集先分成两个部分，比如0.7的训练集和0.3的测试集，再用0.7的训练集测试出的基学习器的0.3测试集输入结果当作blender（第二层）的输入继续训练

优点：Blending比Stacking简单，不用进行K折交叉验证，避开了一些信息泄露问题，因为generalizers和blender用的是不一样的数据集。

缺点：使用了很少的数据集，并且blender可能会过拟合

**Stacking**

（在实践中，stacking和blending的效果差不多）

Stacking是一种分层模型集成框架，在stacking算法中，我们把个体学习器叫做**初级学习器**，用于结合的学习器叫做**次级学习器**

![](http://1.14.100.228:8002/images/2022/01/11/stacking.png)

![](http://1.14.100.228:8002/images/2022/01/11/stacking_2.jpg)

如上图所示，我们将数据集分成均匀的5部分进行交叉训练，使用其中的4部分训练，之后将训练好的模型对剩下的1部分进行预测，同时预测测试集；经过5次cv之后，我们可以得到训练集每个样本的预测值，同时得到测试集的5个预测值，我们将测试集的5个测试集进行平均。有多少个基模型，我们会得到几组不同的预测值；最后使用一个模型对上一步得到预测结果再进行训练预测得到 stacking结果。stacking模型一般使用线性模型。

总得来说，就是将不同基学习器的预测结果结合给下一组学习器进行训练，stacking 有点像神经网络，基模型就像底层的神经网络对输入数据进行特征的提取

在做Stacking的过程中，如果将第一层模型的预测值和原始特征合并加入第二层模型的训练中，则可以使模型的效果更好，还可以防止模型的过拟合

**Bagging与Boosting的区别**

# 其它

**正态分布**

[为什么要假设变量为正态分布？ - 简书 (jianshu.com)](https://www.jianshu.com/p/8f559ad7c4cc)

（这种转化类似于我们去网上买美国的食品，它上面写着这个食品是多少美元/磅，但是我们不清楚到底这是贵还是便宜，我们就需要把计量单位转化为元/kg，转化后我们就能明白到底是什么价位）

概率分布：

中心极限定理：如果一个事物受多种因素的影响，不管每个因素本身是什么分布，它们加总后，结果的均值就是正态分布

正态分布的性质：

正态分布与机器学习算法的联系：

### 优化算法

大多数机器学习问题最终都会涉及一个最优化问题，有的基于最大化后验概率、有的基于最小化化类内距离、有的基于最优化损失函数。

而优化算法有很多种，如果按梯度的类型进行分类，可以分为有梯度优化算法和无梯度优化算法

有梯度优化算法主要有梯度下降法、动量法momentum、Adagrad、RMSProp、Adadelta、Adam等

无梯度优化算法也有很多，像粒子群优化算法、蚁群算法群体智能优化算法，也有贝叶斯优化、ES、SMAC这一类的黑盒优化算法

在机器学习任务中，有时候涉及的优化任务并不是只有优化最主要的模型，还有其它优化目标。例如寻找最优分类阈值，寻找最优分割样本块。或者是有多种模型，需要多种不同的优化思路。这时，无论哪种优化算法都有可能派上用场。

**梯度下降法、稀疏数据**

（typora出问题了，这里原本有挺多笔记的，关于稀疏数据理论的知识也在下面的视频里）

[“随机梯度下降、牛顿法、动量法、Nesterov、AdaGrad、RMSprop、Adam”，打包理解对梯度下降法的优化_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1r64y1s7fU?spm_id_from=333.851.dynamic.content.click)

