## 基础概念

### 通用函数

**卷积**

理解是如果一个系统输入是不稳定的，输出是不稳定的，那么我们就可以用卷积来求系统的存量，在图像中，卷积操作就是周围像素点对中间像素点的影响，在特征中，卷积操作就是提取适合的特征产生特征量来给神经网络来处理。



卷积公式
$$
\int_{-\infin}^{\infin}f(\alpha)g(x-\alpha)d\alpha
$$
相加积分公式
$$
r(x)=\int_{0}^{t}e(x)d(t-x)dx
$$

**logistic**函数
$$
\hat{y}=\sigma(w^Tx+b) \ where\ \sigma(z)=\frac{1}{1+e^{-z}}
$$

$$
\delta(\hat{y},y)=-(y*log\hat{y}+(1-y)log(1-\hat{y}))
$$

$$
J(w,b)=\frac{1}{m}\sum^m_{i=1}\delta(\hat{y},y)=\frac{1}{m}\sum^m_{i=1}(-(y*log\hat{y}+(1-y)log(1-\hat{y})))
$$

$$
w:=w-\alpha\frac{dJ(w)}{dw}
$$

$$
b:=b-\alpha\frac{dJ(w,b)}{db}
$$

**softmax函数**

softmax函数使变量按照大小分配概率，解决a>b的绝对概率问题。
$$
softmax(likelihood):s_i=\frac{e^{v_j}}{\sum_{0}^{j}e^{vj}}
$$

![](http://1.14.100.228:8002/images/2022/01/11/softmax_example.png)

**ReLU函数**

Sigmoid函数或者Tanh函数只有其输入指定范围的值才能获得良好的线性梯度，一旦输入变大，梯度就会降为0，故在神经网络的反向传播中多使用ReLU函数

在卷积神经网络中，ReLU函数为网络增加了非线性，为正输入提供非饱和梯度

Mish

![](http://1.14.100.228:8002/images/2022/01/11/mish.png)

相比于RELU，Mish的梯度更加平滑，更平滑的梯度使信息能够流入更深的网络，使用后准确率会提升，但是它比RELU更加笨重一点，会轻微增加运算量

### 权重(Weight)

#### 初始化

[权重/参数初始化 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/72374385)

神经网络中节点权值W不能初始化为0，否则没法训练，也不能单纯用随机数初始化

因为假如随机分布选择不当，就会导致网络优化陷入困境

假如把权重初始化为较小的值，如均值为0，方差为0.01的高斯分布

![](http://1.14.100.228:8002/images/2022/01/11/2fb6be82cdedc8b6075880d80af39b21.jpg)

则在不过10层的网络就梯度消失的只剩0了

但是如果把**权重初始成一个比较大的值**，如均值为0，方差为1的高斯分布

![](http://1.14.100.228:8002/images/2022/01/11/3ebfcba5da435c60a017c17a81215736.jpg)

则在不过10层的网络梯度爆炸的只剩1和-1

**Xavier初始化：**

Xavier初始化可以帮助减少梯度消失的问题，它的**基本思想**是保持输入和输出的方差一致（服从相同的分布），这样就避免了所有输出值都趋向于0，但是Xavier初始化只适用于tanh激活函数。[具体运算方法在链接中]

**MSRA：**

MSRA的思想是在ReLU网络中，假定每一层有一半的神经元被激活，另一半为0（x负半轴中是不激活的），所以要保持variance不变，只需要在Xavier的基础上再除以2，效果是比Xavier initialization好很多。现在神经网络中，隐藏层常使用ReLU，权重初始化常用He initialization这种方法。

**BN**：

BN虽然牛逼，但是在合适的初始化可以胜任任务时还是尽量减少运算量吧

当然，为了压根儿不去想权重初始化的问题，我们可以用**Pre-train模型**，这又牵涉到一个非常复杂的方面了，一个便于理解的例子是先使用greedy layerwise auto-encoder做unsupervised pre-training，然后再做fine-tuning。

pre-training阶段：将神经网络中的每一层取出，构造一个auto-encoder做训练，使得输入层和输出层保持一致。在这一过程中，参数得以更新，形成初始值。

fine-tuning阶段：将pre-train过的每一层放回神经网络，利用pre-train阶段得到的参数初始值和训练数据对模型进行整体调整。在这一过程中，参数进一步被更新，形成最终模型。

随着数据量的增加以及activation function 的发展，pre-training的概念已经渐渐发生变化。目前，从零开始训练神经网络时我们也很少采用auto-encoder进行pre-training，而是直奔主题做模型训练。**如果不想从零开始训练神经网络时，我们往往选择一个已经在任务A上训练好的模型（称为pre-trained model），将其放在任务B上做模型调整（称为fine-tuning）**。

**随机种子：**

真正的高质量随机数是由物理工具产生的，计算机中产生的随机数是由函数生成，具有一定规律的伪随机数。

随机数的生成需要一套初始值，这一套初始值就是随机种子

深度学习网络模型中**初始的权值参数**通常都是**初始化成随机数**

而使用梯度下降法最终得到的局部最优解**对于初始位置点的选择很敏感**

但是最优随机种子不应该去找，随机性的存在正好用来评估模型的鲁棒性。一个优秀的模型，不会因为随机初始的位置略微不同，而找不

到最优的位置。这是模型本身应该要化解的工作，而不是人为选择一个随机数。

#### 正则化 (regularization)

* 如果网络对数据过拟合，可以增加训练数据，或使用正则化方法

* 可直接作为损失函数使用

**L2正则化**
$$
J(w,b)=\frac{1}{m}\sum_{i=1}^{n}\sigma(\hat{y}^i,y^i)+\frac{\lambda}{2m}||w||^2_2\\||w||^2_2=\sum_i^{l-1}\sum_j^{l}w_{ij}^2=w^Tw
$$

L2正则化最主要的用处即是防止过拟合，由于二次函数的性质，ta具有非常良好的权重衰减功能，在使用时要进行初始化

![](http://1.14.100.228:8002/images/2022/01/11/l2.png)

**droupout正则化(inverted dropout)**

对一层神经元设置超参数keep-prob，随机失活神经元

![](http://1.14.100.228:8002/images/2022/01/11/drop_out.png)

drop-out在计算机视觉网络中应用非常广泛，作用有些类似于L2正则化

缺点：可能让本来拟合的模型又拟合了、在任务比较小的时候尽量不要用dropout

**L1正则化**
$$
J(w,b)=\frac{1}{m}\sum_i^{l-1}\sum_j^{l}\sigma(\hat{y}^i,y^i)+\frac{\lambda}{m}||w||_1\\||w||_1=\sum_i^{l-1}\sum_j^{l}|w|
$$

因为一次函数特性，l1函数使模型明显稀疏。有一些防止过拟合作用

![](http://1.14.100.228:8002/images/2022/01/11/l1.png)

**其它正则化方法**

early stopping方法

在网络训练过程中，网络的值逐渐变大

![](http://1.14.100.228:8002/images/2022/01/11/early-stopping.png)

使用early-stopping方法比较方便，但是该方法在取到了合适的w的同时忽视了网络的训练成熟度，即损失函数还未达到最优就停止训练了

**梯度检验**

手动实现的反向传播很有可能出现问题，通过单独计算损失函数J(x)在x上的双向误差积分，与反向传播的dx相减求欧式距离再除以梯度和积分的范数，由结果评估反向传播的效果。

![](http://1.14.100.228:8002/images/2022/01/11/5a41ae56ed61e759a9b08b2b9447920e.png)

#### 规范化(normalization)

**批规范化**

对隐藏层输入进行均值值为0，方差为1的重新分布处理

**BN的优点：**

- ①极大提升了训练速度，收敛过程大大加快；
- ②增加了分类效果，一种解释是这是类似于Dropout的一种防止过拟合的正则化表达方式，所以不用Dropout或正则化也能达到相当的效果；
- ③简化了调参过程，对于初始化参数权重不太敏感，允许使用较大的学习率

对①，BN使输入回到梯度线性区域，梯度增大，虽然增大了梯度爆炸的风险，但加快了训练速度

对②，大概没法解释，反正很好就是了

对③，BN层使权重对输入结果的影响减弱，故使学习率调整更加简单

### 优化算法(optimizer alogrithms)

**指数加权平均数**

通过取不同权重的β，做一段数据的平均值近似。

![](http://1.14.100.228:8002/images/2022/01/11/3a29088eda672aa59474daba2e6a645d.png)

**MomentumOptimizer**

动量梯度下降可以减少梯度在纵轴上的大小，增大在横轴上的大小

![](http://1.14.100.228:8002/images/2022/01/11/momentum.png)

使用momentum算法使梯度下降的更快，可以轻易的离开local minimal

**RMSprop**

均方根梯度下降可以减少梯度在各个方向上的变化幅度

![](http://1.14.100.228:8002/images/2022/01/11/RMSprop.png)

使用RMSprop算法使梯度下降的更快，并允许使用更大的学习率

**AdamOptimizer**

Adaptive Moment estimation是Momentum算法和RMSprop算法的结合，具有两者的优点，具有良好的模型泛化能力

参数：α：自动衰减、β1：0.9、β2：0.999、ε：一般不用调

![](http://1.14.100.228:8002/images/2022/01/11/Adam.png)

### 模型优化(modeling)

**模型压缩与剪枝**

**模型量化**

**推理软件**

![](http://1.14.100.228:8002/images/2022/01/11/1a8172dd520c71d84c22f515d6ae4752.png)

### 数据集(Dataset)

**验证集与测试集**

 测试集用于估计模型对样本的泛化误差，验证集用于“训练”模型的超参数。前者在用于模型时关注于accuracy准确度，后者关注于不同超参数下loss的变化，二者在数据上的区别就是验证集不需要测试集那么大的数据量

**验证指标**

**验证方法**

**少样本学习(Few-shot leaning)**

[Understanding few-shot learning in machine learning | by Dr. Michael J. Garbade | Quick Code | Medium](https://medium.com/quick-code/understanding-few-shot-learning-in-machine-learning-bede251a0f67)

在很多金融场景，大批量的数据难以进行标注，此时能够使用的有效样本数量有限，**少样本学习**能够在稀疏的特征空间中找到最优。

在文章中提到

1. 少量未标注的样本就可能使模型性能下降很多
2. 大量标注数据集可能花费很多

## 卷积神经网络(convolution neural network)

#### 卷积核、滤波器：

![](http://1.14.100.228:8002/images/2022/01/11/f4d58620597525449e653b3e7cddf33d.png)

卷积核大小(size)表示感受域大小，一般为3、5、7

不同的滤波器内核尺寸会基于滤波器感受野大小进行提取不同粒度级别的特征信息，3x3会比5x5提取到更细粒度级别的特征信息

**1X1卷积**

[(6 封私信 / 52 条消息) １乘１的卷积核到底有什么用? - 知乎 (zhihu.com)](https://www.zhihu.com/question/270007987)

1*1的卷积可以看做是各通道对应点的线性加权，**参数少**，能够**融合各个通道**

直观的讲，1×1的卷积层可以看成一个逐像素的全连接层

例如，输入特征图是10×10×256，通过1×1卷积得到10×10×20的输出。此时输出中的一个像素就是1×1×20的向量，它就是由输入像素的1×1×256的向量全连接得到的。只是这个全连接层的参数是在每个像素上共享的。

#### 卷积步长(stride)

 卷积核的步长度代表提取的精度 ，Size为3的卷积核，如果step为1，那么相邻步感受野之间就会有重复区域；如果step为2，那么相邻感受野不会重复，也不会有覆盖不到的地方；如果step为3，那么相邻步感受野之间会有一道大小为1的缝隙，从某种程度来说，这样就遗漏了原图的信息，直观上理解是不好的。

当卷积核step为1时，输出矩阵Size会和输入矩阵Size保持一致；而卷积核step为2时，由于跨步会导致输出矩阵Size降低为输入矩阵Size的一半。由此会产生一种类似“池化”的效果，利用这一特性可以代替池化层。 

 步长小，提取的特征会更全面，不会遗漏太多信息。但同时可能造成计算量增大，甚至过拟合等问题。步长大，计算量会下降，但很有可能错失一些有用的特征。 



#### 池化层(pooling layer)

图片的相邻像素具有相似的值，因此卷基层中很多信息是冗余的。通过池化来减少这个影响，用于缩减模型的大小，提高计算速度，提高所取特征的鲁棒性，同时减少过拟合，拥有invariance(不变性)，这种不变性包括translation(平移)，rotation(旋转)，scale(尺度)。 **[**池化层没有权重**]**

**最大池化(max pooling)** 

能很好的保留图片纹理特征

**平均池化(average pooling)**

 能很好的保留背景，但容易使得图片变模糊

**全连接层(full contact)**

就是经典隐藏层，因为与输入特征全部相连，所以称为全连接层。

#### 卷积深度

#### 经典网络

LeNet-5

![](http://1.14.100.228:8002/images/2022/01/11/LeNet-5.png)

AlexNet

![](http://1.14.100.228:8002/images/2022/01/11/Alexnet.png)

VGG-16

![](http://1.14.100.228:8002/images/2022/01/11/VGG-16.png)

## 现代卷积神经网络

#### ResNet

[【直播回放】深度卷积神经网络模型设计技术_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1UE411Z7tG/)

(讲了一些网络的变化历史、解释了一些resnet的有效性、介绍了一些分组网络和变种、介绍了一些多分支网络、介绍了一些多输入网络、Attention与动态推理网络、介绍了一些非规则(卷积核、结构)网络、GAN网络、AutoML网络、模型加速)

根据通用逼近定理，给定足够的容量，我们知道具有单层的前馈网络足以表示任何函数。 但是，该层可能很大，并且网络容易过度拟合数据。 因此，研究界有一个共同的趋势，即我们的网络架构需要更深入。

**普通深度网络难以训练**

由于解空间的搜索困难，plain网络很难训练。

**网络结构**

ResNet网络是参考了VGG19网络，在其基础上进行了修改，并通过短路机制加入了残差单元

直接使用stride=2的卷积做下采样，并且用global average pool层替换了全连接层

当feature map大小降低一半时，feature map的数量增加一倍，这保持了网络层的复杂度。

(以上是当时He的网络结构，不一定是现在使用的)

![](http://1.14.100.228:8002/images/2022/01/11/resnet.png)

**残差模块**

ResNet可以被认为是Highway Network的一个特例，因为它同样实现了Highway Network的信息流传输的功能，即通过shortcut connections控制信息流流过。

何凯明随后测试了许多resnet block的variant，最终找到了(e)预激活变体，其中梯度可以无阻碍地通过快捷连接流到任何其他早期层。 事实上，何凯明对不同的残差单元做了细致的分析与实验，最优的即是最右边的残差结构

![](http://1.14.100.228:8002/images/2022/01/11/resnet-variant.png)

**残差映射公式**

**resnet特点**

* resnet 和 plain network在都是没那么深层的情况下，表现都好，但是resnet收敛更快
* resnet和相同结构比起来，所需参数更少（数量级级别的减少）
* resnet 能永远获益于层数的增加，但太深了容易over-fitting
* 普通网络有优化困难的问题，在有BN的情况下，通常不是由于梯度消失或者梯度爆炸导致的，具体原因在后边解释

**残差网络的反直觉解释**

不能单纯用梯度消失和梯度爆炸来解释深度网络难以训练的问题，正则化初始化和中间的正则化层（Batch Normalization）已经可以很好的避免梯度消失问题

网络深度增加会出现另一个问题，就是退化问题，网络层数增加，但是在训练集上的准确率却饱和甚至下降了。这个不能解释为overfitting，因为overfit应该表现为在训练集上表现更好才对。退化问题说明了深度网络不能很简单地被很好地优化

以论文中出现的56层网络和20层网络为例，从原理上来说其实56层网络的解空间是包括了20层网络的解空间的，那为什么56层网络的训练误差和测试误差都比20层网络更高呢

我们在训练网络用的是随机梯度下降策略，往往解到的不是全局最优解，而是局部的最优解，显而易见56层网络的解空间更加的复杂，所以导致使用随机梯度下降算法无法解到最优解。

**网络实现**

Resnet104部分网络结构，可以用tensorflow或者pytorch实现一个

![](http://1.14.100.228:8002/images/2022/01/11/Resnet104.png)

#### ResNeXt

在这个变体中，不同路径的输出通过将它们加在一起来合并，所有路径共享相同的拓扑。作者引入了一个称为基数的超参数——独立路径的数量，以提供一种调整模型容量的新方法。 实验表明，与通过更深或更广的基数相比，增加基数可以更有效地获得准确性。

这种结构块具有以下三种等效形式

![](http://1.14.100.228:8002/images/2022/01/11/Resnext.png)

#### DenseNet

DenseNet是一种新型结构，它进一步利用了快捷连接的效果——将所有层直接相互连接。每一层的输入由所有先前层的特征图组成，其输出传递到每个后续层。 特征图通过深度级联进行聚合。

这种架构还鼓励特征重用，使网络具有高度的参数效率（不一定对）

身份映射的输出被添加到下一个块，**如果两层的特征图具有非常不同的分布**，这可能会阻碍信息流。 因此，连接特征图可以保留它们并增加输出的方差，鼓励特征重用

### 为什么卷积网络如此有用？

* 卷积神经网络一开始被设计出来即是为了解决传统神经网络处理图像的偏移和形变所造成的巨大误差。
* 卷积层用于特征提取，池化层用于特征组合
* 特征映射是分类的关键
* 单纯从学习参数的角度

## 循环神经网络(Recurrent Neural Network)

 **序列模型**

对带有时间或空间的关系的一系列数据处理的模型称为序列模型，而RNN擅长处理此类数据，即RNN能处理前一个输入与后一个输入有关系的序列数据，如：

语音转文字，生成音乐，评价分析，DNA序列分析，机器翻译，视频分析，判断句中成分

#### 多对多单向循环神经网络

![](http://1.14.100.228:8002/images/2022/01/11/676f7e7d5e1cfcf87c4f1fdb8101f47e.png)

图中还有W_aa, W_ax, W_ya这些权重未画出

**前向传播**
$$
a^{<t>}=g_1(W_{aa}*a^{<t-1>}+W_{ax}*x^{<t>}+b_a)\\y^{<t>}=g_2(W_{ya}*a^{<t>}+b_y)
$$
一般的g1为 tanh/relu g2为softmax

**反向传播**

对于每一个y<t>, 都有一个对应类别的概率，我们假设为是否为名字，方便做二值化。此时y非1即0，于是定义交叉熵损失函数为损失函数。通过上式，我们可以很方便的求出反向梯度

#### 多对一、一对多、多对少和少对多单向循环神经网络

大体与上相同，但在x与y的计算上有些许不同

<img src="http://1.14.100.228:8002/images/2022/01/11/b7d49e71bcba2b32c20efd210cf82a4d.png" style="zoom: 33%;" />

#### 双向循环神经网络

![](http://1.14.100.228:8002/images/2022/01/11/b9a80ad13da0ca779c22dcff8ad3e880.png)

有时在模型的建立中，结果的预测需要序列前后的信息，此时需要构筑双向循环神经网络

#### 深层循环神经网络



**梯度消失**

假如给定一个时间为t的RNN单元，如果要训练它的话，那么就是让损失函数对W1、W2、W3和b1、b2、b3求偏导，在假设简单的情况，偏导的情况可能如下

![](http://1.14.100.228:8002/images/2022/01/11/816c606f7d82f481df558617b20d246e.png)

​																								（这里加号应该是等号）

可知，深度对W1没什么影响，但是会使W2和W3出现梯度消失和梯度爆炸的问题，其中梯度爆炸表现在结果上十分明显，并且可以用设定阈值来解决，但梯度消失更加难以发觉

当出现梯度消失时，浅层单元与深层单元的连接几乎终止，后端单元无法得到训练，表现为预测结果混乱等等

#### LSTM(long short term memory)

![](http://1.14.100.228:8002/images/2022/04/09/20220409174453.png)

为解决RNN“长期记忆力不足”的情况，LSTM于1997年被提出，简单来说，相比普通的RNN，LSTM能够在更长的序列中有更好的表现。 

如图，看上去LSTM比普通RNN复杂了很多，不要忘记，LSTM于RNN来说，要解决核心问题即是深度带来的梯度消失问题。于是引入ct和忘记门、记忆门以及输出门机制。

各种公式为
$$
\begin{array}{l}
\sigma_{1,2,3}=sigmoid(W^{1,2,3}_{x}*x^t+W^{1,2,3}_{h}*h_{t-1})\\
z=\tanh(W_x*x_t+W_h*h_{t-1})\\
c^{t}=\sigma_{1}^{f} \odot c^{t-1}+\sigma_{2}^{i} \odot z \\
h^{t}=\sigma_{3}^{o} \odot \tanh \left(c^{t}\right) \\
y^{t}=g\left(W^{\prime} h^{t}\right)
\end{array}
$$
根据图和这些公式，我们可以对LSTM中各单元作用进行分析：

- **输出门** ![[公式]](https://www.zhihu.com/equation?tex=o_%7Bt-1%7D)：输出门的目的是从细胞状态 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_%7Bt-1%7D) 产生隐层单元 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+h_%7Bt-1%7D) 。并不是 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_%7Bt-1%7D) 中的全部信息都和隐层单元 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+h_%7Bt-1%7D) 有关， ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_%7Bt-1%7D) 可能包含了很多对 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+h_%7Bt-1%7D) 无用的信息。因此， ![[公式]](https://www.zhihu.com/equation?tex=o_t) 的作用就是判断 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_%7Bt-1%7D) 中哪些部分是对 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+h_%7Bt-1%7D) 有用的，哪些部分是无用的。
- **输入门** ![[公式]](https://www.zhihu.com/equation?tex=i_t)。![[公式]](https://www.zhihu.com/equation?tex=i_t) 控制当前词 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+x_t) 的信息融入细胞状态 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_t) 。在理解一句话时，当前词 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+x_t) 可能对整句话的意思很重要，也可能并不重要。输入门的目的就是判断当前词 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+x_t) 对全局的重要性。当 ![[公式]](https://www.zhihu.com/equation?tex=i_t) 开关打开的时候，网络将不考虑当前输入 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+x_t) 。
- **遗忘门** ![[公式]](https://www.zhihu.com/equation?tex=f_t): ![[公式]](https://www.zhihu.com/equation?tex=f_t) 控制上一时刻细胞状态 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_%7Bt-1%7D) 的信息融入细胞状态 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_t) 。在理解一句话时，当前词 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+x_t) 可能继续延续上文的意思继续描述，也可能从当前词 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+x_t) 开始描述新的内容，与上文无关。和输入门 ![[公式]](https://www.zhihu.com/equation?tex=i_t) 相反， ![[公式]](https://www.zhihu.com/equation?tex=f_t) 不对当前词 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+x_t) 的重要性作判断，而判断的是上一时刻的细胞状态 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_%7Bt-1%7D) 对计算当前细胞状态 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_t) 的重要性。当 ![[公式]](https://www.zhihu.com/equation?tex=f_t) 开关打开的时候，网络将不考虑上一时刻的细胞状态 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_%7Bt-1%7D) 。
- **细胞状态** ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_t) ： ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_t) 综合了当前词 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+x_t) 和前一时刻细胞状态 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_%7Bt-1%7D) 的信息。这和ResNet中的残差逼近思想十分相似，通过从 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_%7Bt-1%7D) 到 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_t) 的“短路连接”，梯度得已有效地反向传播。当 ![[公式]](https://www.zhihu.com/equation?tex=f_t) 处于闭合状态时， ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_t) 的梯度可以直接沿着最下面这条短路线传递到 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_%7Bt-1%7D) ，不受参数 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+W_%7Bxh%7D) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+W_%7Bhh%7D) 的影响，这是LSTM能有效地缓解梯度消失现象的关键所在。

#### GRU单元(Gate Recurrent Unit)

GRU和LSTM在很多情况下实际表现上相差无几，但是更易于计算，于是相比之下更容易进行训练，能够很大程度上提高训练效率，因此很多时候会更倾向于使用GRU。

<img src="http://1.14.100.228:8002/images/2022/04/09/20220409174405.png" style="zoom: 50%;" />

如图，各种公式为
$$
\begin{array}{1}
z,r=sigmoid(W^{z,r}_x*x^t+W^{z,r}_h*h^{t-1})\\
h^{t-1'}=h^{t-1}\odot{r}\\
h'=sigmoid(W^h_x*x^t+W^h_h*h^{t-1'})\\
h^t=(1-z)\odot{h^{t-1}}+z\odot{h'}\\
\end{array}
$$
可知，GRU只有两个门，ht的更新公式同时起到了记忆和遗忘的功能，那么这里的**h'**实际上可以看成对应于LSTM中的hidden state；上一个节点传下来的 ![[公式]](https://www.zhihu.com/equation?tex=h%5E%7Bt-1%7D) 则对应于LSTM中的cell state。1-z对应的则是LSTM中的 ![[公式]](https://www.zhihu.com/equation?tex=z%5Ef) forget gate，那么 z我们似乎就可以看成是选择门 ![[公式]](https://www.zhihu.com/equation?tex=z%5Ei) 了。

## 现代循环神经网络

### 为什么循环神经网络如此有用？

## 注意力机制(Attention)

