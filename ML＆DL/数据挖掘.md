## 业务理解

**理解技巧**

**有哪些比赛已经看过：**

二手车价格预测：

是什么：通过二手车交易时的各种量化信息来预测价格，是一个经典的回归问题

有什么：数据来自某交易平台的二手车交易记录，总数据量超过40w，包含31列变量信息，其中15列为匿名变量。从中抽取15万条作为训练集，5万条作为测试集A，5万条作为测试集B，同时会对信息进行脱敏

深入理解：赛题特征包含时序信息，可能价格会根据时间呈某种规律变化。可能相似车型之间有价格关联

工业蒸汽量预测：

是什么：在火力发电厂里，影响发电效率的核心因素是锅炉的燃烧效率，通过影响锅炉的燃烧效率的多个工况信息预测锅炉的工业蒸汽量

有什么：本次数据集特征均为匿名，共38个特征变量，训练集共2889条，测试集共1926条，评估指标为MSE
$$
MSE=\frac{1}{n}\sum_{i=1}^{n}(y_{i}-f(x_{i}))^{2}
$$

心跳信号分类预测



## 数据探索分析

**数据脱敏**

对某些敏感信息通过脱敏规则进行数据的变形，实现敏感隐私数据的可靠保护

脱敏方式：

最终脱敏出来的是

**采样、正样本与负样本**

采样方法可以用于解决正负样本不均衡的问题

**箱型图**

一.箱子的中间一条线，是数据的中位数，代表了样本数据的平均水平。

二.箱子的上下限，分别是数据的上四分位数和下四分位数。这意味着箱子包含了50%的数据。因此，箱子的宽度在一定程度上反映了数据的波动程度。

三.在箱子的上方和下方，又各有一条线。有时候代表着最大最小值，有时候代表“上下限”。

箱型图要点：

一.箱线图是针对连续型变量的，解读时候重点关注平均水平、波动程度和异常值。

二.当箱子被压得很扁，或者有很多异常的时候，试着做对数变换。

三.当只有一个连续型变量时，并不适合画箱线图，直方图是更常见的选择。

四.箱线图最有效的使用途径是作比较，配合一个或者多个定性数据，画分组箱线图。

**EDA（数据探索性分析）**

- EDA的价值主要在于**熟悉数据集**，**了解数据集**，对数据集进行**验证**来确定所获得数据集可以用于接下来的机器学习或者深度学习使用
- 当了解了数据集之后我们下一步就是要去了解**变量间的相互关系**以及变量与**预测值之间的存在关系**
- 引导数据科学从业者进行**数据处理以及特征工程**的步骤,使**数据集的结构**和**特征集**让接下来的预测问题更加可靠
- 完成对于数据的探索性分析，并对于数据进行一些**图表或者文字总结**并打卡

假如一个比赛的任务足够复杂，那么应该使用深度网络的特征提取能力。事实上，数据分析能力是将数据落实到模型的基础

### 变量分析

**数值分布**

### 数据处理

#### 无量纲化

在机器学习算法实践中,我们往往有着将不同规格的数据转换到同一规格,或不同分布的数据转换到某种特定分布的需求,这种需求统称为将数据无量纲化。譬如**梯度和矩阵**为核心的算法中，譬如逻辑回归，支持向量机，神经网络，无量纲化可以**加快求解速度**;而在**距离类**模型，譬如K近邻，K- Means聚类中，无量纲化可以帮我们**提升模型精度**,避免某一个取值范围特别大的特征对距离计算造成影响。(一个特例是决策树和树的集成算法提升树我们不需要无量纲化，决策树可以把任意数据与处理的得很好。）

数据的无量纲化可以是线性的，也可以是非线性的。线性的无量纲化包括中心化( Zero-centered或者Mean-subtraction)处理和缩放处理( Scale)，中心化的本质是让所有记录减去一个固定值，即让数据样本数据平移到某个位置。缩放的本质是通过除以一个固定值，将数据固定在某个范围之中，取对数也算是一种缩放处理。

在不同的任务中，标准化的意义不同

* 在回归预测中，标准化是为了让特征值有均等的权重
* 在训练神经网络的过程中，通过将数据标准化，能够加速权重参数的收敛
* 主成分分析中，需要对数据进行标准化处理：默认指标间权重相等，不考虑指标间差异和相互影响

**归一化与标准化**

[(2 封私信 / 81 条消息) 特征工程中的「归一化」有什么作用？ - 知乎 (zhihu.com)](https://www.zhihu.com/question/20455227)

MinMaxScaler： MinMaxScaler在不涉及距离度量、梯度、协方差计算以及数据需要被压缩到特定区间时使用广泛,比如灰度图像处理中量化像素强度时,都会使用 MinMaxScaler将数据压缩于[0,1]区间之中。

StandardScaler：大多数机器学习算法中,会选择 StandardScaler来进行特征缩放，因为 MinMaxScaler对异常数值比较敏感。在PCA，聚类，逻辑回归，支持向量机，神经网络这些算法中，StandardScaler往往是最好的选择

除了 StandardScaler和 MinMaxScaler之外，sklearn中也提供了各种其他缩放处理(中心化只需要一个pandas，广播一下减去某个数就好了，因此 sklearn不提供任何中心化功能）。比如，在希望压缩数据，却不影响数据的稀疏性时(不影响矩阵中取值为0的个数时)，我们会使用MaxAbsScaler；在异常值多，噪声非常大时，我们可能会选用分位数来无量纲化，此时使用 Robustscaler。

#### **缺失值处理**

**删除缺失值**

对于缺失数据，最先想到也是最简单的处理方法即是直接删除

只有当缺失数据占比小，删除后不影响整体数据的表达时可以这样做，事实上，现实数据大多都含有各种各样的缺失值，不能随便删除

**补全**

实际工程应用中，缺失值补全才是应用最广泛的方法，缺点是计算复杂，而且当插值估计不准确时，会对后续的模型引入额外的误差。补全大部分时候用统计量做补值，有时候用差值做补值

**高维映射**

将离散数据的n个取值扩大为n种属性，高维映射是最精确的做法，它完全保留了所有的信息，也未増加仼何额外的信息。

（比如广告的CTR预估模型，预处理时会把所有变量都这样处理，达到几亿维）

* 优点：完整保留了原始数据的全部信息。
* 缺点：计算量大大提升。而且只有在样本量非常大的时候效果才好，否则会因为过于稀疏，效果很差。

#### **异常值处理**

由于数据噪音比较多，所以细致的预处理能够是模型更具泛化性，同时挖掘更多特征。

**可视化删除异常值**

**Box-Cox变换（处理有偏估计）**

* 变换后，可以一定程度上减小不可观测的误差和预测变量的相关性，使得变换后的因变量于回归自变量具有线性相依关系，误差也服从正态分布，误差各分量是等方差且相互独立
* 用这个变换来使得因变量获得一些性质，比如在时间序列分析中的平稳性，或者使得因变量分布为正态分布。

前置概念：似然函数

公式：
$$
y(\lambda)=\left\{\begin{array}{l}
\frac{y_{i}^{\lambda}-1}{\lambda}, \text { 如果 } \lambda \neq 0 ; \\
\ln \left(y_{i}\right), \text { 如果 } \lambda=0 .
\end{array}\right.
$$
**长尾截断**

#### 数据分桶

离散化用于将连续的数值属性转化为离散的数值属性

是否使用特征离散化，这背后是：使用“海量离散特征+简单模型”，还是“少量连续特征+复杂模型

对于线性模型，通常采用**“海量离散特征+简单模型”**。

* 优点：模型简单
* 缺点：特征工程比较困难，但是一旦有成功的经验就可以推广，并且可以很多人并行研究

对于非线性模型（如深度学习），通常使用**“少量连续特征+复杂模型”**

* 优点：不需要进行复杂的特征工程
* 缺点：模型复杂，需要大量调参

**等频分桶**

**等距分桶**

**Best_KS分桶**

**卡方分桶**

## 特征工程

在机器学习中 ，特征是在观测现象中的一种独立、可测量的属性。选择信息量大的、有差别性的、独立的特征是模式识别、分类和回归问题的关键一步

最初的原始特征数据集可能太大，或者信息冗余，因此在机器学习的应用中，一个初始步骤就是选择特征的子集，或构建一套新的特征集，减少功能来促进算法的学习，提高泛化能力和可解释性，其核心目的就是获得更好的训练数据

特征工程可以针对不同类型的数据进行不同的处理以适应任务，对这些数据的处理统称特征工程。大致可分为：表格、图像、信号、文本

一般而言，特征工程可以分为以下方面，数据预处理，特征选择(Feature Selection)，特征提取(Feature Extraction)，特征构造(Feature Construction)，最后当算法部署在线上时，需要进行特征监控以保护算法性能

#### 特征类型

![](http://1.14.100.228:8002/images/2022/01/11/b9da50d7019caef077c8342b1afe740e.png)

* 匿名特征与非匿名特征需要分开处理，有时也可以将匿名特征与非匿名特征做组合

#### 类别特征编码

![](http://1.14.100.228:8002/images/2022/01/11/1b5d40eaec992631970d6a9831366a15.png)

* 类别特征encoder后一般 不会直接用在训练中
* 根据类别特征生成标签统计量特征

#### 数值特征

![](http://1.14.100.228:8002/images/2022/01/11/60a4062269eeee900bb037703303d56b.png)

为什么做数据分箱？

* 离散后稀硫向量内积乘法运算速度更快，计算结果也方便存储，容易扩展
* 离散后的特征对异常值更具鲁棒性，如age>30为1否则为0，对于年龄为200的也不会造成影响
* LR属于广义线性模型，表达能力有限，经过离散化后，每个变量有单独的权重，这相当平衡了样本权重
* 离散后特征可以进行特征交又，提升表达能力，由M+M个变量变成M*N个变量
* 特征离散后模型更稳定，如用户年龄区间，不会因为用户年龄长了一岁就变化

#### 时间特征

![](http://1.14.100.228:8002/images/2022/01/11/2e80e3189736781fc8e2ef39792ddbb9.png)

上图是三种基本时间特征工程方法

### 特征选择

持征选择可能会降低模型的预测能力。因为被剔除的特征中可能包含了有效的信息，抛弃了这部分信息会一定程度上降低预测准确率。

#### 过滤式（flt）

通过计算特征重要性，选取重要特征，计算简单

方法：

* Relief
* 方差选择
* 相关系数
* 卡方检验
* 互信息

**特征重要性**

https://blog.csdn.net/zwqjoy/article/details/97259891

GBDT、RF、Xgboost等树类模型建模时，往往会计算特征的重要性作为接下来改进算法的重要依据，

目前计算特征重要性的方法主要有两个方面，分别是训练中计算（XGBoost、GBDT）和训练后计算（RF）

相关性选择主要用于判别线性相关，对于预测变量如果存在更复杂的函数形式的影响，则应该采取其它方法去筛选特征（eg. 树模型特征重要性）

#### 包裹式（wra）

* 由于直接针对特定学习器进行优化因此从最终学习器性能来看，效果比过滤式特征选择更好。

* 需要多次训练学习器，因此计算开销通常比过滤式特征选择大得多。

方法：LVM（Las Vegas Wrapper）

#### 嵌入式（emd）

结合过滤式和包裹式，学习器训练过程中自动进行了特征选择，如Lasso回归、xgb、lgbm

### 特征构造

特征构造大概可以从三个方面入手，领域特征；交叉特征；多项式特征。

常见特征：

* 统计量特征：计数、求和、比例、标准差等
* 相对时间和绝对时间、节假日、双休日等
* 空间特征：包括分箱，分布编码等方法

#### 非线性变换

包括log、平方、根号等

#### 特征组合＆特征交叉

[特征组合&特征交叉 (Feature Crosses) - SegmentFault 思否](https://segmentfault.com/a/1190000014799038)

特征组合也叫特征交叉，**合成特征是特征组合的一个子集**，特征组合在特征工程中的地位非常之高，从特征类别来分，组合特征可以分为离散特征之间的组合，离散和连续特征的组合，连续特征之间的组合。

通过采用随机梯度下降法，可以有效地训练线性模型。因此，在使用扩展的线性模型时辅以特征组合一直都是训练大规模数据集的有效方法。

一个特征一般在现实中只能投影出一种属性，但是当一种特征与其它特征有了非线性强关联时，再将其作为单独的特征用于模型的训练则不合适了

下面有几个不同类型的特征组合的表征含义

![组合特征](D:\Coder\Github\学习笔记\image\组合特征.png)

**组合独热编码离散特征**

在实际应用中，我们很少对连续数值特征进行特征组合，更多的是对离散特征，尤其是独热编码的离散特征进行特征组合，将独热特征矢量的特征组合视为逻辑连接。假如我们要对连续数值特征进行特征组合时，我们一般用分箱的方法离散化特征，以此获得连续数值特征的逻辑连接

例子：预测狗主人对狗狗的满意程度；国家/地区和语言

[Feature Crosses  | Machine Learning Crash Course  | Google Developers](https://developers.google.com/machine-learning/crash-course/feature-crosses/video-lecture)

### 特征降维

#### LDA线性判别分析

#### SVD奇异值分解

[奇异值分解(SVD)原理与在降维中的应用 - 刘建平Pinard - 博客园 (cnblogs.com)](https://www.cnblogs.com/pinard/p/6251584.html)

特征值和特征向量

![](http://1.14.100.228:8002/images/2022/01/11/81cb673e5b639e7056424be85575d682.png)

A是一个mxn的矩阵，我们定义矩阵A的SVD为
$$
A=U\sum V^{T}
$$
具体计算方法参照博客，这里解释一些SVD的性质，以及为PCA做铺垫

对于奇异值,它跟我们特征分解中的特征值类似，在奇异值矩阵中也是按照从大到小排列，而且奇异值的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上的比例。也就是说，我们也可以用最大的k个的奇异值和对应的左右奇异向量来近似描述矩阵。

![](http://1.14.100.228:8002/images/2022/01/11/SVD.png)

由于这个重要的性质，SVD可以用于PCA降维，来做数据压缩和去噪。也可以用于推荐算法，将用户和喜好对应的矩阵做特征分解，进而得到隐含的用户需求来做推荐。同时也可以用于NLP中的算法，比如潜在语义索引（LSI）

SVD作为一个很基本的算法，在很多机器学习算法中都有它的身影，特别是在现在的大数据时代，由于SVD可以实现并行化，因此更是大展身手。当然，SVD的缺点是分解出的矩阵解释性往往不强，有点黑盒子的味道，不过这不影响它的使用。

例子：图片压缩[SVD矩阵分解 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/130439386)

#### PCA主成分分析

[PCA主成分分析学习总结 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/32412043)

[StatQuest: Principal Component Analysis (PCA), Step-by-Step - YouTube](https://www.youtube.com/watch?v=FgakZw6K1QQ)

主成分分析（principal components analysis）是最重要的降维方法之一。在数据压缩消除冗余和数据噪声消除等领域都有广泛的应用。降维，即是在更低的维度上尽可能的将数据按照原来的分布表达出来。

对于正交特征空间中的样本点，我们在降低其空间维度时，应当遵循两个原则：

* 最近重构性：样本点到新空间的距离足够近
* 最大可分性：样本点到新空间的投影能尽可能分开

将基于最大可分性作为优化目标，就可以推导出主成分分析算法

在PCA降维过程中，当进行**协方差矩阵求解特征值**时，如果面对维度高达 **10000*10000**，可想而知耗费的计算量程平方级增长。面对这样一个难点，从而引出奇异值分解(SVD)，利用SVD不仅可以解出PCA的解，而且无需大的计算量

PCA（主成分分析）和LDA（线性判别分析）有很多的相似点，其本质是要将初始样本映射到维度更低的样本空间中，但是PCA和LDA的映射目标不一样：PCA是为了让映射后的样本具有最大的发散性；而LDA是为了让映射后的样本有最好的**分类性能**。所以说PCA是一种无监督的降维方法，而LDA是一种**有监督**的降维方法。

具体算法流程在博客内有详细介绍，这里做一些总结

作为一个非监督学习的降维方法，它只需要特征值分解，就可以对数据进行压缩，去噪。因此在实际场景应用很广泛。为了克服PCA的一些缺点，出现了很多PCA的变种，比如为解决非线性降维的KPCA，还有解决内存限制的增量PCA方法Incremental PCA，以及解决稀疏数据降维的PCA方法Sparse PCA等。

主要优点：

* 仅仅需要以方差衡量信息量，不受数据集以外因素影响
* 各主成分之间正交，可消除原始数据成分之间的相互影响的因素
* 计算方法简单，主要运算是特征值分解，易于实现

主要缺点：

* 主成分各个维度的含义具有一定的模糊性，不如原始样本特征的解释性强

## 模型调参

不同的模型有不同的优缺点，代表不同的改进思路，但是都是调包训练，一开始使用两到三种不同性质的模型算法即可（算法工程）

**非线性模型**

**线性模型**

线性回归模型对特征以及标签都有要求，在线性模型中有一个假设，噪声都是服从正态分布的

xgboost、lightgbm这种树模型可以将缺失值自动识别为一种类别

**网格搜索**

网格搜索是一项模型超参数（即需要预先优化设置而非通过训练得到的参数）优化技术，常用于优化**三个或者更少数量**的超参数，本质是一种穷举法。对于每个超参数，使用者选择一个较小的有限集去探索。然后，这些超参数笛卡尔乘积得到若干组超参数。网格搜索使用每组超参数训练模型，挑选验证集误差最小的超参数作为最好的超参数。

**随机搜索**

使用随机搜索代替网格搜索的动机是，在许多情况下，所有的超参数可能不是同等重要的。随机搜索从超参数空间中随机选择参数组合，参数由n_iter给定的固定迭代次数的情况下选择。实验证明，随机搜索的结果优于网格搜索。

**贝叶斯搜索**

贝叶斯优化属于一类优化算法，称为基于序列模型的优化(SMBO)算法。这些算法使用先前对损失f观察结果，以确定下一个(最优)点来抽样f。

## 模型验证

Baseline模型意味着用较为普遍，较为通用的方法处理后得到的结果，通常在以该结果为基础，后续的操作要实现result较baseline有一定程度的提高

对于一个任务，从提升结果点数为目标的角度来说，算法模型的改进思路较为固定。重要的改进还是在特征以及数据的处理上，由此，我们需要用各种各样的手段提炼出当前模型存在什么问题，以此得到改进的启发，而得到模型不足的信息，针对模型不足改进的思路。这些需要对统计学、数据、代码、算法都有相当的认识。

**评估指标与目标函数**

对评价指标具有充分了解对任务本身的解读具有帮助

分类算法常见的评估指标如下：

* 对于二类分类器/分类算法，评价指标主要有: accuracy，[Precision，Recall，F-score，PR曲线]，ROC-AUC曲线。

* 对于多类分类器/分类算法，评价指标主要有: accuracy，[宏平均和微平均，F-score]。

回归预测算法常见的评估指标如下：

平均绝对误差( Mean absolute error，MAE)，均方误差(Mean Squared Error，MSE)，平均绝对百分误差(Mean Absolute Percentage Error，MAPE)，均方根误差( Root mean Squared Error )，R2(R-Square)

### 验证方法

模型验证方法要和任务本身相结合，构造一个可满足验证集数据、任务评估指标要求的模型测试环境

**验证曲线：**学习率曲线、验证曲线

**训练集和测试集差异**

**过拟合**

判断模型拟合情况是否良好，可以看样本与预测值的残差，如果残差符合正太分布，则拟合较好

![](http://1.14.100.228:8002/images/2022/01/11/ylog.png)

**多重共线性**

[好好谈谈共线性问题 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/88025370)

定义：多重共线性（Multicollinearity）是指线性回归模型中的解释变量之间由于存在精确[相关关系](https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%E7%9B%B8%E5%85%B3%E5%85%B3%E7%B3%BB/9227098)或高度相关关系而使模型估计失真或难以估计准确，这里的模型估计并不是模型预测，**共线性问题对于损失函数的最优化没影响，参数都是一样更新，一样更新到收敛为止**，如果模型仅用于预测，则只要拟合程度好，可不处理多重共线性问题，存在多重共线性的模型用于预测时，往往不影响预测结果

一般来讲，太多相关度很高的特征并没有提供太多的信息量，并没有提高数据可以达到的上限，相反，数据集拥有更多的特征意味着更容易收到噪声的影响，更容易收到特征偏移的影响等等，简单举个例子，N个特征全都不受到到内在或者外在因素干扰的概率为k，则2N个特征全部不受到内在或外在因素干扰的概率必然远小于k。

多重共线性的影响

1、回归系数的估计值方差变大，回归系数的置信区间变宽，估计的精确性大幅度降低，使估计值稳定性变差

2、本应该显著的自变量不显著，本不显著的自变量却呈现出显著性，致使一些回归系数通不过显著性检验，回归系数的正负号也可能出现倒置。

3、多重共线性使参数估计值的方差增大，模型参数不稳定，也就是每次训练得到的权重系数差异都比较大。

4、如果可以保证自变量的相关自变量的相关类型在预测期不变，即当初建模时自变量间共同的相关趋势在预测时仍保持不变，用具有较强的多重共线性的方程去做预测效果仍会不错。如果自变量的相关类型在预测期发生改变，预测效果会很不好

消除多重共线性的方法

1、剔除一些不重要的解释自变量，选择回归模型时，可以将回归系数的显著性检验、方差扩大因子ⅥF的多重共线性检验与自变量的经济含义结合起来考虑，以引进或剔除变量

2、增大样本量，在实践种，当我们所选的样本个数接近样本量n时，自变量就越不容易受到多重共线性的影响

### 评估指标

[回归模型的评价指标比较 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/143169742)

针对目标函数优化模型固然没错，但是应用多种指标，综合考虑允许我们更加深入的分析模型

下链接解释了部分常见的回归预测指标，包含：MAE，MSE系列和中位误差以及R Squared（r2 score）

[机器学习之分类性能度量指标 : ROC曲线、AUC值、正确率、召回率 | Zhiwei's Blog (zhwhong.cn)](https://zhwhong.cn/2017/04/14/ROC-AUC-Precision-Recall-analysis/)

**MSE**：误差平方的期望值，误差表现在标签值与预测值的差，平方即是总体平方，期望则是总体除以均值

公式为
$$
MSE=\sum_{i=1}^{n}\frac{(y_{i}-\hat{y})^2}{n}
$$
MSE多用于回归模型的评价，作为凸函数，也可直接用于模型的损失评价，结果越小越好，使用十分广泛

**F1-score**：分类准确率与召回率的均值，结果越大越好

具体计算公式如下:
$$
F 1=\frac{2 * P * R}{P+R}
$$
其中P为准确率，计算公式如下：
$$
P=\frac{\text { 预测正确的心电异常事件数 }}{\text { 预测的心电异常事件数 }}
$$
R为召回率，计算公式如下:
$$
R=\frac{\text { 预测正确的心电异常事件数 }}{\text { 总心电异常事件数 }}
$$
P、R中涉及的事件数均是所有样本的累加，该公式要求模型在具有高精确率预测能力的同时，也要保证高召回率，要求模型对出现率低的异常事件也具有一定的检测能力

**混淆矩阵**：单基于错误率来判别分类模型的好坏并不完全，实际上，这样的度量错误掩盖了样例如何被分错的事实

假如一个预测三类问题，其混淆矩阵的形式如下，利用混淆矩阵可以充分理解分类中的错误

![](http://1.14.100.228:8002/images/2022/01/11/fdd5a10fb79e6778f5ae0305d106ae99.png)

**ROC曲线-AUC值**：是反映敏感性和特异性连续变量的综合指标，roc曲线上每个点反映着对同一信号刺激的感受性。

![](http://1.14.100.228:8002/images/2022/01/11/c488a578b9d31fd15461c47a38c39acc.png)

这张图片十分重要，需要牢记

（关于ROC曲线计算方法，TP、FP、FN、TN含义看上面的连接博客）

![](http://1.14.100.228:8002/images/2022/01/11/ROC.png)

* 横坐标：**1-Specificity**，伪正类率(False positive rate， FPR)，**预测为正但实际为负**的样本占**所有负例样本** 的比例；
* 纵坐标：**Sensitivity**，真正类率(True positive rate， TPR)，**预测为正且实际为正**的样本占**所有正例样本** 的比例。

由曲线图的基本描述和横纵坐标解释可知

分类器有一个重要功能“概率输出”，通过更深入地了解各个分类器的内部机理，我们总能想办法得到一种概率输出。通常来说，是将一个实数范围通过某个变换映射到(0,1)区间。

AUC被定义为ROC曲线下的面积，很多时候ROC曲线并不能清晰的说明哪个分类器的效果更好，而作为一个数值，对应AUC更大的分类器效果更好。

ROC曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。在实际的数据集中经常会出现类不平衡(class imbalance)现象，即负样本比正样本多很多(或者相反)，而且测试数据中的正负样本的分布也可能随着时间变化。

![](http://1.14.100.228:8002/images/2022/01/11/ROCPR.png)

由上图ROC与PR曲线对比可知，即使测试集正负样本变化，用ROC曲线依然可以测出该模型的真实性能

**微平均与宏平均**

计算多类分类的准确率，一种计算方法*是***把所有类别的一次性都考虑进来**，计算类别预测的准确率；另外还有一种是对**每个类别分开考虑**，计算单独每个类别的准确率，最后再进行算术平均得到该测试集的准确率。

第一种叫micro-average（微平均），第二种方法则是macro-average（宏平均）。

## 感悟

**数据为中心**

**特征为基础**

**模型是方向**

理解一个模型要将其抽象出来，嵌套进自身的体系知识中，做到多方向理解（迷惑发言）



看《阿里云天池大赛解析》的过程中不懂的部分

* 公式为什么有这样的性质？从怎样的思路可以构思出这样的性质？
* stacking为什么可以达到模型融合的效果？stacking究竟本质是什么？有什么使用的必要？
* 那几种lr各有什么特点？分别应该怎么使用？
* 在数据上，Resnet的本质是什么？深度神经网络到底能做些什么？
