## IO设备

![](http://1.14.100.228:8002/images/2022/05/12/20220512220236.png)

I/O（Input/Output），即输入/输出是系统的重要组成部分，**计算机通过IO存取设备，通过IO和外界交互。程序通过IO执行功能**。因此IO与操作系统结合方式是系统是否高效的关键。首先我们需要了解系统IO架构是怎样的。

根据IO**数据的来源**不同，系统IO架构可分为三层，一是高速内存的IO流，二是高速设备如显卡、高速网卡的IO流，三是慢速设备如磁盘、U盘、键盘鼠标等IO流。现代IO架构也会有专门的**芯片组**和点对点互连用于加速IO操作，例如x86平台。下图展示了IO架构的传统模型和现代系统的IO架构。

<img src="http://1.14.100.228:8002/images/2022/03/10/20220311094452.png" style="zoom:80%;" />

而对于单个设备来说，包含两个部分，一是暴露给操作系统的接口，二是硬件的抽象内部结构。设备接口简化来讲可以分为三个寄存器，分别是状态寄存器、命令寄存器、数据寄存器。操作系统对设备的调用，也可以由这三个寄存器来抽象为以下过程。

（读状态、存命令、取数据）

<img src="http://1.14.100.228:8002/images/2022/03/10/20220311100432.png" style="zoom:80%;" />

通过轮询状态寄存器，我们能够实现设备的长期访问，但是我们也知道，很多情况下轮询总是低效的，因此我们可以用**中断**来代替轮询，具体的，操作系统可以发出一个请求，将调用进程置于睡眠状态，并将上下文切换到另一个任务，而不是重复轮询设备。因此，IO和计算可以重叠。

<img src="http://1.14.100.228:8002/images/2022/03/10/20220311100916.png" style="zoom:80%;" />

但中断并不总是最好的解决方案，一个设备执行任务非常快时第一个轮询通常会找到要完成任务的设备。在这种情况下使用中断实际上会降低系统的速度，并且中断的方法可能造成**活锁**的问题。所以如果不知道设备的速度，或者有时快，有时慢，最好使用一个混合轮询器，轮询一段时间后，如果设备还没有完成，就使用中断。这种分两阶段的方法可以两全其美。

并且中断是可以合并的，在这样的设置中，需要触发中断的设备首先要等待一段时间，然后才将中断发送给CPU。这样能够降低中断的总体开销。

当我们使用programmed IO（PIO）来操作大块数据进入设备时。CPU会忙于搬运数据，浪费很多CPU时间。解决方案就是我们所说的直接内存访问(DMA)。它本质上是系统中的一种非常特定的设备，它可以在不需要太多CPU干预的情况下协调设备和主存之间的传输。为了将数据传输到设备，操作系统将通过告诉DMA引擎数据在内存中的位置、复制多少数据以及将数据发送到哪个设备来编程。此时，操作系统完成了传输，可以继续进行其他工作。当DMA完成时，DMA控制器引发一个中断，这样OS就知道传输完成了。修改后的时间表。有DMA介入的CPU timpline如下图所示。

> 也就是说将数据放到内存的这一操作并不是由cpu完成的，而是有专门的芯片组

<img src="http://1.14.100.228:8002/images/2022/03/10/20220311101641.png" style="zoom:80%;" />

让我们再深入设备的内部，看看操作系统是怎么与设备直接交互的，具体来说，有两种方式。

第一种，也是最古老的方法(IBM大型机使用了多年)是使用显式的I/O指令。这些指令为操作系统指定了一种将数据发送到特定设备寄存器的方式。例如x86系统下的in和out指令，要将数据发送到一个设备，调用者指定一个包含数据的寄存器，以及一个指定设备名称的特定端口。

第二种方法称为内存映射I/O。通过这种方法，硬件使设备寄存器可用，就像它们是内存位置一样。为了访问一个特定的寄存器，操作系统发出一个加载(读)或存储(写)地址;然后硬件将加载/存储路由到设备而不是主存储器。也就是像访问内存一样访问设备。

这两种方法都没有很大的优势。内存映射方法很好，因为不需要新的指令来支持它，但这两种方法今天仍然在使用。

但是设备内部到底是如何呢，这要求我们继续讨论设备驱动的概念，这是操作系统能够与设备交互的最底层方式，任何与设备交互的细节都封装在里面。

通过Linux文件系统软件堆栈，我们可以了解这种抽象如何帮助操作系统的设计和实现。

<img src="http://1.14.100.228:8002/images/2022/03/10/20220311104126.png"  />

可以看到，文件系统(当然，上面的应用程序)完全不知道它使用的是哪个磁盘类;它只是向通用块层发出块读和写请求，通用块层将它们路由到适当的设备驱动程序，后者处理发出特定请求的细节。上面图还有一个Raw接口，它允许特殊的应用程序(如文件系统检查器，稍后[AD14]描述，或磁盘碎片整理工具)直接读写块，而不使用文件抽象。

上面看到的封装也有其缺点。例如，如果有一个设备具有许多特殊的功能，但是必须向内核的其他部分提供一个通用接口，那么这些特殊功能将不会被使用。例如，在带有SCSI设备的Linux中，有非常丰富的错误报告;因为其他块设备(如ATA/IDE)有更简单的错误处理，所有更高级别的软件曾经接收到的是通用EIO错误代码，因此更详细的错误消息就消失了。

因为任何插入系统的设备都需要设备驱动程序，随着时间的推移，它们已经占据了内核代码的很大比例。对Linux内核的研究表明，超过70%的操作系统代码存在于设备驱动程序中[C01];对于基于windows的系统，这个数字可能也相当高。因此，当人们告诉你操作系统有数百万行代码时，他们实际上是在说操作系统有数百万行设备驱动程序代码。当然，对于任何给定的安装，大多数代码可能都不是活动的。

我们以一个IDE驱动的实际例子来解释操作系统怎么通过驱动程序和设备交互，如下图所示。

<img src="http://1.14.100.228:8002/images/2022/03/10/20220311104443.png"  />

* ide_rw()：将一个请求排入队列，或者直接发送到磁盘
* ide_start_request()：向磁盘发出请求，用in和out指令来读写设备寄存器
* ide_wait_ready()：用于确保启动器就绪 
* ide_intr()：中断处理，发送切换进程信息

我们来做一个小结，这次学习了操作系统和设备架构和设备驱动程序，一个顶层设计，一个是底层原理。对于操作系统设备调用方式的优化，我们讨论了中断，对于CPU调度的优化，我们讨论了DMA。有关操作系统如何与设备交互，我们讨论了显式IO指令和内存映射两种方式，通过这一连串的方法，我们能够以以一种与设备无关的方式来构建操作系统的其余部分（这就是设备驱动程序的核心功能）。

## 磁盘驱动

想到操作系统的持久化，很多人首先想到的是文件系统，然而从硬件角度，持久化的基本是硬盘。因此，在构建文件系统之前，首先要里了解存储文件的磁盘是怎么驱动的。

硬盘的接口很简单，对于系统来说硬盘的存储空间是由一个个扇区（512 bytes）组成，每个扇区都能够读和写。这一个个扇区组成的列表就是硬盘的**地址空间。**对于硬盘空间有一个知名的假设，对于连续空间的读写是速度最快的读写操作，这种假设的根据我们在后面会提到。

我们首先了解一下机械硬盘的硬件组成，硬盘中最核心的部位是**盘片**，盘片是一个圆形的硬表面，通过诱导磁变化将数据持久地存储在上面。一个磁盘可以有一个或多个盘片；每个盘片有两面，每一面都被称为面。这些盘片通常由一些硬材料(如铝)制成，然后涂上一层薄薄的磁性层，使驱动器即使在断电时也能持久存储比特信息。

盘片都被绑在一起围绕着**主轴**，主轴与一个电机相连，电机以恒定的速度旋转盘片。转速通常以转速**每分钟(RPM)**来测量，典型的现代RPM值在7,200RPM到15,000RPM范围内。数据以扇区的同心圆在每个表面进行编码；我们称这样一个同心圆为**轨道**。一个单一的表面包含成千上万的轨道，紧密地排列在一起，数百条轨道的宽度相当于一根头发。读写过程是由磁头来完成，驱动器的每个表面都有一个这样的磁头。磁盘头附着在单个磁盘臂上，该臂在表面移动，将磁盘头定位在所需的轨道上。

<img src="http://1.14.100.228:8002/images/2022/03/12/20220312221314.png" style="zoom:80%;" />

现代盘片下有数百万个轨道，了解了硬盘的物理模型后，我们看一个多轨盘片下的IO请求处理流程，如果磁头想要读写某个扇区的数据，会经历**变轨-旋转-传输**三个过程。通常来讲其中变轨开销很大，因为磁头要抬头-旋转-再低头。但是现代硬盘，变轨和旋转两个过程的开销可以做到几乎相同。

硬盘中各个结构还有一些其它特性，首先各个轨道并不是对称的，因为磁头的转动需要时间，一些倾斜可以做到变轨也能读取连续扇区。然后是外部轨道往往比内部轨道有更多的扇区，每个分区在每个轨道上有相同数量的扇区，外部区域比内部区域有更多扇区。最后，任何现代磁盘驱动器的一个重要部分是它的缓存，通常是8或16MB，例如，当从磁盘读取一个扇区时，驱动器可能决定读取该磁道上的所有扇区，并将它们缓存到内存中，这样做可以使驱动器快速响应对同一轨道的任何后续请求。还有一些其它非性能行为，例如，当数据还在内存里面，数据读写完成的报告就已经返回给系统了，这些在这里不表。

<img src="http://1.14.100.228:8002/images/2022/03/12/20220312221954.png" style="zoom:80%;" />

如何理解磁盘性能呢？我们提出IO性能的概念，其计算公式如下。

<img src="http://1.14.100.228:8002/images/2022/03/12/20220312222129.png" style="zoom:80%;" />

我们以两种情况下的IO请求来说明现代磁盘IO性能的到底是怎样的。两种IO分别是随机工作负载，即将少量(例如，4KB)的数据读取到磁盘上的随机位置，和顺序工作负载，即连续地从磁盘读取大量扇区，而不需要跳转。我们用两款不同定位的硬盘来分别进行实验。硬盘参数和实验结果如下。

<img src="http://1.14.100.228:8002/images/2022/03/12/20220312222703.png" style="zoom:80%;" />

可以看出在随机和顺序工作负载之间的性能存在巨大差异，Cheetah的差异接近200倍左右，Barracuda的差异超过300倍。由此可知，尽量大块的传输数据对读写速度提升帮助巨大。

IO请求在实际运行之前需要进行调度，具体的，给定一组I/O请求，**磁盘调度程序**检查这些请求并决定下一步调度哪个请求。由于我们可以很简单地猜测一个磁盘请求将花费多长时间，因此，磁盘调度器在其操作中将尝试遵循SJF(最短作业优先)的原则。

第一种尝试是SSTF（Shortest Seek Time First），SSTF根据跟踪对I/O请求队列进行排序，选择最近的跟踪上的请求先完成。例如下图的模型。看起来不错，但是SSTF有两个问题，第一个是操作系统无法看到硬盘内部信息，相反，它看到的是一个块数组，所以操作系统可以简单地实现最近邻块优先(NBF)，而不是SSTF, NBF调度接下来使用最近邻块地址的请求。第二个问题是starving，如果对于当前头部的位置有一个稳定的IO请求，那么其它所有的IO请求都会被完全无视。

针对这些问题有过一些SSTF的变体，如F-SCAN，S-SCAN等，但是这些都没有完全解决SSTF的问题，相反的，它们也忽视了旋转的开销。

第二种尝试是SPTF，它会“视情况而定”来选择先处理哪个扇区的读写，这取决于寻找的相对时间和旋转的相对时间。在现代磁盘上，查找和旋转基本上是同等开销的，这取决于具体的请求，SPTF重点解决了旋转的开销问题，但是似乎没有解决starving问题，这里我们要提出**IO请求的调度**这个概念。

磁盘调度在哪里执行?在旧的系统中，操作系统完成所有的调度，但是由于不清楚磁盘情况，操作系统实现SPTF这样的方法比较困难，而在现代系统中，磁盘可以容纳多个未完成的请求，并且具有复杂的内部调度器（可以精确地实现SPTF）。OS调度器通常选择它认为最好的几个请求(比如16个)，并将它们全部发送到磁盘。然后，磁盘利用其内部对磁头位置的了解和详细的轨道布局信息，以尽可能好的顺序(SPTF)为上述请求提供服务。由此，解决了starving问题。

<img src="http://1.14.100.228:8002/images/2022/03/12/20220312225340.png" style="zoom:80%;" />

关于磁盘调度器还有一些方面可以讲，磁盘调度器执行的另一个重要相关任务是I/O合并。例如，假设有一系列读取block 33、block 8、block 34的请求，调度器应该将block 33和block 34的请求合并成一个单独的两个block请求，从而降低开销。而系统在向磁盘发出I/O之前应该等待多长时间？对预期磁盘调度的研究表明，有时最好是等一下其它请求再发给磁盘（具体的原因我也不太懂）。

## 冗余磁盘阵列

我们介绍了硬盘的基本模型，理论上，我们总是对磁盘有**可靠性（reliable）**的要求。这要求硬盘有**备份**的功能，不光是备份，磁盘通过备份要有**检测故障**的能力。另外，很多时候我们对**性能**有一定的要求，并且我们大多时候希望在实现以上要求的前提下能够保持可用存储空间尽量大。

基于以上要求，我们提出了Redundant Array of Inexpensive Disks（RAID）。通过RAID，系统能够将多个磁盘视作一个大的存储空间，具体的说，它将一组磁盘驱动器用某种方式联系起来，作为逻辑上的一个磁盘驱动器。通过这种抽象，RAID为系统提供了关键的**透明性**，使得不同的系统能够在不同配置的磁盘下正常工作。

> 向系统添加新功能的时候应当考虑是否可以透明的添加，而不需要对系统的其余部分进行更改，大大降低了部署的难度，可以说RAID这一性质已经隐喻了它的成功。

通过RAID串联起多个磁盘有三大好处，性能：多个同步磁盘可以加速IO时间，容量：存放更多大数据集，可靠性：允许单个硬盘丢失数据，这种分析方法在之后也会用到。一般的，RAID可以分为软件实现和硬件实现，我们主要讨论硬件实现，处于主板上的RAID很像一个小型电脑，拥有它自己的处理器，持久化RAM和所连接的磁盘，但是上面运行的并不是应用，而是专门的固件。

RAID内部处理IO的流程是，接收操作系统传输的逻辑IO请求后，由RAID转化为多个物理IO，同时在多个磁盘副本上进行读写。但RAID设计并不是用来处理IO请求，而是用于检测并且恢复磁盘故障。RAID本身比较复杂，这里我们假设故障都是很简单能够检测到，不会出现隐藏故障和不可访问的块。

评价RAID有三个方面：

* 容量，一块大小为N的硬盘实际可用空间是多少？
* 可靠性，设计可以容忍多少磁盘故障？
* 性能，RAID对于不同的负载下性能如何，差别大吗？

针对这些评估我们来分析几种常见的RAID设计思路。也有一些可量化的评估方法

* 单请求延迟：能够一定程度解释磁盘在单个IO操作中的并发性能
* 静态吞吐量：理论能够承载的最大并发带宽
* 顺序读写性能和随机读写性能：两种常见的工作负载代表在实际情况下能够表现的性能

RAID方法的命名很有意思，以级别为分隔，常见的有RAID-0、RAID-1、RAID-4、RAID-5等。

**第一种**是RAID-0，我们也称之为分条的方法，数据块在磁盘上的分布如下图所示，可以看到数据是没有冗余，即没有备份的。

<img src="http://1.14.100.228:8002/images/2022/03/13/20220314082902.png" style="zoom:80%;" />

可以看到这里有一种块大小的概念，直接影响的就是磁头在各个磁盘间定位的开销大小，块越大，磁盘定位时间越短，大部分系统的块比较大，用64KB左右大小的块。

对于RAID-0的结构分析可以知道，这种方法架构的磁盘阵列容量和性能都是十分不错的，单请求延迟也比较小，但是可靠性是最差的，因为没有任何的数据备份。

**第二种**是RAID-1，我们称之为镜像的方法，简单来讲就是选n个盘在相同位置放数据副本，具体结构如下图所示，一般镜像和分条是一起的，称之为RAID-10或者RAID-01，下图是RAID-10的方法。

<img src="http://1.14.100.228:8002/images/2022/03/13/20220314085343.png" style="zoom:80%;" />

镜像的保留了RAID-0的定位特点，多备份让镜像的方法有并发读、并发写的特点，即读可以随便读一个块，但是写要并行写进去，无论是顺序读写还是随机读写都是这样。对于RAID-1的结构分析可以知道，这种方法容量少一半，读写操作比RAID-0都要慢不少，但是可靠性很强，可以保证单个磁盘故障的检测和修复。

> 磁盘数据有备份时RAID会有持续数据更新的问题，如果磁盘在同时写入块时，一个块已经写好，此时发生断电，另一个块还没写好，这样就不同步了。解决方法就是在RAID里面放一些持久化的RAM，专门用来记录日志，就算断电，读取日志也可以知道下一步要干嘛。

**第三种**是RAID-4，也叫带有奇偶校验位的空间。最根本的改变是用一个磁盘来存放奇偶校验位，表示该条的冗余信息的正确性。如果学习过奇偶校验位的应用，这里应该很好理解。具体的，校验方法可以使用简单的XOR方法，如果这一片有偶数个1，奇偶校验位返回0，有奇数个1，奇偶校验位返回1，当有多个位时，数据在每一位上都放置奇偶校验位。具体结构如下图所示

<img src="http://1.14.100.228:8002/images/2022/03/13/20220314090149.png" style="zoom:80%;" />

这种方式使用很少的冗余数据实现了磁盘故障的检测，不过这种方法只允许一个磁盘故障，无法定位多个故障磁盘。注意，数据备份的目的就是实现磁盘故障的检测和恢复，这里本质上将每一个位的冗余数据压缩到一个位上，是一种极好的压缩方法，只不过丢失的信息有些多。

**第四种**是RAID-5，这基本上是RAID-4的原理，但是将奇偶校验位分布到不同磁盘上。因为如果将奇偶校验位都放在一个磁盘里，那么对同一的磁盘的随机写是无法同步进行的，因此如果有大量的随机写操作，RAID-4方法会出现无法容忍的延迟。

<img src="http://1.14.100.228:8002/images/2022/03/13/20220314091322.png" style="zoom:80%;" />

因此将奇偶校验位这样分开就可以做到对同一磁盘的并发随机写，有了这种提升以后，几乎现在所有的RAID-4都被RAID-5代替，还有一些基于软件的RAID可能会用RAID-4，因为系统本身知道不会进行大量随机写的操作。

我们可以对这些RAID实现方法的性能进行量化分析，如下表所示。

<img src="http://1.14.100.228:8002/images/2022/03/13/20220314091712.png" style="zoom:80%;" />

可以发现不同方法的特点都很明显，它们直接的利弊都很明显，所以可以根据设计目标简单的选出来合适的RAID方法，同时RAID还有其它等级，都有针对不同方面的内容，如针对延迟的RAID-2，和RAID-3、RAID-6等。

## 文件和目录

我们真正开始讨论文件的抽象，这也是重点部分，文件和目录的抽象是文件系统的基石，正如CPU、内存一样，文件也是系统内可共享的持久化存储资源，我们重点谈论一下UNIX文件系统中可交互的接口。

**文件与目录概述**

首先是基础概念，每一个文件都是一组二进制值，都有一个低位名称，用户通常不会注意到这个名称，这个低位名称通常会链接到inode数，即每个文件都有一个与之关联的inode号，文件系统设计之初就是就是存放并确保能够读取到数据。

然后是目录，目录抽象的内容有两种，一是文件-低级名称，一种是目录-目录。通过构建目录树，能够做到对所有文件的索引。一个磁盘就是一个目录系统，根目录就是最上层的目录，一般是\。这些概念和我们在windows中使用的文件管理器不同，文件系统是一种更加抽象宏观的设计，例如在UNIX里面，文件、设备、管道甚至进程都是由文件系统命名的。统一的命名系统让系统更简单、更模块化。再比如，统一的文件系统（一个磁盘分区）命名磁盘、u盘、CD-ROM和很多其它东西，这一切都位于一个单一的目录树下。

**文件接口**

接下来我们通过文件系统的接口处理流程来了解其工作原理

<img src="http://1.14.100.228:8002/images/2022/03/16/20220316125304.png" style="zoom:80%;" />

首先是创建文件，通过系统调用open来打开文件，函数open的描述如上图所示，可以看到open()有三个参数，第一个指定显式文件名，第二个创建文件，如果文件不存在则确保文件只能被写入，如果文件已经存在则将其截断为零个字节，第三个参数指定权限。open()返回的是一个文件描述符，文件描述符简单来讲可以看作是物理文件地址的指针，是其他文件系统调用的基础。

具体的讲，一个简单的数组跟踪每一个进程中已打开的文件，数组中每一个条目都指向一个file结构体，用于保存文件信息。这个表就是**开放文件表**，文件标识符用来打开这个表，表里保存的文件信息文件、当前偏移量、和其他相关细节，比如文件是可读还是可写。如下图所示

> 在xv6系统里面，所有这样的结构体保存在一个数组里面，并且这个数组自带一个锁。

<img src="http://1.14.100.228:8002/images/2022/03/16/20220316130146.png" style="zoom:80%;" />

> strace是专门用来查看程序使用系统调用的工具，是一个十分强大的工具。

然后是读写文件，我们知道echo和cat命令可以用来构建简单的文件读写程序。我们查看这个简单简单命令的调用链。

<img src="http://1.14.100.228:8002/images/2022/03/15/20220316093736.png" style="zoom:80%;" />

cat做的第一件事是打开文件，O_RDONLY标志指示文件只打开读(不是写)，O_LARGEFILE表示使用64位偏移量。文件在打开后返回的标识符是3，那是因为0、1、2是进程在创建时就已经打开的文件了，它们分别是标准输入、标准输出和标准错误。

之后使用read()系统调用从文件中读取一些字节，read()的第一个参数是文件描述符，从而告诉文件系统要读取哪个文件，第二个参数指向一个缓冲区，其中将放置read()的结果，第三个参数是缓冲区的大小，在本例中为4 KB。对read()的调用成功时返回它读取的字符数，这里是”hello\n“六个字符。

接下来是write调用，将字符串写入文件1，也就是标准输出流，于是字符串在用户终端显示，同样返回输出的字符串长度，最后再次调用read试图得到更多数据，但是返回0，因此结束访问，删除文件引用。之后大部分的命令都是这样一个个系统调用组成的。

明显上面提到的读写read和write都是顺序读写，我们需要一种随机读写文件位的方法，这就是lseek调用，如下图所示。

<img src="http://1.14.100.228:8002/images/2022/03/16/20220316125827.png" style="zoom:80%;" />

其中fildes就是文件描述符，offset就是字符偏移量，whence表示三种offset发挥作用的方式。对于进程打开的每一个文件，操作系统会跟踪一个偏移量，这个偏移量决定了下一次读写文件会从哪一个地方开始，这个偏移量有两种方式进行更新。一是将N个字节的读写直接加到偏移量上(自动)，二是使用lseek调用来指定偏移量。系统调用与偏移量之间的关系如下图所示。

![](http://1.14.100.228:8002/images/2022/03/16/20220316130642.png)

很多时候，不只是多个进程会同时访问文件系统，多个线程也需要并发访问文件，但是如果这些线程都使用同一个文件描述符的话，它们就只能在相同的偏移量上进行读写。因此，系统提供了一个dup调用用来复制文件描述符，具体过程如下图所示。通过操作不同文件描述符，可以读取不同的偏移量，可以实现不同线程读写文件不同的区域。

write调用的其中一个特性是，先返回写入成功，再后台延迟写入。而有时我们需要让写入硬盘立即执行。因此我们可以使用fsync调用，fsync调用强制所有脏数据立刻写入磁盘。这会造成操作系统对脏数据的管理出现问题，但是增强了写入数据的安全性。

<img src="http://1.14.100.228:8002/images/2022/03/16/20220316163618.png" style="zoom:80%;" />

有时我们想要对文件进行重命名，我们可以用mv命令，而mv命令实际调用了rename，rename有操作上的**原子性**，这是为了防止系统崩溃造成的命名错误，从旧名到新名一步完成，不会有中间状态。具体的过程如下图所示

<img src="http://1.14.100.228:8002/images/2022/03/16/20220316170025.png" style="zoom:80%;" />

文件如此复杂，我们希望在文件系统中保存文件有关的所有数据，这种数据称为文件元数据。如果想要调用查看文件元数据，可以使用stat或者fstat调用来查看。具体的信息如下图所示。

<img src="http://1.14.100.228:8002/images/2022/03/16/20220316205352.png" style="zoom:80%;" />

这些元数据都存储在inode指向的地方，还有一份inode拷贝留在内存里面加速文件访问。

如果我们要删除文件，可以使用rm命令，照例我们查看rm命令的调用，发现它只是简单的调用了一个**unlink**函数。它的作用原理我们需要结合目录系统来一起讲。

**目录接口**

和文件一样，有一组与目录相关的系统调用能够创建、读取和删除目录。但是和文件不同的是，永远不能直接写入目录。因为目录的格式被认为是文件系统元数据，文件系统认为应该负责目录数据的完整性，只能通过创建文件、目录或其他对象类型等方式间接地更新目录。也就是说，目录是一种封装性强的结构。

特别的，一个空目录会有两个条目，一个引用自身，一个引用其父目录，使用`ls -a`可以在空目录中查看到`./ ../`两个文件，它们同时也是可以访问的。

ls命令也是读取目录的主要方式，照例我们查看ls的调用链，发现三个调用:opendir()、readdir()和closedir()，读取的信息来自于目录为其包含文件构造的结构体，称为目录表，目录表包含的信息比较简单，只有文件名和文件索引一点点，因此所以程序可能会对每个文件调用stat()，以获取每个文件的更多信息，这也就是`ls -l`的底层原理。

<img src="http://1.14.100.228:8002/images/2022/03/16/20220317095355.png" style="zoom:80%;" />

删除目录也是我们常用的一项操作，一种简单的调用是`rmdir()`，但是由于安全考虑，rmdir只能删除空目录。如果想要删除带有文件的目录，也就是删除所有文件，就需要了解删除文件和目录的关系。我们之前讲过删除文件实际调用的是`unlink()`函数，而与之相对的是`link()`，一种在文件系统树中创建条目的方法，该调用有两个参数，一个旧路径名和一个新路径名。另一种说法是，系统调用link，也就是命令`ln`，用来创建文件的引用关系。

**链接**

其底层原理就是将多个文件名链接到同一个文件名上，如下图所示，file和file2的inode是相同的，结合起来看，创建文件时，创建一个inode结构，同时将文件名硬链接到文件，因此rm的unlink都是删除文件名，而不是删除文件。

与这个机制对应的是文件系统会统计某个文件的引用个数，即reference count，当调用link时，引用数加一，调用unlink时，引用数减一，只有当引用计数达到0时，系统才会彻底删除文件。如下图所示。

<img src="http://1.14.100.228:8002/images/2022/03/16/20220317100610.png" style="zoom:80%;" />

 上面的链接方式有一个专门的名称，硬链接。相对的还有一种方式，即软链接，也叫符号链接，提出这一概念的原因是硬链接是有限制的，硬链接不能在同一个目录里面，因为有可能创建循环链接，也不能跨盘，因为inode号只在特定的文件系统中是唯一的。

创建软连接的方式也是使用ln命令，不过要加上-s标识符，即`ln -s`。如下图所示，软链接和硬链接不同，不是链接到同一个inode号，它实际上是一个路径，相当于原文件的另一个引用，删除文件演示也说明了这一原理。

<img src="http://1.14.100.228:8002/images/2022/03/16/20220317102508.png" style="zoom:80%;" />

**其它**

我们知道文件这一抽象资源是各种进程共有的，因此需要更严格的权限管理，在UNIX文件系统这种权限管理是通过权限位来实现的，如下图所示。

<img src="http://1.14.100.228:8002/images/2022/03/16/20220317103408.png" style="zoom:80%;" />

在上面的示例中，ls输出的前三个字符表明该文件的所有者(rw-)可以读和写，只有群组的成员和系统中的其他任何人(r——后跟r——)可以读。文件的所有者可以很简单的通过`chmod`命令更改文件访问模式，设置方法很简单，以`chmod 755`为例，7代表前三位设为111，即可读可写可执行，5表示中间三位设为101，表示可读可执行，另一个5同理。

> 在更复杂的文件系统里，还有一种更复杂的**访问控制列表**允许更精确地控制谁可以访问和操作信息。分布式文件系统AFS用ACL访问控制列表来管理用户权限

我们已经介绍了文件、目录和特殊类型的链接的接口，我们可以看出，文件系统的构建无非就是使用接口将文件和目录一个个挂载到系统中，如果想创建一个文件系统，可以使用`mkfs`调用，简单输入设备和文件系统类型。就能够创建一个空文件系统。创建出的文件系统不可能是孤立的，必须要挂载到已有的文件系统上，这要使用`mount`命令，如下图所示。

<img src="http://1.14.100.228:8002/images/2022/03/17/20220317111042.png" style="zoom:80%;" />

概念比较多，我们来做一些总结：

* 文件是一个字节数组，它可以被创建、读取、写入和删除。它有一个唯一引用它的低级名称(即数字)。低级别的名字通常被称为i-number。
* 目录是元组的集合，每个元组包含一个人类可读的名称和它映射到的底层名称。每个条目都指向另一个目录或一个文件。每个目录本身也有一个低级名称(i-number)。
* 访问文件后，操作系统会返回一个文件描述符，根据权限和意图的允许，这个文件描述符可以用于读写访问。
* 每个文件描述符是一个私有的、每个进程的实体，它引用了打开文件表中的一个条目。其中的条目跟踪该访问指的是哪个文件，文件的当前偏移量(即，下一次读写将访问文件的哪一部分)，以及其他相关信息。
* 调用read()和write()自然地更新当前偏移量;否则，进程可以使用lseek()来更改它的值，从而允许对文件的不同部分进行随机访问。
* 要在文件系统中使用多个人类可读的名称来引用同一个底层文件，要使用硬链接或软链接。每种方法在不同的情况下都是有用的，所以在使用之前要考虑它们的优缺点。请记住，删除文件只是在目录层次结构中执行最后一次unlink()操作。
* 大多数文件系统都有启用和禁用共享的机制。这种控制的基本形式是由权限位提供的；更复杂的**访问控制列表**允许更精确地控制谁可以访问和操作信息。

## 实现文件系统

在有了前面那些知识的铺垫后，我们已经能构建一个非常简单的文件系统了，并且文件系统是纯软件，所以不需要学习额外的硬件支持。构建文件系统有很大的灵活性，所以没有明显的设计规范，这一点会在我们的构建过程中表现出来。

文件系统有两大设计结构，数据存储结构和数据访问方法，现代文件系统一般用比较复杂的树结构作为文件的存储结构，访问方法的意思就是当调用open()、rea()、write()时，它们的访问路径是怎样的。更直接的说，这些系统调用是怎么实现的。

在深入上面的结构之前，我们先看一看文件系统的总体结构是怎样的，简单来说如下图所示

![](http://1.14.100.228:8002/images/2022/03/19/20220319210848.png)

上图是一个最简单的文件系统，每一块4KB，一共64个块，这些块里最多的是D块即数据块，存储最多的是用户数据，称为数据区域。还有一个inode结构跟踪每个文件的信息，比如有哪些数据块，文件的大小、访问权限等，存储在I块即inode块。我们也可以把这一部分称为inode表，inode表大小表示文件系统可以存放的最大文件数量，这是可以动态分配的。我们还缺一种用来跟踪I块和D块是否空闲或者是否已分配的结构，我们采用一种简单的方法，也就是维护一个i块即inode bitmap块和d块即data bitmap块，它们结构类似都很简单，简单结构:每个位用来表示对应的对象/块是空闲的(0)还是正在使用的(1)。最后一个块我们用来存放超级块，用来存储这一整个文件系统各方面的信息，如文件系统的inode和数据块数量，inode表从哪里开始读取，以及用来表示文件系统类型的超级数。这些就是文件系统的基本结构，大多数文件系统都会有类似的结构但是可能会有不同的名字。

**文件的组织方式**，文件系统中最重要的磁盘结构之一是inode，每一个inode在系统中都有一个独有的i-number。每个inode里面实际上是你需要的关于一个文件的所有信息：它的类型(例如,常规文件,目录,等等)，它的大小,分配的块的数量，保护信息(如谁拥有这个文件，以及谁可以访问它)，一些时间信息，包括文件时创建，修改，或上次访问，以及其数据块的信息驻留在磁盘(例如,指针之类的)。我们把关于一个文件的所有这些信息称为元数据（metadata）

<img src="http://1.14.100.228:8002/images/2022/03/19/20220319211558.png" style="zoom:80%;" />

inode中还存储着直接可以索引到文件所在块的指针，它是我们读取文件的直接方式。如果一个指针只指向一个块，那么大文件将需要非常多的指针来索引，这不利于我们的设计，我们需要一种能够灵活处理大小文件的方法

1. 一种简单的压缩思路是在众多直接指针之外设立一个间接指针，指向更多直接指针，当直接指针不够用的时候就用间接指针，
2. 一种更加理想的方式是基于区段，一个磁盘指针加上一个长度，问题是很难找到那么多连续的空闲磁盘块

一般我们会认为第一种方法更加常见，这种方法统称为多级索引方法，本质上是不平衡的树，这种结构就是为了适应文件小的很小，大的很大的特点，很多文件系统使用这种索引方式，这大大提升了inode指向块的数量，一个三级指针可以将4KB的块索引扩大到4GB的块索引。

![](http://1.14.100.228:8002/images/2022/03/19/20220319213747.png)

**目录的组织方式**，目录本身的结构比较简单，就是一个包含（名字、inode号）对的列表，对于给定目录中的每个文件或目录，在该目录的数据块中有一个字符串和一个数字。对于每个字符串，也可能有一个长度(假设名称大小可变)。

<img src="http://1.14.100.228:8002/images/2022/03/19/20220319214208.png" style="zoom:80%;" />

其中reclen是record length的意思，貌似是在删除文件时标记用的。

目录在文件系统中被视为一种特殊类型的文件，因此也是用inode进行索引存储，该inode的类型字段被标记为”目录“而不是”常规文件“，简单的线性列表并不是存储目录信息的唯一方式，事实上，更加精细的现代文件系统会用树结构来存储目录信息，如在XFS里面是B+树，这让文件创建更快。

> 我们用链表来存储文件块索引，有时这种方式十分低效，因此老FAT会在内存里保存链接信息表，要分配文件空间的时候就查表

**空闲空间管理**，文件系统必须跟踪哪些inode节点和数据库是空闲的，在VSFS中，我们的解决方案是提供两个简单的bitmap，在这种方式下分配一个新的数据块的流程是：分配inode——更新inode bitmap——更新on-disk bitmap——分配数据块，而位图只是管理空闲磁盘的一种方式，现代系统可能用更精细的结构如B-tree，同样其它空间管理方案也会有类似的一系列活动。

在分配数据块的时候，一些更精细的系统如ext2和ext3，会在分配的时候将空间分的更连续一些，保证文件的一部分在硬盘上是连续的，从而提高性能。

**访问路径**，接下来我们结合上面的内容，看一下实际在一个简单的文件系统中读写文件会发生的一系列活动有哪些，是怎样组织的。

首先是读取文件，我们以读取名为bar的文件为例，读取路径为`/foo/bar`，文件大小为12KB（3个数据块），具体过程可以看下面的时间轴。任何读取首先都是在root节点开始的。

<img src="http://1.14.100.228:8002/images/2022/03/20/20220320151208.png" style="zoom:80%;" />

然后是写入文件，同样是写入名为bar的文件为例，读取路径为`/foo/bar`，文件大小为12KB（3个数据块），具体过程可以看下面的时间轴。

<img src="http://1.14.100.228:8002/images/2022/03/20/20220320151254.png" style="zoom:80%;" />

可以看出来写入操作产生的IO次数比读取要多不少，这样常见的操作产生的IO流量比较大，我们需要减少文件系统IO成本的方法

**缓存和缓冲**，读写开销大，但是可以发现都是在一些相同的块中反复读写，因此我们考虑使用缓存的方式来减少IO开销，早期系统用固定大小的缓存页来缓存块，也就是在内存中开辟一块固定大小的缓存区，这样可能造成浪费。所以现代系统将虚拟内存页和文件系统页统一到一个页缓存中，可以在内存和文件缓存之间灵活的分配内存。

> 动态的方法和静态的方法各有千秋，静态分区可以确保每个用户都能获得一定份额的资源，通常提供更可预测的性能，而且通常更容易实现。动态分区可以实现更好的利用(通过让需要资源的用户使用空闲资源)，但实现起来可能更复杂，并且可能会导致用户性能下降，因为这些用户的空闲资源被其他人使用，然后需要时需要很长时间才能回收。

缓存对读操作的影响不言而喻，对写操作也有很重要的作用，首先，通过延迟写操作，文件系统可以将一些更新批处理到较小的I/ o集合中；其次，通过在内存中缓冲大量的写操作，系统可以调度随后的I/ o，从而提高性能；最后，有些写操作通过延迟完全避免。这三点是逐渐推进的。

> 有些数据库不想数据写入操作有延迟，因此调用fsync强制写入，虽然大多数应用程序都需要通过文件系统进行权衡，但如果统一的调度不能令人满意，则也有足够的控制来让系统执行您希望它执行的操作。

## 快速文件系统

Fast File System快速文件系统是早期UNIX文件系统的改进版，早期的UNIX文件系统十分简单，如下图所示

![](http://1.14.100.228:8002/images/2022/03/24/20220324184321.png)

问题是，性能很糟糕，到后面这个文件甚至不能提供磁盘总带宽的2%，而且用久了空间碎片化很严重，因此浪费很多磁盘空间。本质上，这个文件系统只是把磁盘当作一个随机访问的内存，在磁盘的使用过程中，空间的清理和分配都不是连续的，索引在空间中乱转，因此性能十分的差。如下图所示。

![](http://1.14.100.228:8002/images/2022/03/24/20220324184950.png)

从现在的眼光来看，这种碎片化的问题能够通过磁盘碎片整理工具来重新组织数据，但是在当时来说这种开销是不可接受的。另一个问题是一个块太小，小的块对碎片化的问题有好处，但是对传输来说会线性的增大开销，这些问题导致这种文件系统难以使用。

伯克利的一个小组决定建立一个更好、更快的文件系统，他们称之为快速文件系统(FFS)。从此开启了文件系统研究的新时代，FFS的第一个特点就是保留文件系统的相同接口(相同的api，包括open()、read()、write()、close()和其他文件系统调用)，但是在内部实现上大大改进。这种开发模式沿用至今，实际上，所有现代文件系统都遵循现有的接口(从而保持与应用程序的兼容性)，同时出于性能、可靠性或其他原因更改其内部结构。

第一步就是更改磁盘上的结构，重新定义磁盘结构为一组cylinder，一个cylinder是不同盘片上平行的一条轨道。下图中，一共有3条cylinder。

![](http://1.14.100.228:8002/images/2022/03/24/20220324191359.png)

但是这种结构并不是易用的，磁盘导出一共基于块的逻辑地址空间，并从客户端隐藏其几何结构。具体的，将逻辑地址空间相对应的分组，一组上的逻辑地址空间由一组cylinder上平行的物理空间组成，每一个组内都有一个和普通文件系统一样的数据结构，如下图所示

<img src="http://1.14.100.228:8002/images/2022/03/24/20220324192157.png" style="zoom:80%;" />

这样分组的意义在于分配文件和目录空间，事实上，分配只有一个原则：把相关的东西放在一起，这里我们用一些简单的启发式方法来定义什么东西是相关的，对于目录数据，找到已分配目录比较少、空间比较大的组(方便组间平衡目录)，全部放进去，对于文件数据，首先是放到对应的inode附近，其次是相同目录的文件放到同一组里面。如下图所示

![](http://1.14.100.228:8002/images/2022/03/24/20220324192947.png)

上图左边是按关联原则分配的空间，右边则没有，可以看出右边的存放方式使基于名称的局部性消失了。而左边因为具有局部性，使得相关文件的索引很快。

>启发式方法：人在解决问题时所采取的一种根据经验规则进行发现的方法。其特点是在解决问题时,利用过去的经验,选择已经行之有效的方法，而不是系统地、以确定的步骤去寻求答案。

**名称局部性**，如下图所示基于名称的局部性的优势可以体现

<img src="http://1.14.100.228:8002/images/2022/03/24/20220324193522.png" style="zoom:80%;" />

对于大文件要特殊处理，不然可能会占满整个组，破坏局部性，如下图左侧所示，没有多少剩余空间给其它文件了。解决的方式也很简单如右图所示。

![](http://1.14.100.228:8002/images/2022/03/24/20220324195735.png)

有人可能会注意到这种分散的存储方式会降低性能，因为需要在不同的组上来回切换。解决方案是增大块大小，因为假如块足够大，文件分散的组就少，这种方式也称为摊销(amortization)，文件系统将会花费大量时间在磁盘传输上而不是搜索磁道。也就是说，峰值性能越高，所需的块大小越大，如下图所示。

![](http://1.14.100.228:8002/images/2022/03/24/20220324200235.png)

FFS还有一些其它特性，比如极小块，因为大部分文件是很小的，为了减少内部碎片，引入小块的概念，这些小块都用于存储小文件，一块可能只有256KB或者512KB。另一个是磁盘布局，如下图所示，FFS将首先向0块发出读取;当读取完成时，FFS发出对block 1的读取，已经太晚了，block 1已经在头部下旋转，现在读取block 1将会引起一个完整的旋转。因此FFS采取右边的磁盘布局来解决这一问题，

![](http://1.14.100.228:8002/images/2022/03/24/20220324201357.png)

但是如果是连续读取的话只能发挥峰值性能的50%（要转两圈），幸运的是，现代磁盘要聪明得多:它们内部读取整个磁道，并将其缓冲到内部磁盘缓存中(由于这个原因，通常称为磁道缓冲区)。然后，在对磁道进行后续读取时，磁盘将从其缓存中返回所需的数据。

还有一些其它的特性，如长文件名、符号链接、原子重命名等，这些特性使得FFS不仅性能更强而且更加可用。

## 系统日志

我们已经讨论了文件系统各种数据结构的实现和，如文件、目录、以及其它元数据。但是不像其它的数据结构，文件数据必须持久化，即使在断电的过程和断电之后都能够保持安全，假如在传输数据的过程中断电，应该怎么办？这就是我们常说的crash-consistency problem，即崩溃一致性问题。

如何在崩溃的情况下依然安全的更新磁盘数据？操作系统可以在任意两个写操作之间崩溃，系统重启后希望上一次没有更新成功的数据再一次更新入磁盘中，关于这一方面我们主要讨论的是日志记录的方法，不过首先讲解一下老系统的方法，也就是文件系统检查器。

我们直接以一个例子开始，以一个典型的4KB追加写工作流为例，调用lseek()将文件偏移量移动到文件的末尾，然后对下列的结构进行操作

![](http://1.14.100.228:8002/images/2022/03/28/20220328145113.png)

和我们之前学的一样，仅写入一个文件就会对三个结构进行更改，data bitmap、inodes和data block，但是一般写入操作在内存中驻留5-30秒之后再进行，三个写操作大概不是同步的，因此如果发生崩溃，即使任何一次写操作失败都是可能引发磁盘错误，具体的，假如只有实际的数据块写入成功，则问题不大，因为inode和bitmap都还没有更新；只有inode更改成功的话会造成文件系统不一致问题，因为bitmap告诉我们数据块5还没有分配，而inode告诉我们数据块5已经分配了。并且inode指向的是垃圾数据；只有bitmap写入成功，则会造成空间泄露问题，文件系统永远不会再使用第5块。其它的情况也类似，总之，崩溃会使文件系统结构处于混乱的状态，我们提出的解决方案必须覆盖所有的问题场景。

早期系统采用了一个麻烦的解决方案，File System Checker（FSCK），就像一个大型的修复工具包一样，fsck对任何有可能出现错误的地方进行排查，包括superblock、bitmap、inode和datablock。甚至是相关的inode，做法就是一步步的排查。这种做类似于把钥匙掉在卧室的地板上，然后开始搜索整个房子的钥匙恢复算法，从地下室开始，逐一搜索每个房间。这很有效，但很浪费。每次崩溃之后排查整个磁盘可能会花费数分钟甚至数小时。

现代系统从数据库管理系统中吸取灵感，采用一种称为日志系统或者提前写日志的方法。第一个这样做的文件系统是Cedar，许多现代文件系统使用了这种思想，包括Linux的ext3和ext4、reiserfs、IBM的JFS、SGI的XFS和Windows的NTFS。

我们从Linux ext3入手，看看日志系统是怎样实现的，简单来说，日志就是在写入操作的时候将一部分的写入信息存储在journal块中，用于缓存写入操作。每个写入操作都是一个事务transaction，一个事务就是一次更新的总体，内部包含有关这次更新的全部信息。

![](http://1.14.100.228:8002/images/2022/03/28/20220328194509.png)

但是这样有一个问题，如果写入日志是一个个进行的，就很缓慢。所以我们希望成批的写入日志来提高效率，但是如果在这一过程中发生崩溃的话，则在恢复后重写进行操作时将错误信息也写入磁盘，这相当于没有日志。如何处理这个问题呢？①在ext4中，日志的写入是成批的，但是会在写入后总体更新前进行校验，如果校验不通过，则将这一段的更新全部抛弃。②而在ext4之前，日志的写入是分两阶段的，先将除TxE以外的所有消息写入日志块，确认可以完成写入后将TxE写入日志，这一阶段叫做日志提交。

![](http://1.14.100.228:8002/images/2022/03/28/20220328201438.png)

我们也可以利用日志来提升写入操作的效率，具体的说，对相同位置的重复写入是很可能发生的，因为文件的改动通常都在同一个文件夹下进行。因此我们可以设立一个全局事务，缓冲所有更新，但是仅将数据块标记为dirty，将多个更新同时写入到块中。

日志的写入空间应当是有限的，当某一个事务完成后，应当进行标记，并在新事务进入时进行替换。因此我们可以设计一种日志超级块，并且在总体的workload中加入一个free步骤。具体的，加入超级块有两个作用：①检查哪些事务还没有被checkpointed减少恢复时间
②记录哪些事务是free的，以提供后续更新覆盖

![](http://1.14.100.228:8002/images/2022/03/28/20220328201355.png)

对于更新过程依然有一个问题，如果每一个写入操作都要在日志中进行缓存，那么磁盘写入速度将大打折扣，因为相当于每一次数据都要写入两次。为此，我们引入元数据日志记录，意思是占磁盘IO最大的数据块写入不进行两次，只进行一次直接写入，而其它的写入则要先缓存在日志中。这样做确实有效，但是我们又要解决另一个问题，什么时候写入数据块才能保持一致性？

一些文件系统的做法是将数据块的写入作为整个workload的开头，保证数据块的写入完成是一次完整更新的前提，如果数据块写入失败，则整个更新会被直接放弃。这种方式被称为**元数据日志系统**。在大多数系统中，元数据日志记录(类似于ext3的有序日志记录)比完整数据日志记录更受欢迎

> 在向日志(步骤2)发出写操作之前强制完成数据写操作(步骤1)是不需要正确性的，如上面的协议中所示，意味着它们的顺序不需要固定，也可以并发执行，这一点在之后会讲到。

最后一个需要解决的问题是，当崩溃发生并恢复时，整个日志中的更新会被重新执行一次，但是明显这是有问题的，先不说很多事务本身是已经完成的，也有很多事务本身是冲突的。这个问题有很多解决方案。例如，对于一个块可能永远不会重用，直到该块的删除被替换出日志，Linux ext3所做的是将一种新类型的记录添加到日志中，称为撤销记录。当重播日志时，系统首先扫描这些撤销记录；任何此类被撤销的数据都不会重放，从而避免了上述问题。

现在我们总结一下日志系统工作的全流程：

1. 数据写入：将更新写入数据块，系统等待写入完成（或者不等）；
2. 日志元数据写入：将开始块和元数据写入日志块，系统等待写入完成；
3. 日志提交：将结束块写入日志块，系统等待写入完成；
4. 检查元数据：将元数据写入磁盘，系统等待写入完成；
5. 释放空间：将事务标记为已完成，并释放空间；

如果从时间线来看的话，可以画出如下图所示：

![](http://1.14.100.228:8002/images/2022/03/28/20220328203429.png)

当然关于一致性问题不止有这两个解决方案，还有：

* Soft Updates：这种方法小心地安排对文件系统的所有写操作，以确保磁盘上的结构永远不会处于不一致的状态。软更新需要对每个文件系统数据结构有复杂的了解，因此给系统增加了相当多的复杂性。
* Copy-On-Write：它被用于许多流行的文件系统，包括Sun的ZFS [B07]。这种技术永远不会覆盖现有的文件或目录;相反，它将新的更新放到磁盘上以前未使用的位置。在完成多次更新之后，COW文件系统会翻转文件系统的根结构，以包含指向最新更新结构的指针。
* Backpointer：系统中的每个块都添加了一个额外的反向指针;例如，每个数据块都对它所属的索引节点有一个引用。当访问一个文件时，文件系统可以通过检查forward指针(例如，inode或direct块中的地址)是否指向指向它的块来判断该文件是否一致。
* optimistic crash consistency：通过使用广义的事务校验和的形式，尽可能多地对磁盘进行写操作，并包括一些其他技术，以检测出现的不一致性。对于某些工作负载，这些乐观技术可以将性能提高一个数量级。然而，要真正发挥作用，需要一个稍微不同的硬盘接口。

## 日志结构文件系统

  承上一章节的内容，这次我们讨论“Copy-On-Write”类型的文件系统，也称作日志型文件系统，它最初的开发是基于以下事实：

* 系统内存增长，文件系统的性能很大程度上取决于写性能，而写入操作的大小会使用到大量内存；
* 随机IO与顺序IO性能差距很大，如果能以顺序的方式使用磁盘，能够获得巨大的性能优势；
* 现有的文件系统在许多常见的工作负载上表现不佳，FFS会导致短寻路，和很多的旋转延迟；
* 不支持RAID，具体的，RAID-4和RAID-5都有small-write-problem，现有的文件系统不会试图避免这种最坏情沉下的RAID写行为。

> 高效的系统思想往往都很简单，真正产生作用的是不计其数的细节，真正构建一个工作系统，必须考虑所有棘手的情况。正所谓，the devil is in the detail

实现利用顺序写性能优势的文件系统是Log-structured file system，简称LFS，也称为日志型文件系统。LFS 首先在内存段中缓冲所有更新（包括元数据），这样的一组缓存数据称作segment，当segment已满时，它被写入磁盘，通过一长段**顺序写入**到磁盘的一个未使用的部分。也就是说，LFS 从不覆盖现有数据，而是总是将段写入空闲位置。因为段很大，所以可以有效地使用磁盘（或**RAID**），由此文件系统的性能达到了顶峰。

如何将文件系统的所有写入都转换为一系列对磁盘的顺序写入？由一个最简单的写入来说，将数据块写下之后，将inode写在它旁边，这种同一次写入的数据挨着写的做法就是LFS的基础思想。仅这一点，就和我们之前所架构的FFS有根本的不同。

在这种情况下，我们必须要对写入进行缓存，为什么呢？试想一下，假如两个紧挨着的块是分两次写入的话，很有可能在中间会遭到旋转，仔细想想！我们需要将大块的顺序写数据缓存到内存中。segment越大，写入操作就越高效，试想一下，假如完全不需要变道，写入的效率就是磁盘传输的带宽，这是非常快的。

![](http://1.14.100.228:8002/images/2022/03/29/20220329220404.png)

虽然说segment越大越好，但是实际上缓存多少是合适呢？其实这无非就是一个计算多大缓存能够将定位开销摊开（摊销）的问题，具体的计算过程不重要，举例来讲，一个定位时间为10ms，峰值带宽为100MB/s的磁盘，如果想达到90%的有效带宽。按照计算就大概需要9MB大小的buffer。

在以前的FFS中，inode被组织在一个数组中，但是在LFS中，inode是随意写入的，我们已经把索引节点分散到整个磁盘上了！更糟糕的是，我们从未在适当的位置覆盖，因此最新版本的inode(即我们想要的那个)一直在**移动**。

解决问题的关键在如何找到inode，我们直接再加一个间接层，具体的，再添加一个imap结构，始终指向inode的最新位置，当inode位置更新，它所指的位置跟着更新。那么imap应该在磁盘的什么位置呢？它当然可以存在一个固定的位置，但是这样和inode就没有什么区别了，依然会增大磁盘开销。因此，我们在每次更新文件时将imap更新到新数据写入位置的附近。

>我们常可以发现，间接的方法非常实用，只需要增加一点点的开销就能换取更好的架构

但是总是需要一个固定位置的数据让磁盘开始扫描的，这个数据结构就是checkpoint-region，简称CR，它记录各个文件所占块的大小和imap所在位置。因此，在LFS中读写一个文件的流程就是CR->imap->inode->datablock。具体如下图所示

![](http://1.14.100.228:8002/images/2022/03/29/20220329223935.png)

我们只讨论了文件在LFS中的变化，那么目录呢，幸运的是，目录的结构基本上与经典款相同，因此不需要多做改变。如下图所示。

> 我们通过LFS还顺带解决了另一个问题，通过inode映射，即使inode的位置可能会改变，但是这种改变永远不会反映在目录本身，也就是说不会出现目录的递归式更新问题。

![](http://1.14.100.228:8002/images/2022/03/29/20220329224852.png)

接下来进入到一个重头戏，在LFS里面，系统写入新数据时只在磁盘空余的位置写，也就是说旧版文件都是垃圾文件，需要进行收集并回收。举例来说，典型有如下图所示的两种垃圾数据。

![](http://1.14.100.228:8002/images/2022/03/29/20220329225304.png)

一种处理方式是留着这些数据，并允许用户恢复旧的文件版本(例如，当他们不小心覆盖或删除一个文件时)，这样做会非常方便，这样的文件系统称为版本控制文件系统，因为它跟踪文件的不同版本。

但是对于LFS来说，只有最新版的文件是有用的，所以需要设计一种能够收集并清理旧版文件的功能，首先能够想到的就是定期扫描整个磁盘，然后将所有的垃圾文件清除。但是不说这样性能很差的问题，如果是一个个清除的话，会有很多内部碎片产生。因此我们的策略是利用segment，定期的把各个段少量的不可覆盖数据汇总起来，再写入新的段，之前的所有空间就可以覆盖了。这样一段一段的清除垃圾数据能够最大程度的利用磁盘的传输带宽。

因此问题就落在了怎样检测哪些块是可覆盖的，而哪些是不可覆盖的。我们再次添加一个间接结构，在每个段的头部添加一个segment summary block，用于记录段中每个数据块D，inode号（它属于哪个文件）和它的偏移量（它是文件的哪个块）。如果想查看一个块是否可覆盖，就只需要在正常的查找过程中添加一个和segment summary block，进行一次验证。

![](http://1.14.100.228:8002/images/2022/03/30/20220330101307.png)



> 通过例如版本号等方法，确定数据块是否可覆盖的过程可以更高效

在上面描述的机制之上，LFS必须包括一组策略，以确定何时清理以及哪些块值得清理。决定什么时候清洗更容易；可以是周期性的，或者在磁盘空闲的时间，或者当磁盘已满，而不得不清理空间时。

决定清理哪些段则更有挑战性，关于这一点有许多研究者提出了许多方法，这里挺复杂的，而且设计自由度很高，所以不展开讲了。

最后一部分是崩溃恢复，这里涉及到两个方面，一是在写入段的时候崩溃，二是在写入CR的时候崩溃

对于第一个方面（看不懂）

> 因为LFS每30秒左右写一次CR，文件系统的最后一个一致性快照可能非常旧。因此，在重新引导时，LFS可以很容易地恢复，只需读取检查点区域、它所指向的imap块以及随后的文件和目录;但是，最后许多秒的更新将会丢失。
>
> 为了改进这一点，LFS试图通过一种数据库社区中称为前滚的技术来重建其中的许多段。其基本思想是从最后一个检查点区域开始，找到日志的结尾(包含在CR中)，然后使用它读取下一个片段，并查看其中是否有任何有效的更新。如果存在，LFS相应地更新文件系统，从而恢复自上一个检查点以来写入的大部分数据和元数据。详情可以查阅原文中的论文。

对于第二个方面

> 为了确保CR更新是自动发生的，LFS实际上保留了两个CR，一个在磁盘的两端，并交替地对它们进行写操作。当使用inodemap和其他信息的最新指针更新crs时，LFS也实现了一个谨慎的协议;具体来说，它首先写出一个报头(带有时间戳)，然后是CR的主体，最后是最后一个块(也带有时间戳)。如果系统在CR更新期间崩溃，LFS可以通过查看不一致的时间戳对来检测到这一点。LFS将始终选择使用具有一致时间戳的最新CR，从而实现CR的一致更新。

## SSD磁盘系统

随着电子技术的不断发展，一种名为闪存的新型存储技术越来越重要，基于这种技术设计的存储设备称为固态存储，和CPU一样，这种电子存储设备建造在复杂的晶体管电路之上，这种基础硬件架构的不同使得闪存设备有一些机械硬盘设备没有的特性：1.要写入给定的块，必须要擦除更大的块 2.过于频繁的在页面上写入会导致晶体管寿命减少（不可靠）

不同闪存技术的区别主要在于单位颗粒内的存储位数，主流的闪存技术有SLC、MLC、TLC和QLC，分别对应单位颗粒内存储位一位、两位、三位和四位，存储位每上升一位，对存储大小的提升都十分大，但是相应的传输带宽越小、存储寿命越短。

> ![](http://1.14.100.228:8002/images/2022/03/31/20220331113621.png)
>
> 附上三种颗粒的各种操作开销对比
>
> ![](http://1.14.100.228:8002/images/2022/03/31/20220331125803.png)

大量的闪存颗粒组成的存储设备就是SSD，对于操作系统来说，SSD同样是大块的存储空间。而在底层的管理上，SSD有其特殊的概念，首先是将存储空间分成两个等级，一是可擦除的块，二是可访问的页。

> 闪存中的块、页的概念和磁盘、虚拟内存中的都不一样

一般块大小比页大小要大很多，例如块为256KB的话，页大小就是4KB。对块和页的操作就是SSD的基本操作，有以下三种：

* 读取（对页）：客户端输入读取命令和适当的页码，就可以实现对任意块的读取，这个操作只需要几十微秒，这意味着SSD是随机访问的设备
* 擦除（对块）：写入页面之前，**设备的性质**要求首先擦除页面所在的整个块，因此，在执行擦除之前，您必须确保块中所关心的任何数据都已经复制到其他地方(内存，或者可能是另一个闪存块)。擦除块十分昂贵，通常需要几毫秒。
* 编程（对页）：一旦一个块被擦除，程序命令就可以用来将页面中的一些1改为0，并将页面中需要的内容写入闪存中。编程一个页面比擦除一个块要便宜，但比读取一个页面要贵，在现代闪存芯片上通常需要一百微秒左右。

每一个页面都有与之相关的状态，页面以INVALLD开始，还有ERASED、VALID，已一个四个页大小的块为例，如下图所示。

> ![](http://1.14.100.228:8002/images/2022/03/31/20220331125048.png)
>
> 注意page2，3，4的数据要保存到其它块中

总的来说，SSD是一种性能很高的磁盘（尤其在随机读性能上）。但是它有两个最大的问题，一是写入性能，因为在写入之前擦除操作是必须的，所以我们如果只写入一页就擦除整个块的话，那写入速度将会慢的离谱，二是可靠性，因为闪存颗粒的擦除次数是有限的，所以太过频繁的擦除会造成芯片很快的磨损。另外除了擦除，还有干扰的问题，即访问闪存中的特定页面时，相邻页面中的一些位可能会翻转，这也是我们需要解决的一个问题。

再讨论解决上述问题之前，我们先思考如何将那么多的闪存颗粒组织起来称为可用的SSD，简单来说，SSD由标准存储接口、闪存芯片（内含一大堆的闪存颗粒）、内存、一个称为FTL的固件区域。它们的作用大概可以这样概括：

* 存储接口：**不知道是啥**

* 闪存芯片：简单来讲就是大量的闪存颗粒的聚合体，现代SSD内部通常会有多个闪存芯片，并发执行获得更高的性能。
* 内存：用于缓存数据和存储映射表
* FTL：获取设备接口上的数据访问请求，并转化为物理块和物理页面上的读取、擦除、编程命令。

它们之间的组成关系如下图所示

![](http://1.14.100.228:8002/images/2022/03/31/20220331132101.png)

现在我们来看FTL是怎样工作的，对于FTL来说，每一个请求都要落实到块或页上，因此需要建立一个逻辑-物理的地址映射。最直接也是最低效的方式是直接映射，逻辑页面N的读取直接映射到物理页面N的读取。可想而知，对于外部请求，它直接读取整个数据块，擦除整个数据块，对整个数据块进行编程，非常之低效。更严重的是可靠性问题，擦除次数多不只是低效，而且意味着更高的损耗。

很多人可能想到了，目前最适合SSD的映射是日志结构的映射，因为在写入数据的时候，大段的写入对SSD来说更加友好。还是用一个例子来看，输入如下图所示

![](http://1.14.100.228:8002/images/2022/03/31/20220331163946.png)

可看到，虽然是对四个不同位置逻辑块的写入，但是物理块上它们是连续写入的。

>  大多数SSD会按照块内顺序写入页面，从而减少与程序干扰相关的可靠性问题

那么如何对磁盘进行读取呢，进一步将，FTL如何将逻辑块请求的地址转化为物理块请求的地址，这要依靠一种称为映射表的结构，映射表存储在磁盘内存中，它保存着逻辑地址和物理地址的映射关系，如下图所示。

![](http://1.14.100.228:8002/images/2022/03/31/20220331164703.png)

这种日志结构的基本方法有一些缺点，首先，逻辑块的覆盖会导致我们称之为垃圾的东西，过多的垃圾收集会增加写入放大，设备必须周期性地执行垃圾收集(GC)，以找到所述的块并为将来的写释放空间；第二个是内存映射表的高成本，设备越大，这样的表需要的内存就越多。

> 写入放大是闪存 和 固态硬盘 （SSD）中一种不良的现象，即实际写入的物理数据量是写入数据量的多倍。

 我们先看第一个问题，假如在块中有两个页更新，两个新页写入新块，则旧块中的数据变成垃圾数据，如果想对这些数据进回收，则要进行三步操作：

![](http://1.14.100.228:8002/images/2022/03/31/20220331171204.png)

可以看到，垃圾回收的成本很高，需要读取和重写整个块，所以我们要尽量减少垃圾回收的次数，现代SSD利用overprovision方法，overprovision就是磁盘实际容量比标记容量高一些，用于推迟GC，增加内部带宽

> 记录某个页是否能够垃圾回收需要用到一个新的API trim，trim API取一个地址和一段长度，简单的通知设备由该地址和长度指定的块已经被删除

现在我们看第二个问题，简单的说，页面级粒度的映射表是不实际的，因为哪怕只是一个1tb的SSD，最高就会有1GB的映射表。一种粒度更大的映射关系是块映射，要做到块映射，逻辑空间的索引也要变成**块号+偏移量**，这样在FTL中只需要在内存中读取块与块的映射即可，但是出于性能原因，在基于日志的FTL中使用基于块的映射并不能很好地工作。当发生**small write**(即小于物理块大小的写)时，FTL必须从旧块中读取大量的实时数据，并将其复制到一个新块中(以及小写入中的数据)。这种数据复制大大增加了写放大，从而降低了性能。

我们可以采用一种混合的方法，具体为，FTL在其内存中存储两种类型的映射表，一种是日志表，以页为单位映射，一种是数据表，以块为单位映射。当寻找特定的逻辑块时，FTL首先查阅日志表，找不到就查询数据表。

它们之间的关系是，当日志表超过一定大小的时候，就将在同一块内的映射转化到数据表中，这一操作称为**switch merge**。如下图所示

![](http://1.14.100.228:8002/images/2022/03/31/20220331172825.png)

> 如果有大量的随机小写，则会出现什么情况？

混合的方法虽然好，但是比较复杂，可能是简单的方法就是只在内存中缓存FTL的活动部分，从而减少所需的内存量，大部分情况下这种方法表现的很好。

擦除均衡也是SSD的工作过程中很重要的一环，多次擦除/编程周期会损害闪存块，对于哪些长时间都不会被覆盖的数据，需要定期的进行重写，来平衡损耗。

现在我们已经介绍了SSD的大部分内容，可以做出一些总结，SSD是随机存取的设备，和机械硬盘最大的区别在于随机读写，对于随机读写IO，机械硬盘的每秒只能做到几百次IO，但固态硬盘可以做到更好。下图是不同硬盘之间的性能比较图。

![](http://1.14.100.228:8002/images/2022/03/31/20220331192231.png)

## 数据完整性保护

除了那些文件系统的基本特性，还有一些其它的特性需要研究，现代存储设备越来越快，但是相对的可靠性下降。因此我们需要保证读取数据时返回的数据和我们存入的是一致的，即数据的完整性。如果数据不是完整的，则需要引发故障，磁盘有不同的故障模式，之前我们学到的是fail-stop模式，也就是一遇到故障就立刻停机。这种模式不适用于现代系统，因为现代磁盘的情况是大部分时候大部分的空间都是可用的，只有少部分会有故障。

如果细究的话，常见的现代磁盘故障可以总结为两种，扇区错误（lantent-sector errors）和块损坏（block corruption）。

* 当一个磁盘扇区出于某种原因物理损坏而无法读取了，这就是LSE，这是可检测的错误，当读取这些位置时，磁盘返回一个错误
* 磁盘上的数据出现错误则不能够显式的检测出来，当读取它们时，磁盘返回错误的数据，这就是corruption

出现这两类错误的模型被称为fail-partial模型，磁盘看起来是可用的，只是访问某些块时会返回报错或错误数据，事实上，这两类错误发生的概率都不高，有研究表明，在采样150万个磁盘中，出现LSE和Corruption的概率都不高，但是不可忽略，如下表所示。

![](http://1.14.100.228:8002/images/2022/04/01/20220401161750.png)

研究还总结了两类错误的特征，其中

对于LSEs，有：

* 具有多个LSE的昂贵驱动器与廉价驱动器一样，可能会产生额外的错误
* 对于大多数磁盘来说，第二年的误差率会逐年上升
* lse的数量随着磁盘大小的增加而增加
* 大多数具有lse的磁盘都小于50
* 具有lse的磁盘更有可能开发额外的lse
* 存在着大量的时空局部性
* 磁盘擦洗很有用(大多数lse都是通过这种方式找到的)

对于corruption，有：

* 在相同的驱动器类中，不同的驱动器模型损坏的机会差别很大
* 不同型号的老化效应不同
* 工作负载和磁盘大小对损坏的影响很小
* 大多数磁盘只发生少量Corruption
* 在磁盘内或跨磁盘RAID中，Corruption不是独立产生的
* 既有空间局部性，也有时间局部性
* 与lse的相关性较弱

我们开始着手处理两种错误，扇区错误更简单处理一些，因为扇区错误更容易检测，当检测到扇区错误时，直接激活磁盘内置的冗余机制返回正确数据即可，如RAID-2和RAID-45所做的那样。

对于corruption，重点在检测，如果知道哪个块错了，就直接采用冗余机制恢复即可。而检测的方法就是我们这次的重点，现代存储系统用来保持数据完整性的主要机制就是**校验和**，校验和只是一个函数的结果，该函数以一个数据块(比如一个4KB的块)作为输入，并在该数据上计算一个函数，生成一个数据内容的小摘要(比如4或8个字节)。这个摘要称为校验和。这样的计算的目的是使系统能够检测数据是否以某种方式被破坏或改变，方法是将校验和存储在数据中，然后在以后访问时确认数据的当前校验和与原始存储值相匹配。

由许多不同的函数被用来计算校验设，并且强度各不相同，它们在保护数据完整性和计算速度上基本上成反比。最基本的校验和函数是基于异或计算的XOR函数，以16字节的块上计算4字节的校验和为例，4字节的校验和意思就是每4个字节一个校验位。

![](http://1.14.100.228:8002/images/2022/04/01/20220401164534.png)

XOR的优点是简单快速，但是如果有两位发生变化，XOR就失效了，泛用性不足。另一个基本的校验和函数是加法。这种方法的优点是速度快，它可以检测数据中的许多变化，但如果数据发生了移动，它就不太好用了。一个稍微复杂一点的算法被称为弗莱彻校验和（Fletcher check- sum），能够检测所有的单比特、双比特错误和许多突发错误，是一种很强的算法。最后一个常用的校验和称为循环冗余校验（cyclic redundancy check），可以相当有效地实现二进制模运算，因此CRC在网络中的普及程度也很高。

> 对于一个校验和函数，不同的内容也可能会有相同的校验和结果，这称为collision，一个好的校验和函数应当最小化collision

那么如何在存储系统中使用校验和呢？通常的做法是给每一个扇区或者块一个校验和，校验和的存储如下图所示。

![](http://1.14.100.228:8002/images/2022/04/01/20220401165255.png)

如果磁盘支持小块，那么第一种更好，如果不支持，第二种更合适，但是效率可能会降低。

上面描述的基本方案在损坏块的一般情况下工作得很好。然而，现代磁盘有两种不同寻常的故障模式，需要不同的解决方案。

第一种是**错向写**，即写入数据正确，但位置错误，这种情况下读取磁盘返回完全不相干的信息，解决方案就是增加一个物理ID，用于标志磁盘和块的位置，每次检验前对照一下，如下图所示。

![](http://1.14.100.228:8002/images/2022/04/01/20220401165755.png)

第二种是**写入丢失**，磁盘通知系统写入完成后，实际上写入因为某种原因没有成功，这种错误的检验比较复杂，因为之前校验和的方法不能处理这种错误，我们需要额外的机制来处理，关于这一点不同系统的解决的方式有很多种，各不相同，因此这里不讨论。

最后一个点是磁盘擦洗，我们并不是只在数据写入时检查数据正确性，也需要在选择某一个时间统一对整个磁盘进行数据检验，很多时候被安排在每晚或每周，例行检查被称为磁盘擦洗。不同系统也有不同的擦洗规划。

了解了上述的知识后，我们来总结一下校验和的开销，总结来说，校验和的开销有时间开销和空间开销两种。

关于空间开销：

* 磁盘开销：校验和占磁盘的典型的比率可能是每4 KB数据块需要8字节的校验和，这将导致0.19%的磁盘空间开销。
* 内存开销：当访问数据时，现在必须在内存中有足够的空间来存放校验和和数据本身。但是，如果系统只是简单地检查校验和，然后在完成校验和之后丢弃它，那么这种开销是短暂的，不太值得关注。

关于时间开销：

* CPU开销：CPU必须在每个块上计算校验和，无论是在存储数据的时候(确定存储的校验和值)，还是在访问数据的时候(再次计算校验和并与存储的校验和进行比较)。许多使用校验和的系统(包括网络栈)都采用了一种减少CPU开销的方法，将数据复制和校验和合并到一个精简的活动中；因为无论如何都需要复制(例如，将数据从内核页面缓存复制到用户缓冲区)，因此结合复制/校验和可以非常有效。
* IO开销：如果校验和完全存储在数据块之外，我们需要额外的IO来访问它们，另外，擦洗所需的IO也是不少的，前者可通过设计减少;可以对后者进行调整，从而限制其影响，或许可以通过控制这种擦洗活动发生的时间来实现

## 总结

