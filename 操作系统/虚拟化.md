## 抽象进程

操作系统的终极目标即使使程序更加简单易用，为了达成这一目的，我们需要对程序本身进行抽象。进程就是运行程序的主要操作系统抽象，在任何时候，进程都可以用它的三个状态来描述

* 地址空间中的内存内容
* CPU寄存器的内容
* 关于IO的信息

这些状态在操作系统中是怎样存储的呢？我们要提到一种数据结构，叫做进程列表，它包含系统中所有进程的信息，列表中每个条目都存在于有时被称为过程控制块(PCB)的东西中，它实际上只是一个包含特定进程信息的结构。

通过这三个状态，我们可以使进程成为虚拟化技术的基石，即通过调节进程的执行顺序来进行计算资源的分配。

进程 API 由程序与进程相关的调用组成。通常包括创建、销毁、和其他有用的调用例如等待、查询状态或者进行控制等。

进程存在于许多不同的进程状态之一，包括运行、准备运行和阻塞。不同的事件(例如，被调度或被取消调度，或等待 I/O
完成）会将进程从这些状态之一转换到另一个状态。

<img src="http://1.14.100.228:8002/images/2022/01/18/20220118223041.png" style="zoom:50%;" />

这就是进程的三个不同工作状态的切换。这一部分与虚拟化的关系在于，对计算资源和存储资源的虚拟化就是建立在进程这一概念之上的，进程拥有自己独立的cpu资源和内存资源。通过**时间共享**和**上下文转换**等机制，能够实现现实意义上的虚拟化

## 运行受限

既然操作系统也是由许多程序组成的，也就是说操作系统本身也是诸多进程的集合。但我们要形成一种共识，普通程序的运行应当受到操作系统的调控，保证其运行受限，防止其做出危害系统软件和硬件的行为。

要如何做到呢，操作系统为CPU套上“保护套”，在开机时操作系统初始化trap表并启动中断计时器，让操作系统处于内核模式，普通进程处于受限模式。也就是limited direct execution(LDE)的思想，在某些关键的时间点，例如当一个进程发出一个系统调用，或者一个定时器中断发生时，安排操作系统参与进来，并确保“正确”的事情发生。

这里有三个概念需要进行解释，首先操作系统定义了两种进程状态，内核的和受限的。相对于内核进程，受限进程无法进行IO，申请CPU资源和更多内存占用。这些都需要通过操作系统进行系统调用来协助进行。

trap在表现上是一种由异常情况（例如，断点，非法操作码）引起的同步中断，它一开始就在硬件中初始化，多个trap组成trap table。当触发trap则将进入操作系统进程，也就是内核模式来处理异常。

中断计时器，在CPU被用户进程占用时，操作系统是无法主动得到进程的控制权的，这时候硬件通过设置中断计时器来让进程每隔一段时间进行一次中断，帮助操作系统能够在一段时间内获得操作进程和进行system call的能力。

通过这些基础机制，操作系统只需要处理内核操作和切换用时过长进程即可保证其它进程安全的进行。但是，总是有多个进程同时向操作系统请求cpu占用，操作系统该怎么安排，这是系统能否高效运行的保证。

进行system call和context switch的时间成本都非常低

> Based on our observations, the average direct cost of the kernel-level context switch is 0.62\u0016sand the average direct cost of user-level context switch time is 1.18\u0016s.

system call也差不多在这个量级

## 调度策略

我们已经了解了操作系统调度资源的底层策略，如进程的运行等。但是如果要想理解更高级的调度策略（scheduling policies），那么首先要明确我们要解决的是什么问题：早期计算机的调度策略是怎样的？调度策略的关键假设和评估指标是什么？怎样建立一个调度策略的基本框架？

我们抽象出一个概念，工作量（workload）来描述操作系统的运行主体也就是进程集合，用作业(job)来描述进程，我们用一些假设来帮助我们定义workload，我们假设在操作系统中

1. 每个作业运行相同的时间。
2. 所有的工作同时出现。
3. 一旦启动，每个作业就会一直运行到完成。
4. 所有的作业只使用CPU(例如:它们不执行I/O)。
5. 每个作业的运行时是已知的。

假如进程都遵循以上的假设，那么操作系统就可以用十分简单的调度策略来安排CPU资源的使用如FIFO策略，这也是早期计算机使用的调度策略。但是明显假设不成立，在介绍其它策略之前，我们需要对cpu资源的调度性能确立指标，我们有两个主要指标：turnaround time和response time，这也是调度策略的两个主要体系。

turnaround time = 进程完成时间 - 进程出现时间

response time = 进程响应时间 - 进程出现时间

假如我们以turnaround time作为评价体系，那么在假设5成立的情况下，我们应当使用STCF调度策略，它的基本思想就是：每当有程序进入运行状态，在当前所有进程中总运行时间最短的对cpu资源进行抢占，以此近似最优化turnaround time。

假如我们以response time作为评价体系，那么在假设5成立的情况下，我们应当使用RR调度策略，它的基本思想就是：选取一个cpu时间的n倍作为基本时间片，然后每隔一个时间片就切换一次进程，这样确保每个进程从开始到结束都有相对良好的响应速度。注意，时间片的选取对响应时间的影响至关重要。但是并不是时间片越短越好，因为上下文切换的时间成本会因此指数级上升。

我们注意到，假如进程有IO任务，那么在它进行IO时，CPU可以使用overlap策略对IO时间片用其它进程进行抢占。这样可以利用时间片切换进程的特性来使IO对CPU的影响降到很低。

总结一下就是，在假设5成立的情况下，我们可以采用STCF、RR、overlap结合的策略来对进程进行调度，但是RR会大大增加turnaround time，而STCF会增加response time，怎样对他们进行trade-off是构建调度策略的基本框架的关键。

## MLFQ策略

我们已经学习了在假设状态下的最基础调度策略，它们都是为了优化既定指标而粗略设计的调度策略。在实际的操作系统中，调度是一项十分复杂的工作，要同时面对数百个状态不一的进程，还要保证较高的处理速率和较快的响应时间。基于同时优化两个指标的目的，科学家设计了一种策略，叫做Multi-level Feedback Queue(MLFQ)策略。它的核心思想是：关注进程在一段时间内行为，并依此进行处理。

基础的设计是使用多个优先队列，让所有进程都在队列中，并遵循5个最基本的规则：

* 如果进程A所处队列优先级比进程B高，则运行A
* 如果进程A和进程B在同一个队列，则它们之间遵循RR策略，交替运行
* 当有进程产生时，放在最高级的队列中
* 当一个进程用完了在当前队列中的时间份额，则切换到下一级队列中
* 在一段时间S后，所有进程都重置到最高级队列中

MLFQ策略成功的实现了turnaround time和response time的双重优化，对于需要长时间运行的进程，能够在S时间后得到一段运行的时间，而交互性强的进程，则总是在较上级的队列中，能够较频繁的访问CPU，但不至于霸占CPU不放。第四条规则让所有进程不管在什么状态下都不能长时间占用CPU，保证了所有进程之间的公平性。

在MLFQ策略下的workload如下图所示

<img src="http://1.14.100.228:8002/images/2022/02/06/20220206160119.png" style="zoom: 80%;" />

由于它优良的性能，许多系统包括UNIX衍生，Solaris、Windows系统都采用不同形式的MLFQ来作为进程的基本调度器。

## Propor-Share策略

在上一节我们介绍了基于turnaround time和response time两个metrics的MLFQ策略，现在我们引入一种新的体系，基于fair-share理念，也就是CPU时间分块分配的调度策略。这一类策略会给每个进程一个确定百分比的CPU时间，不已优化具体指标为目的，而是直接实现按需分配。我们将讨论三种基于这一理念的调度策略。

> 这里提了一种fairness的量化方法，大致就是相同长度的进程之间结束的时间越接近，则越fair

第一种是lottery scheduling，这一策略首先定义了票（tickets）作为分配CPU时间的标准。即给定每个进程一定份额的票，进程所持有的票数占全部票的比例就是该进程在这次时间片切换中获得运行时间的概率。workload如下图所示

<img src="http://1.14.100.228:8002/images/2022/02/07/20220207161152.png" style="zoom:67%;" />

在lottery scheduling中，进程运行时间越长，获得响应比例的时间的准确性越强，且基于随机性的分配有三大好处，可靠（长时间下）、轻量（无需存储中间状态）、快速（就算一下概率）。

lottery scheduling的基础理念比较简单，也有一些底层机制（ticket mechanisms）是一类的方法的基础，不清楚为什么现在用的少

第二种是stride scheduling，这一策略首先定义每一个进程有一个stride，以此计算出pass，在一个时间片我们在当前运行的进程的counter上累加它的pass，每一次进程切换都选择counter最小的进程开始。workload如下图

<img src="http://1.14.100.228:8002/images/2022/02/07/20220207162608.png" style="zoom:50%;" />

基于stride scheduling，CPU时间被精准的分配到每一个进程上。看上去很好，但是还有几个疑问，stride是怎么初始化的？当有新的进程加入时，counter怎么算？

基于stride scheduling，科学家提出了The Linux Completely Fair Scheduler (CFS)。它的基础理念是为每一个进程分配各自不同的时间片，有些像加权版的RR策略，但是要复杂的多，具体的实现规则为

定义**sched latency**，代表所有进程的时间片时间总和，再定义**min granularity**，代表进程的最小时间片，以确保不会在context上开销太大。定义每个进程的**weight**，也叫nice level，用固定的表组成，有-20到19共40个权值，对应stride scheduling中的stride。

<img src="http://1.14.100.228:8002/images/2022/02/07/20220207171130.png" style="zoom: 67%;" />

stride scheduling中的counter在CFS中称为**vruntime**，在计算出每个进程的时间片后，选vruntime最小的进程开始，这就是CFS的基本规则。此时进程表中每个进程都有对应的weight，为了帮助搜索众多进程中weight最小的进程，CFS采用红黑树存储进程信息，将O(n)的搜索时间复杂度降为O(logn)

当处理那些需要休眠的进程，也就是长时间IO的进程，CFS会将它们恢复时的vruntime设置为树中进程的最小值，这样能够解决休眠进程占用的问题，但是对于频繁休眠的进程，可能会造成其无法获得CPU时间

通过巧妙的设计和高级数据结构的合理使用，CFS做到了较少的调度开销，提高了效率。此外CFS还有许多高级特性如提高缓存性能的启发式方法、处理多CPU和跨进程组调度等。

**小结**

通过前面一段的学习，我们初步了解了CPU的虚拟化的原理。首先，了解一系列重要的机制：trap和trap处理程序、计时器中断，以及操作系统和硬件在进程之间切换时如何保存和恢复状态。我们还了解到操作系统总是在确保一直负责机器的运行，通过运行受限保证恶意进程一直是收到限制的。我们还了解了调度器的概念，构建高性能的调度器是提升操作系统性能的关键，目前也存在许许多多的调度器，例如在Linux系统中CFS，BFS，O(1)等调度器一直都占有一定的部分。

## 地址空间

虚拟化技术的另一个核心技术是虚拟化内存，通过这项技术，操作系统给每一个用户进程都提供了一个大的、稀疏的、私有的地址空间的错觉，这些地址空间中包含了一个进程运行所需的全部状态。在一些重要硬件的帮助下，操作系统能够获取每个进程的内存引用，并将它们映射到实际的物理地址上。

我们以一个单线程的进程为例，操作系统为其分配的地址空间中包含了

* 代码，也就是由代码编译而来形成的一条条指令；
* 堆栈，包含函数的链式调用栈、参数、临时变量、返回值；
* 堆，由指令申请的空间，包含数据和主动分配的空间如malloc、new等

<img src="http://1.14.100.228:8002/images/2022/02/08/20220208194222.png" style="zoom:50%;" />

为了达到现代操作系统的性能要求，虚拟化内存技术有三大目标

* 透明性：进程不应该知道内存是虚拟的，相反，要像真的有那么多内存一样让进程自由分配
* 高效率：虚拟化内存应该要快，不能让程序运行的更慢，应该要实现简单，不需要为支持虚拟化而使用太多内存
* 保护：进程之间、进程与操作系统之间的内存应当收到保护，不能相互覆盖

要实现这些目标，我们将会用到的方法有三个方面，必要的硬件支持、大量的低级机制、一些关键(高级)方法，这些都是之后需要了解的内容

> 在Linux中，我们可以使用free命令来查看内存的使用情况，free命令产生内存使用表，利用man(manual操作面板)命令来查看free的使用描述和表头含义

## 地址转换

之前学习过的limited direct execution可以简单理解为操作系统通过一系列机制限制用户进程进行任何“不正确”的操作，现在再提出一个机制：address translation。它是LDE概念的延申，通过address translation操作系统可以将进程的每一个内存访问地址从地址空间，转换为实际存在的物理内存。

这样做有两大意义，一是以此方式为进程提供一个可以随意分配的空间，方便程序的开发；二是为进程之间的运行和进程与操作系统之间的运行提供内存保护，任何一个进程都不能随意访问另一个进程的地址空间。

base-bounds或者称为dynamic-relocation，是一种基于硬件的地址转换机制，每一个CPU都含有一对base register和bound register。其中base register负责将进程中的每一次内存访问进行推移到实际的物理地址上，bound register负责查看进程的每一次内存访问是否在其所分配的地址空间内，如果超出空间，则触发trap，由操作系统按照exception程序来进行处理错误进程。

基于硬件的address translation十分高效稳定，但是无法利用到进程地址空间中栈和堆栈之间被分配但未使用的大量空间，也被称为internal fragmentation即内存碎片，这也是接下base-bounds要改良的方向

至此，我们已经学习了不少在操作系统中由硬件支持的底层机制，我们可以将机器开机和简单进程运行时的各种过程抽象为下图

<img src="http://1.14.100.228:8002/images/2022/02/09/1.png" style="zoom: 50%;" />

包含了许多硬件机制：trap table、interrupt time、base-bound registers、switch context、kernel/user mode。

当然也有许多操作系统的底层功能：各种硬件可触发的system call如time、exception等，存储进程状态和内存空间状态的process table、free list

## 分段

segmentation是一种更加通用的base-bound设计方法，上文提出的base-bound内存模型虽然达到了功能要求，但是地址空间中heap和stack中间大片的内存无法被灵活使用，我们需要一种更灵活的地址转移技术，也就是segmentation。

![](http://1.14.100.228:8002/images/2022/02/10/20220210153743.png)

上图右半部分是segmentation的内存模型，segmentation将进程的code、stack、heap在物理内存上分开放置，分为三个不同的段，并给每一段都配置一对base-bound registers，当进程发出一个地址请求时，地址经由处理变为offset，也就是相对于其地址空间三个不同分段的的偏移量，再发送到bounds确认偏移量没有超出范围，之后与base相加/减得到在物理内存上的实际地址

大致过程如上，但是还有许多细节，硬件怎样确定发过来的偏移量是相对于那一段的？stack段地址空间向上填充需要相减是怎样做的？这些由于篇幅掠过。

segmentation分段处理的方式还带来了另一个好处，不同进程的code段、stack段、heap段都可以实现一定程度的共享。在有序的前提下，不同进程共用一段代码，一个进程访问另一个进程的数据，这些操作可以大大增加的内存的利用率。

segmentation分段处理的方式也有一个难以解决的问题，不同进程的不同段在物理内存中稀疏的分布导致空余空间小而散，难以利用，这些空间被称为external fragmentation外部碎片。

<img src="http://1.14.100.228:8002/images/2022/02/10/20220210160303.png" style="zoom:67%;" />

操作系统可以通过移动段位置来整理空间，但是这是一个trade-off的问题，空间紧凑则已有进程申请新空间则比较困难，空间稀疏则新进程申请空间十分麻烦。目前没有最优方案，一般可以通过一些智能算法或者定期压缩段来缓解这个问题。

segmentation实现了更深一步的虚拟化，硬件只需要根据接收的信息找到内存中各个分段的地址，无需关心分段的含义是什么。

## 分配器

在进程中，具体的讲，是在地址空间的heap区，存在许多空闲内存，或者在操作系统上，进程内存空间之中也有许多未被分配的空闲内存。这两种空闲内存相对对象不同，但是都面临相同的问题，即怎样合理安排的空间的使用，才能让新空间的申请变的更加高效。这引出了一类问题，即空闲空间管理问题。

对于操作系统级的空间管理之前已经有所涉及，我们以更加简单的情况，即进程中heap区的空闲内存管理为模型，分析怎样管理内存。

首先是三个功能方面的底层机制，首先是splitting和coalescing，我们抽象进程heap区的空余空间都在free space table这个结构上，当一片新内存被申请时，splitting机制让一块内存分离出和新内存相同大小的空间以供使用，而剩余空间依然在表里。当两片剩余空间地址上相邻时，coalescing机制让相邻空间合并成更大的一块。

![](http://1.14.100.228:8002/images/2022/02/11/20220211204849.png)

第二个功能是记录已分配空间的的大小，在使用free()函数时，我们并不需要指定需要释放的空间的大小，只需要把地址指针传入即可。这是因为一个已分配空间的大小并不是保存在操作系统中，而是就在分配内存的头部，

第三个功能是将free space table嵌入到地址空间中，与记录已分配空间的的大小类似，将空余空间的大小保存在空余空间的头部

抽象的，我们可以认为heap中某一段空间的内存模型以及free space table在地址空间中嵌入的模型如下图所示

![](http://1.14.100.228:8002/images/2022/02/11/20220211205658.png)

其中free space table以链式形式存储，假如两片空间相邻，则会合并为一个空间。事实上，大部分进程一开始heap空间都是很小的，当需要更多空间时，可以通过system call如sbrk申请增加heap空间。

当一个变量/进程申请空间时，如何安排已有的空间来给出响应的空间是十分重要的，这一方面有许多策略，简单的有如Best Fit、First Fit、Next Fit等，而更复杂更高级的策略有Segregated Lists、Buddy Allocation等。事实上，这个领域到现在还是没有相当统一的优化方法，如果之后对空余空间调度器感兴趣的话，可以看一下glibc allocator这个调度器，了解一下现代调度器。

## 分页（一）

segmentation方法是将空余空间分为variable-sized即可变大小的块，也由此带来了fragmentation的问题。我们介绍一种新的基于fixed-sized的方法，称为page，即基本思想就是将物理内存分为固定大小的page frames，每一个frame都可以对应一个virtual-memory page。page最大的优点就是灵活，系统能够高效的提供地址空间的抽象，而不用管进程怎么使用这些虚拟内存。

<img src="http://1.14.100.228:8002/images/2022/02/12/20220212210941.png" style="zoom:67%;" />

操作系统为每一个进程都提供了一个数据结构称为page table，用来存储每一个进程由地址空间到物理地址的address translation。这种情况下，进程发出的每一个虚拟地址都不是单独的地址大小，而是包含了至少两部分，即virtual page number (VPN)和offset，offset的概念上一节已经提到过了。我们举例一个大小为64 bytes的address space，每一个page是16 bytes，那么这个进程的虚拟地址总长是6 bit，其中2 bit是VPN，意思是属于哪一个page，4 bit是offset。

<img src="http://1.14.100.228:8002/images/2022/02/12/20220212210953.png" style="zoom:67%;" />

操作系统和硬件一起将这个虚拟地址转化为物理地址，我们知道物理地址也是由一片片page组成的，由此我们得到VPN后就在page table中找这个属于这个进程的VPN对应的是物理内存中的哪一个page。

那么page table存储在哪里呢？对于一个32位、4KB per page的系统来说，光是VPN就有20位，一个X86系统的page table也有32位那么多，因此所有进程的page table加起来是相当大的，不可能存储在除存储硬件以外的硬件里，因此page table就存储在内存中，有可能在操作系统内存区以外的内存（老系统），也有可能在操作系统内存区中（现代系统）。

那么page table中存储着什么呢？以一个x86系统的page table为例，32位长度的page table占最多的是PFN，通过PFN可以直接映射到物理内存的某一个page上。此外还有一些用于其它功能的bit位，例如protection bit、present bit、dirty bit、reference bit、accessed bit、read/write bit等

<img src="http://1.14.100.228:8002/images/2022/02/12/20220212211028.png" style="zoom:67%;" />

那么page table到底如何工作呢？一个虚拟地址的VPN被提取出来后，经由专门的page table base register得到属于这一VPN的page table的物理地址，硬件再由物理地址提取page table中的FPN等信息找到实际的物理地址，可以发现，这一过程比segmentation多了一步查找-转换。导致page在实际上是比较慢的，事实上，内存读取速度可能慢两倍甚至更多。要想实际使用page策略，必须在仔细的设计硬件和软件。下图是page table工作流程的伪代码。

<img src="http://1.14.100.228:8002/images/2022/02/12/20220212211111.png" style="zoom:67%;" />

我们以一个C程序为例，看一看实际的内存访问情况

<img src="http://1.14.100.228:8002/images/2022/02/12/20220212211140.png" style="zoom: 80%;" />

C程序很简单，右边是它的二进制码编译而来的机器语言

<img src="http://1.14.100.228:8002/images/2022/02/12/20220212211206.png" style="zoom:80%;" />

实际的内存访问情况如图

## 分页（二）

分页的基础方法对物理地址的查询依赖于page table，但是page table的查询非常之慢。这也就导致了分页方法带来了非常高的性能开销，为了解决这个问题，科学家提出了translation-lookaside buffer(TLB)，一基于硬件的虚拟地址-物理地址转换的缓存技术。通过TLB技术，大部分的内存访问申请都会经由缓存快速的定位，使page这一技术的可用性大大提升。

具体的，我们以一个C语言程序为例子，这个例子中page大小位16 bytes，一个整型变量的大小为4 bytes，数组在地址空间中的占用情况如图，可以知道从程序开始到程序结束，TLB的使用情况为miss, hit, hit, miss, hit, hit, hit, miss, hit, hit。相当于70%的访问是被提前记录的，事实上，如果分页更大，变量空间占用越大，命中率还会继续提高到90%以上。

<img src="http://1.14.100.228:8002/images/2022/02/13/20220213190939.png" style="zoom:67%;" />

既然提到了命中率，那么没有命中的时候要怎样处理呢。这也是现代操作系统的一大分歧点，对于复杂指令集系统如CISC，例如intel x86系列，以硬件为基础来处理TLB miss的情况，也就是说，TLB miss并不会引发任何trap，硬件负责将新的TLB entry加入TLBs中，要做到这一点需要不少的硬件代价。

<img src="http://1.14.100.228:8002/images/2022/02/13/20220213191006.png" style="zoom: 80%;" />

对于简单指令集系统如RISC，例如MIPS R10k，则由操作系统来处理TLB miss，这样做有两个方面的优点，一是灵活，操作系统可以基于相同的逻辑，使用任意数据结构来构建page table和TLB，易于使用也易于更改；二是简单，硬件只需要在遇到TLB miss时引发一个trap，之后让操作系统处理即可。

<img src="http://1.14.100.228:8002/images/2022/02/13/20220213191018.png" style="zoom:80%;" />

早期RISC系统的芯片产生时引发了很多影响，因为明显RISC更块，但是经过这些年的演变，CISC逐渐吸收了很多RISC的特点，现在两者都可以达到很快的处理速度。

TLB到底由什么组成呢？一个典型的TLB会有32、64或者128个entry，一个entry字段包含VPN、PFN以及其它的功能bit位。VPN和PFN的功能很明显就是不经由寄存器，直接由VPN映射PFN。我们由功能延申一下其它bit位的含义，首先是最重要的ASID位。当进行context switches时，不同进程的TLB可能会出现冲突，我们在entry字段中安放ASID字段作为进程的标识符。固定的，entry字段中还会有valid bit、protection bit、dirty bit等。

<img src="http://1.14.100.228:8002/images/2022/02/13/20220213191100.png" style="zoom:80%;" />

以简化版MIPS R4000系统中entry字段为例，由图可知，该系统中一个entry字段长64 bytes，包含除了我们上面说的部分以外，还有global bit、coherence bit等部分。

<img src="http://1.14.100.228:8002/images/2022/02/13/20220213191111.png" style="zoom:80%;" />

## 分页（三）

如果简单的使用linear page table，那么每个进程的page table所占用的内存加起来会十分巨大。如果不解决这一问题page将难以实用。我们有许多不同的思路来解决这一问题。

首先是直接增大page size。假如增大n倍的page size，那么page table的空间占用也会减少n倍，但是很明显这样会造成internal fragmentation问题，page size越大，page内部空间被浪费的概率就越高。不过由于这一方法的实现简单，现在multiple page size被应用于不少现代体系结构如MIPS、SPARC、x86-64等，但是主要是用于减少TLB miss的压力，而不是节省页表空间。

另一种更加复杂的思路是混合page和segment，给进程中每一个segment一个page table，同样的，给每一个segment配一对base-bound。只不过这里的base用于记录page table的起始物理地址，bound用于记录page table的结束物理地址。通过这种方式page table只记录已经分配的部分，大大减少了page table的内存消耗，但是这样的话就又会出现segmentation的external fragmentation问题。

<img src="http://1.14.100.228:8002/images/2022/02/14/20220214163417.png" style="zoom:67%;" />

page table只是数据结构，我们用很多方式去更改它以达到我们需要的目的，一种方式是multi-level page table多级页表。它的基本思想就是，如果一个page没有被具体分配，那么就不在page table里给它分配空间。听上去和hybrid有些像是么？我们使用一种更灵活的实现方式，以两级页表为例，定义一种新的线性数据结构page directory。其中的内容称为page directory entry，包含两个基本信息，valid bit和PFN，如果valid bit为0，则不需要记录PFN。如果valid bit为1，则可直接通过PFN找到所属的page进行访问，具体的抽象结构如图

<img src="http://1.14.100.228:8002/images/2022/02/14/20220214163458.png" style="zoom:67%;" />

multi-level page table有两个好处，一是最为关键的，由它只给地址空间中正在使用的部分分配页表空间，因此天然是紧凑的并且支持稀疏的地址空间。二是page table中每一个部分都完整的在一个page中，这使得它本身的空间容易进行管理，操作系统可以简单的进行增删。

但是multi-level page table也有两个坏处，一是明显的，当出现TLB miss时，它所耗费的代价更大，因为需要两次访问进行新table的构建。二是增加了页表查找的复杂度，这一点也就是一所指的。

我们以一个实际的例子来理解multi-level page table的工作过程，一个地址空间大小为16KB，page size为64 bytes，那么virtual address为14位，其中8位是VPN，6位是offset。假如还是使用linear page table的话，page table有256个entry，每个entry占用4 bytes，那么这些entry一共占用16个pages，也就是说我们需要4个bit位的page directory index来进行page table的page索引，这4个位包含在VPN中，余下4个bit位用于在page中索引page table。如下图所示

<img src="http://1.14.100.228:8002/images/2022/02/14/20220214163524.png" style="zoom:67%;" />

索引到page directory后，根据page directory对page table进行映射，在本例中，原本要占用16个连续pages的page table只占用3个pages，如下图所示

<img src="http://1.14.100.228:8002/images/2022/02/14/20220214163536.png" style="zoom:67%;" />

与TLB联系在一起，工作流程的伪代码如下

<img src="http://1.14.100.228:8002/images/2022/02/14/20220214163548.png" style="zoom:67%;" />

正如上面所说page table只是数据结构，还有其它更加节省空间的设计方式如inverted page tables等，但是更加小的空间占用无疑会带来更加大的索引开销，这依然是一个trade-off问题。并且内存问题并不是只有这一种解决思路，当内存紧张时，通过kernel virtual memory与硬盘进行swap也是一种page table内存问题的解决方法。

## 交换空间（一）

之前所讲的page策略一直都是基于一个基本的假设，那就是进程的地址空间都比较小，至多不会大于内存总大小。但是在现实中，进程随时有可能会申请非常大的地址空间，更不用说在现代操作系统中多进程同时进行是常态了。为了在此时也能够达到内存的虚拟化，swap space的概念被提出了

抽象的说，swap space就是在disk space硬盘中保留的一片可以由操作系统以page形式读或写的区域，通过这一区域，操作系统能够将进程中不常用的部分交换到访问速度慢非常多的硬盘中，以此给内存缓解压力。

交换空间相当于是内存空间“虚假”的扩大，它的大小决定了操作系统在给定时间内可以使用的最大内存页数，以四个进程为例，可以看到不同进程都各有一部分的空间在swap space中，处于blocked状态的进程则有可能全部在swap中，如Proc3。

<img src="http://1.14.100.228:8002/images/2022/02/15/20220215151518.png" style="zoom:67%;" />

为了嵌入这一新的机制，需要加入一个present bit抽象结构来表示page是否处于disk中，这里我们在page table entry的结构中加入一位，当访问到page table中present bit为0时，表示这一个page实际在disk中，触发page fault trap，由page fault handler来处理。此时操作系统得到这个page的disk address，将其加载回内存中，再重新执行内存访问，之后的过程就和正常的TLB miss一样。需要注意的是，在加载回内存之前，操作系统需要判断内存是否足够，如果不够，则会触发replacement polices，将一部分内存中的page换到swap space中，这也是我们第二部分要讨论的。到目前位置，一个完整的基于硬件的page fault处理流程如下图。

<img src="http://1.14.100.228:8002/images/2022/02/15/20220215151552.png" style="zoom:67%;" />

replacement polices（RP）对于swap space非常重要，硬盘访问的速度比内存慢足足1万到10万倍，如果RP经常判断失误，很可能造成显著的处理速度变慢。操作系统用high watermark（HW）和low watermark（LW）来确保总是有一些空余内存，当内存低于HW，操作系统调用swap daemon也就是RP的执行线程来释放内存直到高于HW。

分组或集群是一类优化swap效率的方法，通过将多个page集群或分组，统一的swap入disk中能够显著的提高性能。

## 交换空间（二）

操作系统通过swap space扩大内存，保留访问活跃的page，每一次都通过replacement policy来选择哪一个page来换出内存。我们只看page与进程的关系的话，某种意义上高速内存可以看作整个memory space的cache，衡量一个replacement policy好不好的指标可以是AMAT，具体公式如下

<img src="http://1.14.100.228:8002/images/2022/02/16/20220216191616.png" style="zoom: 80%;" />

其中Tm是访问内存的开销，Td是访问硬盘的开销，Pm是cache miss的概率，通过公式量化可以算出，由于内存和硬盘速度上的差异，进程访问内存的开销会随cache miss次数呈现巨大差异。事实上在高速硬盘出现前，页面替换算法的重要性是在降低，因为内存和硬盘的差距实在是太大了，频繁替换的成本太高，但是现在有SSD，所以替换算法的重要性也陡然增加。

准寻着由易到难的设计思路，replacement policy可以分为FIFO和Random、LRU以及LRU的变体。在进程未来所有的内存访问我们都已知的情况下，我们可以让每一次replacement去除掉最远以后才访问的page，这是最优的情况。但是很明显这是不可能的，我们只能尝试去接近这个目标，FIFO和Random是简单的初步尝试，从名字就可知道，FIFO像队列一样处理page的替换顺序，Random则是随机选择替换page，模拟如下图所示

![](http://1.14.100.228:8002/images/2022/02/16/20220216182531.png)

FIFO实现简单，但是性能很差，Random同样实现简单，并且性能不稳定，但是在边界情况下也能发挥出性能。

在计算机设计中有locality的概念，即某一个对象在被触发后，具有spatial locality和temporal locality的feature，假如P被访问，则P+1和P-1更有可能被访问，并且接下来P更有可能被再次访问。利用这种locality，Least-Recently-Used（LRU）方法被提出，它的基本思想就是，设当前page被访问的时间为T，那么里现在最久没被访问的page则最没有可能被访问，优先换出最远的page。为了实现LRU，一种实现方法是假设page table或者物理空间中存储page中上一次被访问的时间，在出现需要换出page的时候搜索所有page中访问时间最远的那个page。

然而，对所有page进行搜索，尤其是现代系统page动辄上百万的数量，是极其耗费时间的。因此，近似LRU被提出了，加入了两个概念：use bit和clock。通过硬件支持，当进程每一次进行内存访问时，硬件将访问的page所属的use bit设为1，当需要换出page时，操作系统从clock hand指向的page开始遍历（或者其它任何方式，随机也可以），遇到1，则置为0，遇到0，则确认这一个就是要换出的page。

<img src="http://1.14.100.228:8002/images/2022/02/16/20220216185024.png" style="zoom:67%;" />

基础的policies大概的思想就是这些，以下是它们的性能对比图，越接近黄线的最优越好，左上角的是当内存访问没有locality的一种边界情况，右下角的是一种只有Random有效的边界情况

<img src="http://1.14.100.228:8002/images/2022/02/16/20220216185447.png" style="zoom:80%;" />

还有一些swap的优化方法，之前介绍page table中有一种dirty bit位，用于标记page是否进行了IO，如果没有进行IO，说明其数据相较于disk没有改动，如果要换出的话可以直接覆盖，如果进行了IO，那么就要将其写回disk中。有的policy对于没有进行IO的page更优先swap。还有page selection策略用于判断操作系统何时进行page swap，还有clustering and grouping策略用于增加swap的效率，这些policies没有replacement那么重要但是在现代操作系统中也是普遍存在的。

## 完整的虚拟系统

到目前为止我们已经学习了很多CPU虚拟化和内存虚拟化的底层机制和设计策略，现在我们以两个经典的操作系统分别是：VAX/VMS、Linux为例来看一下现代操作系统比较完整的虚拟化技术栈是怎样的。

### VAX/VMS

VAX/VMS operating system是一款在七十年代末由Digital Equipment Corporation（DEC）开发出来的经典操作系统，其中有关虚拟化的技术很多是今天现代操作系统的来源，当时VAX系列系统需要在各种各样硬件条件的机器上运行，因此，操作系统系统必须有在那么大范围内良好工作的机制和策略。

VAX-11是一款 32-bit address space，512-bytes page size的系统，一条virtual address中包含23-bit VPN和9-bit offset，并且头两位的VPN用于标记page所属的是哪个segment。因此，这个系统基本内存策略是hybrid of paging and segmentation。

可以看出VAX-11的page size十分的小，为了确保VAX-11不会因为page table占太多空间。系统地址空间分为上下两半部分，上半部分P用于用户进程使用，下半部分(S)用于内核进程使用。这两段分别有不同的page table，防止外部碎片。其中内核page table所有进程共享，用户的进程的page table保存在kernel virtual address space中，通过这两种方法缓解page table占用问题。

系统地址空间上半部分又分为两个等大的部分P0和P1，P0用于code和heap，P1用于stack。如下图所示，其中我们规定page 0是作为NULL无效的，任何更改page 0的操作都会触发trap。这对代码调试有很大的帮助。

<img src="http://1.14.100.228:8002/images/2022/02/17/20220217204544.png" style="zoom:67%;" />

在上下文切换的时候，操作系统更换P0和P1的寄存器，但是S的寄存器始终是固定的。将系统进程放进address space的好处有很多，例如它只需要负责本进程的系统调用即可，进行数据交互也无需额外的步骤，就像一个固定的程序库一样，虽然比较占空间，但是这种构造因其优越的性能得到现代操作系统的广泛采用。

VAX系统的page table entry包含valid bit、protection field、dirty bit、留给OS使用的5 bits以及PFN。并没有present bit，就算有，LRU策略并不是进程公平的，如果某一个进程一直大量申请空间，LRU会将其它进程的几乎所有page都swap出去。VAX使用不依赖present bit的segmented FIFO来进行page replacement。具体规则就是每个进程都有它可以在内存中保存的最大页数，当页数超过限额时“先入”的页会被换出，这似乎与普通的FIFO一样，分段FIFO还引入了两个列表，分别时clean-page free list和dirty-page list。当有页面要换出时，会根据dirty bit放入两个列表中，当另一个进程需要空间时，先从clean-page free list中取页面进行覆盖，可以免去与disk进行交互。对于dirty-page list，因为磁盘在大的传输中表现更好，为了使交换I/O更有效率，VMS使用clustering策略将list中的page分组一起交换出去。

VAX系统另外有两种“延迟“优化方法，也叫lazy optimal。它们分别是demand zeroing和copy-on-write。其中demand zeroing是大多数现代操作系统都会有的一种lazy策略，当进程申请空间的时候，操作系统只在page table中放一个标记表示已经占用，当实际做IO的时候才触发trap让操作系统找到对应物理内存，清零并将其映射回地址空间。同样，copy-on-write也是几乎所有的现代操作系统都会有的一种lazy策略，当操作系统需要将一个页面从一个地址空间复制到另一个地址空间时，它可以将其映射到目标地址空间，并在两个地址空间中都标记为只读，当某一个进程对其进行写操作的时候，触发一个trap，操作系统将其内容真正复制到另一个物理地址上并映射回地址空间。lazy optimal是一种十分有效且应用广泛的优化设计思想，其它的例子还有操作系统在接收到文件处理的指令后立即返回处理成功，但是实际在后台慢慢的处理文件。

### Linux

Linux的虚拟内存系统功能齐全、特性丰富且相当复杂，Linux的发展是由真正的工程师解决生产中遇到的实际问题而推动的，因此它的功能也在不断迭代。Linux和老系统有一些共同点，但是也有很多方面超越了传统的虚拟机系统（如VAX/VMS）。我们讨论最主要的英特尔x86的Linux上的虚拟内存系统。

同样的，Linux的地址空间有内核和用户两部分，同样内核部分在不同进程中是相同的。在经典的32位Linux中，地址空间的用户和内核部分的分割发生在地址0xC0000000处，或者是地址空间的四分之三处，而64位的Linux分割的点略有不同。

从下图我们可以看出，在它的内核空间中有两种内核虚拟地址，第一种被称为kernel logic address，大多数内核的数据结构都在这里，且内核逻辑地址与物理内存有直接的映射联系，例如内核逻辑地址0xC0000000转换为物理地址0x00000000，是有一一对应关系的。这么做有简化内核逻辑地址访问的作用，同时这使得在内核地址空间的这一部分分配的内存适合于需要连续的物理内存才能正常工作的操作，比如通过目录内存访问（DMA）进行的设备间的I/O传输。另一种是kernel virtual address，这是一片是由操作系统主动申请，可以自由访问的空间，与内核逻辑内存不同，内核虚拟内存通常不是连续的，它的具体作用有很多，这里就不展开细讲了。

<img src="http://1.14.100.228:8002/images/2022/02/17/20220217221718.png" style="zoom:80%;" />

x86的Linux有基于硬件的多级page table structure，操作系统只需在其内存中设置映射，将一个privileged register指向页目录的开始，其余部分由硬件处理。具体的，目前64位系统使用四级页表。然而，虚拟地址空间的全部64位性质还没有被使用，而只是底部的48位，随着系统内存的增长，地址空间的更多部分将被启用，从而产生五级乃至六级的页表树结构，如下图所示

<img src="http://1.14.100.228:8002/images/2022/02/17/20220217221731.png" style="zoom:80%;" />

Intel x86架构允许使用多种页面大小page，最近的设计在硬件上支持2MB甚至1GB的页面。Linux也使用这些huge page来提升TLB和其它相关的性能。因为如果page总是那么小，那么一个大内存的访问就会把TLB迅速填满，造成大量的TLB miss，最近的研究表明，一些应用程序花了10%的周期来处理TLB miss。Linux对huge page的支持是以一种渐进的方式发生的，起初，Linux的开发者知道这种支持只对少数应用程序（eg.数据库）很重要，提供系统调用来申请huge page，由于对更好的TLB行为的需求在许多应用程序中更为普遍，Linux开发者增加了transparent huge page support。具体的说，操作系统系统会自动寻找机会来分配巨huge page提升TLB性能。

为了提升访问硬盘的性能，大多数操作系统包括Linux都会建立cache subsystem将活跃的数据保留在内存中，内存中保留的page来自三个方面：内存映射的文件、来自设备（eg.硬盘）的文件数据、元数据（eg.独立的数组变量）以及每个进程的heap和stack（有时被称为anonymous memory，因为它的下面没有命名的文件，而是交换空间）。这些entry被保存在一个page cache hash table中，当需要上述数据时，可以快速查找。

page cache跟踪这些entry是clean还是dirty。脏数据被后台线程定期写到支持存储（即，对于文件数据写到一个特定的文件，或者对于heap和stack则写到交换空间），这种后台活动要么在某个时间段后进行，要么在有太多页面被认为是脏的情况下进行。这么做是为了帮助Linux判断哪些page是要从内存中swap出去的，并且Linux使用一种修改过的2Q replacement策略来进行page replacement。如果一个进程读取大量数据，但是却使用，那么LRU这时是没用的，2Q replacement创建两个列表分别称为active list活动列表和inactive list非活动列表，并将内存分给它们，当page第一次被访问时被放入非活动列表中，当被重新引用时，这个page被提升到活动列表中，当需要进行替换时，替换的候选页会从非活动列表中取出。Linux也会定期地将页面从活动列表的底部移到非活动列表中，使活动列表保持在总页面缓存大小的三分之二左右。这样做就可以处理LRU无法处理的情况。

硬盘与内存的映射出现的比Linux早一些年，在Linux中有一个系统调用mmap()能够指出内存映射在地址空间上的位置，事实上在Linux中每个普通的Linux进程都使用内存映射的文件，我们调用pmap命令行工具能够查看搭配进程中文件的映射情况

<img src="http://1.14.100.228:8002/images/2022/02/17/20220217224926.png" style="zoom:67%;" />

从上图可以看到来自tcsh二进制的代码，以及来自libc、libcrypt、libtinfo的代码，还有来自动态链接器本身（ld.so）的代码都被映射到了地址空间。此外，还有两个匿名区域，即堆（第二个条目，标记为anon）和栈（标记为stack）。存映射的文件为操作系统构建地址空间提供了一种直接而有效的方法。

以上都不是Linux和老操作系统的最大区别，它们之间的最大区别在于现代操作系统对安全的重视，buffer overflows是一种针对系统缓冲区溢出的攻击手段，这种漏洞的出现有时是因为开发者设计失误，当系统得到一个过长的输入，会发生缓冲区溢出，有人将溢出的恶意程序部分覆盖到目标的内存，成功注入到目标系统中，如果在与网络连接的用户程序上攻击成功，攻击者可以在被攻击的系统上运行任意的计算（挖矿）。AMD在他们的x86版本中引入NX bit位来标记地址空间的某些区域不能够运行代码来防御这种攻击，但是攻击者发现即使注入的代码不能被攻击者明确添加，任意代码序列也能被恶意代码执行。这个想法在其最一般的形式下被称为面向返回的编程（ROP）。为了防御ROC，Linux（和其他系统）增加了另一种机制，称为地址空间布局随机化（ASLR）。将代码、堆栈和堆放在虚拟地址空间的随机位置，从而使制作实现这类攻击所需的复杂代码序列变得相当有挑战性，在Linux中可以模拟这种随机放置，如右图所示，变量stack的地址每次都不一样。

<img src="http://1.14.100.228:8002/images/2022/02/17/20220217230433.png" style="zoom:67%;" />

ASLR对于用户级程序来说是一个非常有用的防御手段，因此它也被纳入了内核，其功能被称为内核地址空间布局随机化（KASLR）然而安全问题不止这些，在2018年，system相对安全的世界被两个新的相关攻击颠覆了。第一个叫做Meltdown，第二个叫做Spectre。它们大约在同一时间被四组不同的研究人员/工程师发现，并导致我们对计算机硬件和上述操作系统提供的基本保护提出深刻质疑。

这些攻击所利用的是现代CPU处理推测性行为的方式，CPU猜测哪些指令将在未来很快被执行，并提前开始执行它们。如果猜测是正确的，程序就会运行得更快，这样做能够显著增加系统运行速度，但问题是，它往往会在系统的各个部分留下其执行的痕迹，这种状态可以使内存的内容变得脆弱，甚至是我们认为受到MMU保护的内存。因此，增加内核保护的一个途径是将尽可能多的内核地址空间从每个用户进程中删除，因此，不再将内核的代码和数据结构映射到每个进程中，而是只保留最基本的内容，当切换到内核时，现在需要切换到内核页表，这种策略称为kernel table isolation。这样做提高了安全性，但是也降低了一部分性能，并且没有解决上面列出的所有的安全问题，而最简单的解决方案就是终止对推测性行为的优化，但这是不可能的，因为系统的运行速度会慢几千倍。、

从上面的描述我们可以看出想要真正理解现代操作系统，要先理解操作系统的安全。







