我主要关注大模型的部署增强技术

增加上下文

~~~
背景
大模型的推理本质是在已有知识（参数量）上玩猜字游戏，其记忆能力是因为transformer可以根据历史输入来推理下一个字，一个字一个字的拼起来，注意力机制使得超出token长度限制的输入失去位置编码，回答就会变得乱七八糟。而token越长，计算复杂度越大，因此需要大模型在底层就支持更大的token长度

外推
1.滑动窗口
2.优化位置编码
3.引入向量数据库

减少模型参数量

注意力化简
1.稀疏注意力（只关注最近/随机）
2.线性注意力（减少计算量）

运算优化
1.矩阵GPU运算在缓存上的优化
~~~

