## 基础概念

##### **logistic**函数

$$
\hat{y}=\sigma(w^Tx+b) \ where\ \sigma(z)=\frac{1}{1+e^{-z}}
$$

$$
\delta(\hat{y},y)=-(y*log\hat{y}+(1-y)log(1-\hat{y}))
$$

$$
J(w,b)=\frac{1}{m}\sum^m_{i=1}\delta(\hat{y},y)=\frac{1}{m}\sum^m_{i=1}(-(y*log\hat{y}+(1-y)log(1-\hat{y})))
$$

$$
w:=w-\alpha\frac{dJ(w)}{dw}
$$

$$
b:=b-\alpha\frac{dJ(w,b)}{db}
$$

##### softmax函数

softmax函数使变量按照大小分配概率，解决a>b的绝对概率问题。
$$
softmax(likelihood):s_i=\frac{e^{v_j}}{\sum_{0}^{j}e^{vj}}
$$

![softmax_example](D:\Coder\Github\学习周记\image\softmax_example.png)

##### ReLU函数

Sigmoid函数或者Tanh函数只有其输入指定范围的值才能获得良好的线性梯度，一旦输入变大，梯度就会降为0，故在神经网络的反向传播中多使用ReLU函数

在卷积神经网络中，ReLU函数为网络增加了非线性，为正输入提供非饱和梯度

##### TPU

 [Use TPUs  | TensorFlow Core](https://www.tensorflow.org/guide/tpu) 

张量处理单元（TPU）是一种定制化的 ASIC 芯片，它由谷歌从头设计，并专门用于机器学习工作负载 ， 它专注于神经网络推断。这使得量化选择、CISC指令集、矩阵处理器和最小设计都成为可能。

##### 交叉熵损失函数(cross entropy error function)

$$
L=-[ylog\hat{p}+(1-y)log(1-\hat{p})]二分类\\
L=\frac{1}{N} \sum_{i} L_{i}=\frac{1}{N} \sum_{i}-\sum_{c=1}^{M} y_{i c} \log \left(p_{i c}\right)多分类\\P(y|x)=\hat{p}^y*(1-\hat{p}^{1-y})\ (0<\hat{p}<1)
$$

y为label的值，p为样本为真的概率(softmax)，M为类别的数量

在用梯度下降法做参数更新的时候，模型学习的速度取决于两个值：一、**学习率**；二、**偏导值**。所以使用逻辑函数得到概率，并结合交叉熵当损失函数时，在模型效果差的时候学习速度比较快，在模型效果好的时候学习速度变慢。

在分类问题中，需要定义一个classification error来判定网络错误的离谱程度，而由于错误的离谱方式不同，选用何种error function来体现量化离谱程度就非常重要，这样，当离谱值大时梯度也要大，当离谱值小时梯度也要小，才能动态调整达到加速训练的效果。

但缺点就是原理比较简单，导致大量分类时出现准确率不足的问题，原因即是1.分类层线性变化矩阵参数也变大2.所学特征没有足够的区分性

##### 随机初始化

神经网络中节点权值W不能初始化为0，也不能单纯用随机数初始化

![屏幕截图 2020-12-28 153257](.\image\屏幕截图 2020-12-28 153257.png)

#### 数据扩充

mirroring

对图像沿垂直轴进行水平翻转

Random Cropping

剪裁出图片中特征较强的部分

colorshifting

对R、G、B值进行在分布上有规律的加减

### 正则化 (regularization)

* 如果网络对数据过拟合，可以增加训练数据，或使用正则化方法

* 可直接作为损失函数使用

##### L2正则化

$$
J(w,b)=\frac{1}{m}\sum_{i=1}^{n}\sigma(\hat{y}^i,y^i)+\frac{\lambda}{2m}||w||^2_2\\||w||^2_2=\sum_i^{l-1}\sum_j^{l}w_{ij}^2=w^Tw
$$

L2正则化最主要的用处即是防止过拟合，由于二次函数的性质，ta具有非常良好的权重衰减功能

![l2](.\image\l2.png)

##### droupout正则化(inverted dropout)

对一层神经元设置超参数keep-prob，随机失活神经元

![drop_out](.\image\drop_out.png)

drop-out在计算机视觉网络中应用非常广泛，作用有些类似于L2正则化

缺点：

##### L1正则化

$$
J(w,b)=\frac{1}{m}\sum_i^{l-1}\sum_j^{l}\sigma(\hat{y}^i,y^i)+\frac{\lambda}{m}||w||_1\\||w||_1=\sum_i^{l-1}\sum_j^{l}|w|
$$

因为一次函数特性，l1函数使模型明显稀疏。有一些防止过拟合作用

![l1](.\image\l1.png)

##### 其它正则化方法

early stopping方法

在网络训练过程中，网络的值逐渐变大

![early-stopping](.\image\early-stopping.png)

使用early-stopping方法比较方便，但是该方法在取到了合适的w的同时忽视了网络的训练成熟度，即损失函数还未达到最优就停止训练了

#### 梯度消失和梯度爆炸

当网络深度很高时，随着网络层数增加，权重W很可能以指数变大，或以指数减小，造成训练梯度过大或过小

参考解决方法，对每一层网络的权重随机初始化值进行处理

![梯度消失和梯度爆炸](.\image\梯度消失和梯度爆炸.png)

##### 梯度检验

手动实现的反向传播很有可能出现问题，通过单独计算损失函数J(x)在x上的双向误差积分，与反向传播的dx相减求欧式距离再除以梯度和积分的范数，由结果评估反向传播的效果。

![梯度检验](D:\Coder\Github\学习周记\image\梯度检验.png)

### 规范化(normalization)

##### 批规范化



### 优化算法(optimizer alogrithms)

指数加权平均数

通过取不同权重的β，做一段数据的平均值近似。

![指数加权平均](D:\Coder\Github\学习周记\image\指数加权平均.png)

##### MomentumOptimizer

动量梯度下降可以减少梯度在纵轴上的大小，增大在横轴上的大小

![momentum](D:\Coder\Github\学习周记\image\momentum.png)

使用momentum算法使梯度下降的更快，可以轻易的离开local minimal

##### RMSprop

均方根梯度下降可以减少梯度在各个方向上的变化幅度

![RMSprop](D:\Coder\Github\学习周记\image\RMSprop.png)

使用RMSprop算法使梯度下降的更快，并允许使用更大的学习率

##### AdamOptimizer

Adaptive Moment estimation是Momentum算法和RMSprop算法的结合，具有两者的优点，具有良好的模型泛化能力

参数：α：自动衰减、β1：0.9、β2：0.999、ε：一般不用调

![Adam](D:\Coder\Github\学习周记\image\Adam.png)

#### 网络超参数取值

抄袭原则：看别人论文调参

随机原则：画KPI曲线图

权重原则：距离点占权重很小

### 迁移学习与多任务(多GPU)学习

* 任务AB输入输出格式相同
* A的低级特征可以帮助任务B的进行(AB共享一些低级特征)
* 迁移学习与多任务学习所需的样本数量不一致

![迁移学习](D:\Coder\Github\学习周记\image\迁移学习.png)

在前几个卷积层中，训练得到的都是比较简单通用的特征

当训练集与预训练模型训练集性质差异不大时，只有全连接层与输出层的权重需要重新训练

当训练集与预训练模型训练集性质差异很大时，可以先冻结网络前几层的权重，对后部分进行重新训练



## 卷积神经网络(convolution neural network)

#### 卷积层(convolution layer)

卷积层越深，网络就能学到越复杂的特征，从边缘线到几何形状到动物的眼睛等等。

##### 卷积核、滤波器：通常为奇数

![1608452634683](.\image\1608452634683.png)

卷积核大小(size)表示感受域大小，一般为3、5、7

不同的滤波器内核尺寸会基于滤波器感受野大小进行提取不同粒度级别的特征信息，3x3会比5x5提取到更细粒度级别的特征信息

##### 鲁棒性(robust)

 算法对数据变化的容忍度 ，向算法内添加噪声（如对抗训练），可以测试算法的「鲁棒性」。 

##### pedding

通过计算步长带来的损失，用0填充外围像素

![屏幕截图 2020-12-25 140637](.\image\屏幕截图 2020-12-25 140637.png)

##### 卷积步长(stride)

 卷积核的步长度代表提取的精度 ，Size为3的卷积核，如果step为1，那么相邻步感受野之间就会有重复区域；如果step为2，那么相邻感受野不会重复，也不会有覆盖不到的地方；如果step为3，那么相邻步感受野之间会有一道大小为1的缝隙，从某种程度来说，这样就遗漏了原图的信息，直观上理解是不好的。

当卷积核step为1时，输出矩阵Size会和输入矩阵Size保持一致；而卷积核step为2时，由于跨步会导致输出矩阵Size降低为输入矩阵Size的一半。由此会产生一种类似“池化”的效果，利用这一特性可以代替池化层。 

 步长小，提取的特征会更全面，不会遗漏太多信息。但同时可能造成计算量增大，甚至过拟合等问题。步长大，计算量会下降，但很有可能错失一些有用的特征。 

##### 三维卷积(通道passage)

![屏幕截图 2020-12-26 155323](.\image\屏幕截图 2020-12-26 155323.png)

##### 单层卷积神经网络

![屏幕截图 2020-12-26 164638](.\image\屏幕截图 2020-12-26 164638.png)

#### 池化层(pooling layer)

图片的相邻像素具有相似的值，因此卷基层中很多信息是冗余的。通过池化来减少这个影响，用于缩减模型的大小，提高计算速度，提高所取特征的鲁棒性，同时减少过拟合，拥有invariance(不变性)，这种不变性包括translation(平移)，rotation(旋转)，scale(尺度)。 **[**池化层没有权重**]**

##### 最大池化(max pooling) 

能很好的保留图片纹理特征

##### 平均池化(average pooling)

 能很好的保留背景，但容易使得图片变模糊

##### 全连接层(full contact)

就是经典隐藏层，因为与输入特征全部相连，所以称为全连接层。

![屏幕截图 2020-12-26 185026](.\image\屏幕截图 2020-12-26 185026.png)

##### 经典网络

LeNet-5

![LeNet-5](D:\Coder\Github\学习周记\image\LeNet-5.png)

AlexNet

![Alexnet](D:\Coder\Github\学习周记\image\Alexnet.png)

VGG-16

![VGG-16](D:\Coder\Github\学习周记\image\VGG-16.png)

#### ResNet

残差网络可以帮助网络达到难以到达的深度

#### InceptionNet

模块化网络

### 目标检测

批规范化(Batch Normalization)

批规范化操作，不仅加快了模型收敛速度，而且更重要的是在一定程度缓解了深层网络的一个难题“梯度弥散”，从而使得训练深层网络模型更加容易和稳定。另外，批规范化操作不光适用于深层网络，对传统的较浅层网络而言，批规范化也能对网络泛化性能起到一定提升作用。

#### YoLo

论文思想

* 将图片分为SxS的网格，如果object的中心位于某个网格中，那么该网格就负责预测这个object
* 每个网格预测B个bounding box，每个box预测xywh和confidence值(IOU)
* 在每个卷积层后加上BN层，对于训练收敛有巨大的帮助，并具有一定防过拟合的正则化作用，使用BN后可以不用dropout
* 使用更高的分辨率输入图片
* 采用anchor，能够使网络更好的收敛，使recall上升，模型更好调
* 基于训练集box，采用k-means获取anchor
* 将低层特征图与高层特征图融合，使网络能更好的检测小目标
* 每迭代10个batch，网络就随机输入一个32倍数的图像大小
* 误差计算很特殊

交并比(IOU-intersection over union)

两个bounding box的交集与并集比，为1则完全重合

anchor boxes(锚框)

anchor boxes概念的提出是为了解决同一个grid cell中有多个object的中心存在，预先定义一定数量一定形状的bounding box，当输入图像时，令object与anchor box中IOU最大的box作为该object的输出

非极大值抑制

### 为什么卷积网络如此有用？

* 卷积神经网络一开始被设计出来即是为了解决传统神经网络处理图像的偏移和形变所造成的巨大误差。
* 卷积层用于特征提取，池化层用于特征组合
* 特征映射是分类的关键

## 循环神经网络(Recurrent Neural Network)

## TensorFlow-v2.1.0

### tensorflow底层概念

##### 图(graph)

 一个`Graph`包含一系列`tf.Operation`对象和`tf.Tensor`对象；`tf.Operation`对象代表计算单元，`tf.Tensor`对象代表operation之间流动的数据单元). 

用 tf.Graph.as_default 创建图。

##### 会话(session)

 `Session`对象封装了执行`Operation`对象和计算`Tensor`对象的环境。  调用`tf.Session.close`方法 ，释放会话占用的资源。

 如果在构造会话时没有指定`graph`参数，则将在会话中启动默认图。但是每个图都可以在多个会话中使用。 

##### 变量(varible)

 [变量简介  | TensorFlow Core](https://www.tensorflow.org/guide/variable) 

 变量通过 `tf.Variable`类进行创建和跟踪。`tf.Variable`表示张量，对它执行运算可以改变其值。利用特定运算可以读取和修改此张量的值。大部分张量运算在变量上也可以按预期运行，不过变量无法重构形状。 

##### 张量(tensor)

 [张量简介  | TensorFlow Core](https://www.tensorflow.org/guide/tensor) 

 张量是具有统一类型（称为 `dtype`）的多维数组，与 `np.arrays` 有一定的相似性，就像 Python 数值和字符串一样，所有张量都是不可变的：永远无法更新张量的内容，只能创建新的张量。 

张量有形状。下面是几个相关术语：

- **形状**：张量的每个维度的长度（元素数量）。
- **秩**：张量的维度数量。标量的秩为 0，向量的秩为 1，矩阵的秩为 2。
- **轴**或**维度**：张量的一个特殊维度。
- **大小**：张量的总项数，即乘积形状向量

### Keras

one-hot处理

```python
iris_target = np.float32(tf.keras.utils.to_categorical(iris_target, num_classes=3))
```

输入层input

```python
input_xs  = tf.keras.Input(shape=(4), name='input_xs')
```

中间层Dens

```python
x = tf.keras.layers.Dense(32, activation='relu')(inputs)
```

模型保存与调用

```python
model.save()
new_model = tf.keras.models.load_model('./saver/the_save_model.h5')
model = tf.keras.Model(inputs=inputs, outputs=predictions)
```

compile函数Tensorflow2.0专用于配置训练模型的编译函数

fit函数用于对输入数据进行修改(泛化)

 [The Sequential model  | TensorFlow Core](https://www.tensorflow.org/guide/keras/sequential_model) 

[Module: tf.keras  | TensorFlow Core v2.4.0](https://www.tensorflow.org/api_docs/python/tf/keras) 

### Dataset



## Pytorch-v1.7

# ?

为什么顺序编程那么抽象？怎样的编程算不抽象？为什么有时要对数据进行重新计算？什么是Keras函数式API？神经网络中的批次具体是什么？点在特征空间上分不开到底是因为什么？怎样对已保存的模型进行更改？权重随网络深度增加会怎样变化？为什么？

yolo网络在运行是是怎样得到大概的box？

yolo回归是啥？为啥可以这样回归？

视觉里recall怎么定义？意味着什么？

yolo网络所需的数据集需要哪些标注？

downsample是啥？为什么会造成特征采集粗糙？

数据集怎样载入，原理是什么？

正负样本？ground truth？

问老师

训练数据的问题，有哪些通用的经验

yolo算法运行时的数学逻辑混乱，该怎么办

问同学

openpose能不能达到像opencv的要求

# 要学什么

* 了解RNN对语义的处理逻辑，即怎么做到的，能不能做到
* 了解视觉数据基本原理和处理方法(老师)
* 了解openpose的属性，看能不能满足要求