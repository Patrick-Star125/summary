## 基础概念

##### **logistic**函数

$$
\hat{y}=\sigma(w^Tx+b) \ where\ \sigma(z)=\frac{1}{1+e^{-z}}
$$

$$
\delta(\hat{y},y)=-(y*log\hat{y}+(1-y)log(1-\hat{y}))
$$

$$
J(w,b)=\frac{1}{m}\sum^m_{i=1}\delta(\hat{y},y)=\frac{1}{m}\sum^m_{i=1}(-(y*log\hat{y}+(1-y)log(1-\hat{y})))
$$

$$
w:=w-\alpha\frac{dJ(w)}{dw}
$$

$$
b:=b-\alpha\frac{dJ(w,b)}{db}
$$

##### softmax函数

softmax函数使变量按照大小分配概率，解决a>b的绝对概率问题。
$$
softmax(likelihood):s_i=\frac{e^{v_j}}{\sum_{0}^{j}e^{vj}}
$$

##### TPU

张量处理单元（TPU）是一种定制化的 ASIC 芯片，它由谷歌从头设计，并专门用于机器学习工作负载 ， 它专注于神经网络推断。这使得量化选择、CISC指令集、矩阵处理器和最小设计都成为可能。

##### one-hot处理

将一个序列化数据分散到不同数据空间进行表示

```python
iris_target = np.float32(tf.keras.utils.to_categorical(iris_target, num_classes=3))
```

##### 交叉熵损失函数

$$
L=-[ylog\hat{y}+(1-y)log(1-\hat{y})]\\P(y|x)=\hat{y}^y*(1-\hat{y}^{1-y})\ (0<\hat{y}<1)
$$

在用梯度下降法做参数更新的时候，模型学习的速度取决于两个值：一、**学习率**；二、**偏导值**。所以使用逻辑函数得到概率，并结合交叉熵当损失函数时，在模型效果差的时候学习速度比较快，在模型效果好的时候学习速度变慢。

##### 随机初始化

神经网络中节点权值W不能初始化为0，也不能单纯用随机数初始化

![屏幕截图 2020-12-28 153257](.\image\屏幕截图 2020-12-28 153257.png)

### 正则化 (regularization)

* 如果网络对数据过拟合，可以增加训练数据，或使用正则化方法

* 可直接作为损失函数使用

##### L2正则化

$$
J(w,b)=\frac{1}{m}\sum_{i=1}^{n}\sigma(\hat{y}^i,y^i)+\frac{\lambda}{2m}||w||^2_2\\||w||^2_2=\sum_i^{l-1}\sum_j^{l}w_{ij}^2=w^Tw
$$

L2正则化最主要的用处即是防止过拟合，由于二次函数的性质，ta具有非常良好的权重衰减功能

![l2](.\image\l2.png)

##### droupout正则化(inverted dropout)

对一层神经元设置超参数keep-prob，随机失活神经元

![drop_out](.\image\drop_out.png)

drop-out在计算机视觉网络中应用非常广泛，作用有些类似于L2正则化

缺点：

##### L1正则化

$$
J(w,b)=\frac{1}{m}\sum_i^{l-1}\sum_j^{l}\sigma(\hat{y}^i,y^i)+\frac{\lambda}{m}||w||_1\\||w||_1=\sum_i^{l-1}\sum_j^{l}|w|
$$

因为一次函数特性，l1函数使模型明显稀疏。有一些防止过拟合作用

![l1](.\image\l1.png)

#### 其它正则化方法

##### 数据扩充

就是多搞一点数据，没钱就左右翻转、放大、剪裁，但这些操作最多到在人眼能看出原物的程度下。

##### early stopping方法

在网络训练过程中，网络的值逐渐变大

![early-stopping](.\image\early-stopping.png)

使用early-stopping方法比较方便，但是该方法在取到了合适的w的同时忽视了网络的训练成熟度，即损失函数还未达到最优就停止训练了

#### 梯度消失和梯度爆炸

## 全卷积网络(full convolution neural network)

#### 卷积层(convolution layer)

##### 卷积核、滤波器：通常为奇数

![1608452634683](.\image\1608452634683.png)

卷积核大小(size)表示感受域大小，一般为3、5、7

##### 鲁棒性(robust)

 算法对数据变化的容忍度 ，向算法内添加噪声（如对抗训练），可以测试算法的「鲁棒性」。 

##### pedding

![屏幕截图 2020-12-25 140637](.\image\屏幕截图 2020-12-25 140637.png)

##### 卷积步长(stride)

 卷积核的步长度代表提取的精度 ，Size为3的卷积核，如果step为1，那么相邻步感受野之间就会有重复区域；如果step为2，那么相邻感受野不会重复，也不会有覆盖不到的地方；如果step为3，那么相邻步感受野之间会有一道大小为1的缝隙，从某种程度来说，这样就遗漏了原图的信息，直观上理解是不好的。

当卷积核step为1时，输出矩阵Size会和输入矩阵Size保持一致；而卷积核step为2时，由于跨步会导致输出矩阵Size降低为输入矩阵Size的一半。由此会产生一种类似“池化”的效果，利用这一特性可以代替池化层。 

 步长小，提取的特征会更全面，不会遗漏太多信息。但同时可能造成计算量增大，甚至过拟合等问题。步长大，计算量会下降，但很有可能错失一些有用的特征。 

##### 三维卷积

![屏幕截图 2020-12-26 155323](.\image\屏幕截图 2020-12-26 155323.png)

##### 单层卷积神经网络

 ![屏幕截图 2020-12-26 164638](.\image\屏幕截图 2020-12-26 164638.png)

#### 池化层(pooling layer)

用于缩减模型的大小，提高计算速度，提高所取特征的鲁棒性，同时减少过拟合，拥有invariance(不变性)，这种不变性包括translation(平移)，rotation(旋转)，scale(尺度)。 **[**池化层没有参数**]**

##### 最大池化(max pooling) 

能很好的保留图片纹理特征

##### 平均池化(average pooling)

 能很好的保留背景，但容易使得图片变模糊

##### 全连接层(full contact)

就是经典隐藏层，因为与输入特征全部相连，所以称为全连接层。

![屏幕截图 2020-12-26 185026](.\image\屏幕截图 2020-12-26 185026.png)

#### 为什么卷积网络如此有用？

论文

## TensorFlow-v2.1.0

##### 图(graph)

 一个`Graph`包含一系列`tf.Operation`对象和`tf.Tensor`对象；`tf.Operation`对象代表计算单元，`tf.Tensor`对象代表operation之间流动的数据单元). 

用 tf.Graph.as_default 创建图。

##### 会话(session)

 `Session`对象封装了执行`Operation`对象和计算`Tensor`对象的环境。  调用`tf.Session.close`方法 ，释放会话占用的资源。

 如果在构造会话时没有指定`graph`参数，则将在会话中启动默认图。但是每个图都可以在多个会话中使用。 

##### 张量(tensor)

 张量是具有统一类型（称为 `dtype`）的多维数组，与 `np.arrays` 有一定的相似性，就像 Python 数值和字符串一样，所有张量都是不可变的：永远无法更新张量的内容，只能创建新的张量。 

张量有形状。下面是几个相关术语：

- **形状**：张量的每个维度的长度（元素数量）。
- **秩**：张量的维度数量。标量的秩为 0，向量的秩为 1，矩阵的秩为 2。
- **轴**或**维度**：张量的一个特殊维度。
- **大小**：张量的总项数，即乘积形状向量

##### AdamOptimizer



## Pytorch-v1.7