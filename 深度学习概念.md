## 基础概念

##### **logistic**函数

$$
\hat{y}=\sigma(w^Tx+b) \ where\ \sigma(z)=\frac{1}{1+e^{-z}}
$$

$$
\delta(\hat{y},y)=-(y*log\hat{y}+(1-y)log(1-\hat{y}))
$$

$$
J(w,b)=\frac{1}{m}\sum^m_{i=1}\delta(\hat{y},y)=\frac{1}{m}\sum^m_{i=1}(-(y*log\hat{y}+(1-y)log(1-\hat{y})))
$$

$$
w:=w-\alpha\frac{dJ(w)}{dw}
$$

$$
b:=b-\alpha\frac{dJ(w,b)}{db}
$$

##### softmax函数

softmax函数使变量按照大小分配概率，解决a>b的绝对概率问题。
$$
softmax(likelihood):s_i=\frac{e^{v_j}}{\sum_{0}^{j}e^{vj}}
$$

![softmax_example](D:\Coder\Github\学习周记\image\softmax_example.png)

##### ReLU函数

Sigmoid函数或者Tanh函数只有其输入指定范围的值才能获得良好的线性梯度，一旦输入变大，梯度就会降为0，故在神经网络的反向传播中多使用ReLU函数

在卷积神经网络中，ReLU函数为网络增加了非线性，为正输入提供非饱和梯度

##### TPU

 [Use TPUs  | TensorFlow Core](https://www.tensorflow.org/guide/tpu) 

张量处理单元（TPU）是一种定制化的 ASIC 芯片，它由谷歌从头设计，并专门用于机器学习工作负载 ， 它专注于神经网络推断。这使得量化选择、CISC指令集、矩阵处理器和最小设计都成为可能。

##### 交叉熵损失函数(cross entropy error function)

$$
L=-[ylog\hat{p}+(1-y)log(1-\hat{p})]二分类\\
L=\frac{1}{N} \sum_{i} L_{i}=\frac{1}{N} \sum_{i}-\sum_{c=1}^{M} y_{i c} \log \left(p_{i c}\right)多分类\\P(y|x)=\hat{p}^y*(1-\hat{p}^{1-y})\ (0<\hat{p}<1)
$$

y为label的值，p为样本为真的概率(softmax)，M为类别的数量

在用梯度下降法做参数更新的时候，模型学习的速度取决于两个值：一、**学习率**；二、**偏导值**。所以使用逻辑函数得到概率，并结合交叉熵当损失函数时，在模型效果差的时候学习速度比较快，在模型效果好的时候学习速度变慢。

在分类问题中，需要定义一个classification error来判定网络错误的离谱程度，而由于错误的离谱方式不同，选用何种error function来体现量化离谱程度就非常重要，这样，当离谱值大时梯度也要大，当离谱值小时梯度也要小，才能动态调整达到加速训练的效果。

但缺点就是原理比较简单，导致大量分类时出现准确率不足的问题，原因即是1.分类层线性变化矩阵参数也变大2.所学特征没有足够的区分性

##### 随机初始化

神经网络中节点权值W不能初始化为0，也不能单纯用随机数初始化

![屏幕截图 2020-12-28 153257](.\image\屏幕截图 2020-12-28 153257.png)

### 正则化 (regularization)

* 如果网络对数据过拟合，可以增加训练数据，或使用正则化方法

* 可直接作为损失函数使用

##### L2正则化

$$
J(w,b)=\frac{1}{m}\sum_{i=1}^{n}\sigma(\hat{y}^i,y^i)+\frac{\lambda}{2m}||w||^2_2\\||w||^2_2=\sum_i^{l-1}\sum_j^{l}w_{ij}^2=w^Tw
$$

L2正则化最主要的用处即是防止过拟合，由于二次函数的性质，ta具有非常良好的权重衰减功能，在使用时要进行初始化

![l2](.\image\l2.png)

##### droupout正则化(inverted dropout)

对一层神经元设置超参数keep-prob，随机失活神经元

![drop_out](.\image\drop_out.png)

drop-out在计算机视觉网络中应用非常广泛，作用有些类似于L2正则化

缺点：

##### L1正则化

$$
J(w,b)=\frac{1}{m}\sum_i^{l-1}\sum_j^{l}\sigma(\hat{y}^i,y^i)+\frac{\lambda}{m}||w||_1\\||w||_1=\sum_i^{l-1}\sum_j^{l}|w|
$$

因为一次函数特性，l1函数使模型明显稀疏。有一些防止过拟合作用

![l1](.\image\l1.png)

##### 其它正则化方法

early stopping方法

在网络训练过程中，网络的值逐渐变大

![early-stopping](.\image\early-stopping.png)

使用early-stopping方法比较方便，但是该方法在取到了合适的w的同时忽视了网络的训练成熟度，即损失函数还未达到最优就停止训练了

#### 梯度消失和梯度爆炸

当网络深度很高时，随着网络层数增加，权重W很可能以指数变大，或以指数减小，造成训练梯度过大或过小

参考解决方法，对每一层网络的权重随机初始化值进行处理

![梯度消失和梯度爆炸](.\image\梯度消失和梯度爆炸.png)

##### 梯度检验

手动实现的反向传播很有可能出现问题，通过单独计算损失函数J(x)在x上的双向误差积分，与反向传播的dx相减求欧式距离再除以梯度和积分的范数，由结果评估反向传播的效果。

![梯度检验](D:\Coder\Github\学习周记\image\梯度检验.png)

### 规范化(normalization)

##### 批规范化

对隐藏层输入进行均值值为0，方差为1的重新分布处理

**BN的优点：**

- ①极大提升了训练速度，收敛过程大大加快；
- ②增加了分类效果，一种解释是这是类似于Dropout的一种防止过拟合的正则化表达方式，所以不用Dropout或正则化也能达到相当的效果；
- ③简化了调参过程，对于初始化参数权重不太敏感，允许使用较大的学习率

对①，BN使输入回到梯度线性区域，梯度增大，虽然增大了梯度爆炸的风险，但加快了训练速度

对②，大概没法解释，反正很好就是了

对③，BN层使权重对输入结果的影响减弱，故使学习率调整更加简单

### 优化算法(optimizer alogrithms)

指数加权平均数

通过取不同权重的β，做一段数据的平均值近似。

![指数加权平均](D:\Coder\Github\学习周记\image\指数加权平均.png)

##### MomentumOptimizer

动量梯度下降可以减少梯度在纵轴上的大小，增大在横轴上的大小

![momentum](D:\Coder\Github\学习周记\image\momentum.png)

使用momentum算法使梯度下降的更快，可以轻易的离开local minimal

##### RMSprop

均方根梯度下降可以减少梯度在各个方向上的变化幅度

![RMSprop](D:\Coder\Github\学习周记\image\RMSprop.png)

使用RMSprop算法使梯度下降的更快，并允许使用更大的学习率

##### AdamOptimizer

Adaptive Moment estimation是Momentum算法和RMSprop算法的结合，具有两者的优点，具有良好的模型泛化能力

参数：α：自动衰减、β1：0.9、β2：0.999、ε：一般不用调

![Adam](D:\Coder\Github\学习周记\image\Adam.png)

### 激活函数

**RELU**

**Swish**

**Mish**

![mish](D:\Coder\Github\学习周记\image\mish.png)

相比于RELU，Mish的梯度更加平滑，更平滑的梯度使信息能够流入更深的网络，使用后准确率会提升，但是它比RELU更加笨重一点，会轻微增加运算量

### 数据收集与预处理



### 迁移学习与多任务(多GPU)学习

* 任务AB输入输出格式相同
* A的低级特征可以帮助任务B的进行(AB共享一些低级特征)
* 迁移学习与多任务学习所需的样本数量不一致

![迁移学习](D:\Coder\Github\学习周记\image\迁移学习.png)

在前几个卷积层中，训练得到的都是比较简单通用的特征

当训练集与预训练模型训练集性质差异不大时，只有全连接层与输出层的权重需要重新训练

当训练集与预训练模型训练集性质差异很大时，可以先冻结网络前几层的权重，对后部分进行重新训练

### 模型压缩与剪枝



#### 验证集与测试集

 测试集用于估计模型对样本的泛化误差，验证集用于“训练”模型的超参数。前者在用于模型时关注于accuracy准确度，后者关注于不同超参数下loss的变化，二者在数据上的区别就是验证集不需要测试集那么大的数据量

## 卷积神经网络(convolution neural network)

#### 卷积层(convolution layer)

卷积层越深，网络就能学到越复杂的特征，从边缘线到几何形状到动物的眼睛等等。

##### 卷积核、滤波器：通常为奇数

![1608452634683](.\image\1608452634683.png)

卷积核大小(size)表示感受域大小，一般为3、5、7

不同的滤波器内核尺寸会基于滤波器感受野大小进行提取不同粒度级别的特征信息，3x3会比5x5提取到更细粒度级别的特征信息

##### 鲁棒性(robust)

 算法对数据变化的容忍度 ，向算法内添加噪声（如对抗训练），可以测试算法的「鲁棒性」。 

##### pedding

通过计算步长带来的损失，用0填充外围像素

![屏幕截图 2020-12-25 140637](.\image\屏幕截图 2020-12-25 140637.png)

##### 卷积步长(stride)

 卷积核的步长度代表提取的精度 ，Size为3的卷积核，如果step为1，那么相邻步感受野之间就会有重复区域；如果step为2，那么相邻感受野不会重复，也不会有覆盖不到的地方；如果step为3，那么相邻步感受野之间会有一道大小为1的缝隙，从某种程度来说，这样就遗漏了原图的信息，直观上理解是不好的。

当卷积核step为1时，输出矩阵Size会和输入矩阵Size保持一致；而卷积核step为2时，由于跨步会导致输出矩阵Size降低为输入矩阵Size的一半。由此会产生一种类似“池化”的效果，利用这一特性可以代替池化层。 

 步长小，提取的特征会更全面，不会遗漏太多信息。但同时可能造成计算量增大，甚至过拟合等问题。步长大，计算量会下降，但很有可能错失一些有用的特征。 

##### 三维卷积(通道passage)

![屏幕截图 2020-12-26 155323](.\image\屏幕截图 2020-12-26 155323.png)

##### 单层卷积神经网络

![屏幕截图 2020-12-26 164638](.\image\屏幕截图 2020-12-26 164638.png)

#### 池化层(pooling layer)

图片的相邻像素具有相似的值，因此卷基层中很多信息是冗余的。通过池化来减少这个影响，用于缩减模型的大小，提高计算速度，提高所取特征的鲁棒性，同时减少过拟合，拥有invariance(不变性)，这种不变性包括translation(平移)，rotation(旋转)，scale(尺度)。 **[**池化层没有权重**]**

##### 最大池化(max pooling) 

能很好的保留图片纹理特征

##### 平均池化(average pooling)

 能很好的保留背景，但容易使得图片变模糊

##### 全连接层(full contact)

就是经典隐藏层，因为与输入特征全部相连，所以称为全连接层。

![屏幕截图 2020-12-26 185026](.\image\屏幕截图 2020-12-26 185026.png)

##### 经典网络

LeNet-5

![LeNet-5](D:\Coder\Github\学习周记\image\LeNet-5.png)

AlexNet

![Alexnet](D:\Coder\Github\学习周记\image\Alexnet.png)

VGG-16

![VGG-16](D:\Coder\Github\学习周记\image\VGG-16.png)

#### ResNet

当网络过于深时，训练集loss反而上升，这称为网络退化，当网络退化时，浅层网络能够达到比深层网络更好的训练效果，这时如果我们把低层的特征传到高层，那么效果应该至少不比浅层的网络效果差，或者说如果一个VGG-100网络在第98层使用的是和VGG-16第14层一模一样的特征，那么VGG-100的效果应该会和VGG-16的效果相同。所以，我们可以在VGG-100的98层和14层之间添加一条直接映射（Identity Mapping）来达到此效果。

从信息论的角度讲，由于DPI（数据处理不等式）的存在，在前向传输的过程中，随着层数的加深，Feature Map包含的图像信息会逐层减少，而ResNet的直接映射的加入，保证了 ![[公式]](https://www.zhihu.com/equation?tex=l%2B1) 层的网络一定比 ![[公式]](https://www.zhihu.com/equation?tex=l) 层包含更多的图像信息。

基于这种使用直接映射来连接网络不同层直接的思想，残差网络(Resnet)应运而生。

#### InceptionNet





### 目标检测网络(Object-Detection)

**批规范化(Batch Normalization)**

批规范化操作，不仅加快了模型收敛速度，而且更重要的是在一定程度缓解了深层网络的一个难题“梯度弥散”，从而使得训练深层网络模型更加容易和稳定。另外，批规范化操作不光适用于深层网络，对传统的较浅层网络而言，批规范化也能对网络泛化性能起到一定提升作用。

**MAP**

MAP中M表示取平均，AP即在目标检测中一个类在不同confidence下precision和recall所围成的面积

<img src="D:\Coder\Github\学习周记\image\map.png" alt="map" style="zoom:50%;" />

其中， Precision翻译成中文就是“**分类器认为是正类并且确实是正类的部分占所有分类器认为是正类的比例**”。Recall翻译成中文就是“**分类器认为是正类并且确实是正类的部分占所有确实是正类的比例**”。 

为什么用map？目标检测中置信度是核心概念，单个指标会使得对模型效果的评价片面化，二者进行结合才是评价的正确方法。

#### YoLo

论文思想

* 将图片分为SxS的网格，如果object的中心位于某个网格中，那么该网格就负责预测这个object
* 每个网格预测B个bounding box，每个box预测xywh和confidence值(IOU)
* 在每个卷积层后加上BN层，对于训练收敛有巨大的帮助，并具有一定防过拟合的正则化作用，使用BN后可以不用dropout
* 使用更高的分辨率输入图片
* 采用anchor，能够使网络更好的收敛，使recall上升，模型更好调
* 基于训练集box，采用k-means获取anchor
* 将低层特征图与高层特征图融合，使网络能更好的检测小目标
* 每迭代10个batch，网络就随机输入一个32倍数的图像大小
* 误差计算并没有被作者明确给出，可以自己修改

**交并比(IOU-intersection over union)**

两个bounding box的交集与并集比，为1则完全重合，当一个gird cell检测anchor与目标边界框IOU小于阈值时或经过NMS(Non-Maximum-Suppression)定为不能作为object的中心box，将该grid cell定为负样本，反之，定为正样本

**anchor boxes(锚框)**

anchor boxes概念的提出是为了解决同一个grid cell中有多个object的中心存在，预先定义一定数量一定形状的bounding box，当输入图像时，令object与anchor box中IOU最大的box作为该object的输出

**非极大值抑制(Non-Maximum-Suppression)**

非极大值抑制，在计算机视觉任务中得到了广泛的应用，例如边缘检测、人脸检测、目标检测（DPM，YOLO，SSD，Faster R-CNN）等。 在同一目标的位置上会产生大量的候选框， 需要利用非极大值抑制找到最佳的目标边界框，消除冗余的边界框 。

![nms](D:\Coder\Github\学习周记\image\nms.jpg)

**残差块(Res-Block)**

在yolov3网络中，卷积深度达到53，如此深的网络可能会造成高层特征提取不全的问题，因此引入残差块，将X层feature map进行卷积-BN-ReLu-卷积-BN-深度相加与X+L层feature map相加，其中可能会出现维度不同的问题，因此需要1x1卷积层进行升维或降维

<u>YOLOv4相对于之前三个版本有很大提升，使用了2020年以前在目标检测上有效的许多技术，综合性、技术性、研究性很强，故单独分析这些技术作为学习</u>

**cutmix**

**mosaic**

**Stylized-ImageNet**

**label smooth**

**DropBlock**

对每一个feature map都进行随机区域的删除，这种方式其实是借鉴**2017年的cutout数据增强**的方式，cutout是将输入图像的部分区域清零，而Dropblock则是将Cutout应用到每一个特征图。而且并不是用固定的归零比率，而是在训练时以一个小的比率开始，随着训练过程**线性的增加这个比率**。

**CIOU_loss**

在了解CIOU_loss之前，先了解其原型，即DIOU_loss、GIOU_loss、IOU_loss

IOU_Loss：主要考虑检测框和目标框重叠面积。

GIOU_Loss：在IOU的基础上，解决边界框不重合时的问题。

DIOU_Loss：在IOU和GIOU的基础上，考虑边界框中心点距离的信息。

CIOU_Loss：在DIOU的基础上，考虑边界框宽高比的尺度信息。

Yolov4中采用了CIOU_Loss的回归方式，使得预测框回归的**速度和精度**更高一些。

**网络结构**

YOLOv1

YOLOv2

YOLOv3

**YOLOv4**

![yolov4](D:\Coder\Github\学习周记\image\yolov4.png)

主要的网络结构图里都有简图，prediction和YOLOv3类似的head，只是分支来源不同，这里说一下PANet，即图中应用了上采样的部分，这里同样应用了特征融合技术

在目标检测任务中，融合不同尺度的特征是提升性能的一个重要手段，低层特征分辨率更高，包含更多**位置、细节信息**，但是由于经过的卷积更少，其**语义性更低，噪声更多**。高层特征**具有更强的语义信息，但是分辨率很低，对细节的感知能力较差**。 

### 为什么卷积网络如此有用？

* 卷积神经网络一开始被设计出来即是为了解决传统神经网络处理图像的偏移和形变所造成的巨大误差。
* 卷积层用于特征提取，池化层用于特征组合
* 特征映射是分类的关键

## 循环神经网络(Recurrent Neural Network)

 **序列模型**

对带有时间或空间的关系的一系列数据处理的模型称为序列模型，而RNN擅长处理此类数据，即RNN能处理前一个输入与后一个输入有关系的序列数据，如：

语音转文字，生成音乐，评价分析，DNA序列分析，机器翻译，视频分析，判断句中成分

##### 多对多单向循环神经网络

![单向循环](D:\Coder\Github\学习周记\image\单向循环.png)

图中还有W_aa, W_ax, W_ya这些权重未画出

**前向传播**
$$
a^{<t>}=g_1(W_{aa}*a^{<t-1>}+W_{ax}*x^{<t>}+b_a)\\y^{<t>}=g_2(W_{ya}*a^{<t>}+b_y)
$$
一般的g1为 tanh/relu g2为softmax

**反向传播**

对于每一个y<t>, 都有一个对应类别的概率，我们假设为是否为名字，方便做二值化。此时y非1即0，于是定义交叉熵损失函数为损失函数。通过上式，我们可以很方便的求出反向梯度

##### 多对一、一对多、多对少和少对多单向循环神经网络

大体与上相同，但在x与y的计算上有些许不同

![多种循环神经网络](D:\Coder\Github\学习周记\image\多种循环神经网络.png)

##### 双向循环神经网络

![双向循环](D:\Coder\Github\学习周记\image\双向循环.png)

有时在模型的建立中，结果的预测需要序列前后的信息，此时需要构筑双向循环神经网络

##### 深层循环神经网络



**梯度消失**

假如给定一个时间为t的RNN单元，如果要训练它的话，那么就是让损失函数对W1、W2、W3和b1、b2、b3求偏导，在假设简单的情况，偏导的情况可能如下

![偏导](D:\Coder\Github\学习周记\image\偏导.png)

​																								（这里加号应该是等号）

可知，深度对W1没什么影响，但是会使W2和W3出现梯度消失和梯度爆炸的问题，其中梯度爆炸表现在结果上十分明显，并且可以用设定阈值来解决，但梯度消失更加难以发觉

当出现梯度消失时，浅层单元与深层单元的连接几乎终止，后端单元无法得到训练，表现为预测结果混乱等等

##### LSTM(long short term memory neural network)

![LSTM](D:\Coder\Github\学习周记\image\LSTM.png)

为解决RNN“长期记忆力不足”的情况，LSTM于1997年被提出，简单来说，相比普通的RNN，LSTM能够在更长的序列中有更好的表现。 

如图，看上去LSTM比普通RNN复杂了很多，不要忘记，LSTM于RNN来说，要解决核心问题即是深度带来的梯度消失问题。于是引入ct和忘记门、记忆门以及输出门机制。

各种公式为
$$
\begin{array}{l}
\sigma_{1,2,3}=sigmoid(W^{1,2,3}_{x}*x^t+W^{1,2,3}_{h}*h_{t-1})\\
z=\tanh(W_x*x_t+W_h*h_{t-1})\\
c^{t}=\sigma_{1}^{f} \odot c^{t-1}+\sigma_{2}^{i} \odot z \\
h^{t}=\sigma_{3}^{o} \odot \tanh \left(c^{t}\right) \\
y^{t}=g\left(W^{\prime} h^{t}\right)
\end{array}
$$
根据图和这些公式，我们可以对LSTM中各单元作用进行分析：

- **输出门** ![[公式]](https://www.zhihu.com/equation?tex=o_%7Bt-1%7D)：输出门的目的是从细胞状态 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_%7Bt-1%7D) 产生隐层单元 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+h_%7Bt-1%7D) 。并不是 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_%7Bt-1%7D) 中的全部信息都和隐层单元 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+h_%7Bt-1%7D) 有关， ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_%7Bt-1%7D) 可能包含了很多对 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+h_%7Bt-1%7D) 无用的信息。因此， ![[公式]](https://www.zhihu.com/equation?tex=o_t) 的作用就是判断 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_%7Bt-1%7D) 中哪些部分是对 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+h_%7Bt-1%7D) 有用的，哪些部分是无用的。
- **输入门** ![[公式]](https://www.zhihu.com/equation?tex=i_t)。![[公式]](https://www.zhihu.com/equation?tex=i_t) 控制当前词 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+x_t) 的信息融入细胞状态 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_t) 。在理解一句话时，当前词 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+x_t) 可能对整句话的意思很重要，也可能并不重要。输入门的目的就是判断当前词 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+x_t) 对全局的重要性。当 ![[公式]](https://www.zhihu.com/equation?tex=i_t) 开关打开的时候，网络将不考虑当前输入 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+x_t) 。
- **遗忘门** ![[公式]](https://www.zhihu.com/equation?tex=f_t): ![[公式]](https://www.zhihu.com/equation?tex=f_t) 控制上一时刻细胞状态 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_%7Bt-1%7D) 的信息融入细胞状态 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_t) 。在理解一句话时，当前词 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+x_t) 可能继续延续上文的意思继续描述，也可能从当前词 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+x_t) 开始描述新的内容，与上文无关。和输入门 ![[公式]](https://www.zhihu.com/equation?tex=i_t) 相反， ![[公式]](https://www.zhihu.com/equation?tex=f_t) 不对当前词 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+x_t) 的重要性作判断，而判断的是上一时刻的细胞状态 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_%7Bt-1%7D) 对计算当前细胞状态 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_t) 的重要性。当 ![[公式]](https://www.zhihu.com/equation?tex=f_t) 开关打开的时候，网络将不考虑上一时刻的细胞状态 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_%7Bt-1%7D) 。
- **细胞状态** ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_t) ： ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_t) 综合了当前词 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+x_t) 和前一时刻细胞状态 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_%7Bt-1%7D) 的信息。这和ResNet中的残差逼近思想十分相似，通过从 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_%7Bt-1%7D) 到 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_t) 的“短路连接”，梯度得已有效地反向传播。当 ![[公式]](https://www.zhihu.com/equation?tex=f_t) 处于闭合状态时， ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_t) 的梯度可以直接沿着最下面这条短路线传递到 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_%7Bt-1%7D) ，不受参数 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+W_%7Bxh%7D) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+W_%7Bhh%7D) 的影响，这是LSTM能有效地缓解梯度消失现象的关键所在。

##### GRU单元(Gate Recurrent Unit)

GRU和LSTM在很多情况下实际表现上相差无几，但是更易于计算，于是相比之下更容易进行训练，能够很大程度上提高训练效率，因此很多时候会更倾向于使用GRU。

<img src="D:\Coder\Github\学习周记\image\GRU.png" alt="GRU" style="zoom:60%;" />

如图，各种公式为
$$
\begin{array}{1}
z,r=sigmoid(W^{z,r}_x*x^t+W^{z,r}_h*h^{t-1})\\
h^{t-1'}=h^{t-1}\odot{r}\\
h'=sigmoid(W^h_x*x^t+W^h_h*h^{t-1'})\\
h^t=(1-z)\odot{h^{t-1}}+z\odot{h'}\\
\end{array}
$$
可知，GRU只有两个门，ht的更新公式同时起到了记忆和遗忘的功能，那么这里的**h'**实际上可以看成对应于LSTM中的hidden state；上一个节点传下来的 ![[公式]](https://www.zhihu.com/equation?tex=h%5E%7Bt-1%7D) 则对应于LSTM中的cell state。1-z对应的则是LSTM中的 ![[公式]](https://www.zhihu.com/equation?tex=z%5Ef) forget gate，那么 z我们似乎就可以看成是选择门 ![[公式]](https://www.zhihu.com/equation?tex=z%5Ei) 了。

##### 词嵌入(word embaddings) 

**词嵌入是什么**

**词嵌入怎么算**

**词嵌入的应用**

**Word2vec**

##### 负采样(nagtive sampling)



## TensorFlow-v2.1.0

### tensorflow底层概念

##### 图(graph)

 一个`Graph`包含一系列`tf.Operation`对象和`tf.Tensor`对象；`tf.Operation`对象代表计算单元，`tf.Tensor`对象代表operation之间流动的数据单元). 

用 tf.Graph.as_default 创建图。

##### 会话(session)

 `Session`对象封装了执行`Operation`对象和计算`Tensor`对象的环境。  调用`tf.Session.close`方法 ，释放会话占用的资源。

 如果在构造会话时没有指定`graph`参数，则将在会话中启动默认图。但是每个图都可以在多个会话中使用。 

##### 变量(varible)

 [变量简介  | TensorFlow Core](https://www.tensorflow.org/guide/variable) 

 变量通过 `tf.Variable`类进行创建和跟踪。`tf.Variable`表示张量，对它执行运算可以改变其值。利用特定运算可以读取和修改此张量的值。大部分张量运算在变量上也可以按预期运行，不过变量无法重构形状。 

##### 张量(tensor)

 [张量简介  | TensorFlow Core](https://www.tensorflow.org/guide/tensor) 

 张量是具有统一类型（称为 `dtype`）的多维数组，与 `np.arrays` 有一定的相似性，就像 Python 数值和字符串一样，所有张量都是不可变的：永远无法更新张量的内容，只能创建新的张量。 

张量有形状。下面是几个相关术语：

- **形状**：张量的每个维度的长度（元素数量）。
- **秩**：张量的维度数量。标量的秩为 0，向量的秩为 1，矩阵的秩为 2。
- **轴**或**维度**：张量的一个特殊维度。
- **大小**：张量的总项数，即乘积形状向量

### Keras

one-hot处理

```python
iris_target = np.float32(tf.keras.utils.to_categorical(iris_target, num_classes=3))
```

输入层input

```python
input_xs  = tf.keras.Input(shape=(4), name='input_xs')
```

中间层Dens

```python
x = tf.keras.layers.Dense(32, activation='relu')(inputs)
```

模型保存与调用

```python
model.save()
new_model = tf.keras.models.load_model('./saver/the_save_model.h5')
model = tf.keras.Model(inputs=inputs, outputs=predictions)
```

compile函数Tensorflow2.0专用于配置训练模型的编译函数

fit函数用于对输入数据进行修改(泛化)

 [The Sequential model  | TensorFlow Core](https://www.tensorflow.org/guide/keras/sequential_model) 

[Module: tf.keras  | TensorFlow Core v2.4.0](https://www.tensorflow.org/api_docs/python/tf/keras) 

### Dataset



## Pytorch-v1.7

# ?

为什么顺序编程那么抽象？怎样的编程算不抽象？为什么有时要对数据进行重新计算？什么是Keras函数式API？神经网络中的批次具体是什么？点在特征空间上分不开到底是因为什么？怎样对已保存的模型进行更改？权重随网络深度增加会怎样变化？为什么？
