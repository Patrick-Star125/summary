## 基础概念

##### **logistic**函数

$$
\hat{y}=\sigma(w^Tx+b) \ where\ \sigma(z)=\frac{1}{1+e^{-z}}
$$

$$
\delta(\hat{y},y)=-(y*log\hat{y}+(1-y)log(1-\hat{y}))
$$

$$
J(w,b)=\frac{1}{m}\sum^m_{i=1}\delta(\hat{y},y)=\frac{1}{m}\sum^m_{i=1}(-(y*log\hat{y}+(1-y)log(1-\hat{y})))
$$

$$
w:=w-\alpha\frac{dJ(w)}{dw}
$$

$$
b:=b-\alpha\frac{dJ(w,b)}{db}
$$

##### softmax函数

softmax函数使变量按照大小分配概率，解决a>b的绝对概率问题。
$$
softmax(likelihood):s_i=\frac{e^{v_j}}{\sum_{0}^{j}e^{vj}}
$$

##### TPU

张量处理单元（TPU）是一种定制化的 ASIC 芯片，它由谷歌从头设计，并专门用于机器学习工作负载 ， 它专注于神经网络推断。这使得量化选择、CISC指令集、矩阵处理器和最小设计都成为可能。 

## 全卷积网络(full convolution neural network)

#### 卷积层(convolution layer)

##### 卷积核、滤波器：通常为奇数

![1608452634683](D:\Coder\Github\学习周记\image\1608452634683.png)

卷积核大小(size)表示感受域大小，一般为3、5、7

##### 鲁棒性(robust)

 算法对数据变化的容忍度 ，向算法内添加噪声（如对抗训练），以便测试算法的「鲁棒性」。 

##### pedding

![屏幕截图 2020-12-25 140637](D:\Coder\Github\学习周记\image\屏幕截图 2020-12-25 140637.jpg)

##### 卷积步长(stride)

 卷积核的步长度代表提取的精度 ，Size为3的卷积核，如果step为1，那么相邻步感受野之间就会有重复区域；如果step为2，那么相邻感受野不会重复，也不会有覆盖不到的地方；如果step为3，那么相邻步感受野之间会有一道大小为1的缝隙，从某种程度来说，这样就遗漏了原图的信息，直观上理解是不好的。

当卷积核step为1时，输出矩阵Size会和输入矩阵Size保持一致；而卷积核step为2时，由于跨步会导致输出矩阵Size降低为输入矩阵Size的一半。由此会产生一种类似“池化”的效果，利用这一特性可以代替池化层。 

 步长小，提取的特征会更全面，不会遗漏太多信息。但同时可能造成计算量增大，甚至过拟合等问题。步长大，计算量会下降，但很有可能错失一些有用的特征。 

##### 三维卷积

![屏幕截图 2020-12-26 155323](D:\Coder\Github\学习周记\image\屏幕截图 2020-12-26 155323.jpg)

##### 单层卷积神经网络

 ![屏幕截图 2020-12-26 164638](D:\Coder\Github\学习周记\image\屏幕截图 2020-12-26 164638.jpg)

#### 池化层(pooling layer)

用于缩减模型的大小，提高计算速度，提高所取特征的鲁棒性，同时减少过拟合，拥有invariance(不变性)，这种不变性包括translation(平移)，rotation(旋转)，scale(尺度)。 **[**池化层没有参数**]**

##### 最大池化(max pooling) 

能很好的保留图片纹理特征

##### 平均池化(average pooling)

 能很好的保留背景，但容易使得图片变模糊 

##### 全连接层(full contact)

就是经典隐藏层，因为与输入特征全部相连，所以称为全连接层。

![屏幕截图 2020-12-26 185026](D:\Coder\Github\学习周记\image\屏幕截图 2020-12-26 185026.jpg)

#### 为什么卷积网络如此有用？



## TensorFlow-v2.1.0

##### 图(graph)



##### 会话(session)



##### 张量(tensor)

 张量是具有统一类型（称为 `dtype`）的多维数组，与 `np.arrays` 有一定的相似性，就像 Python 数值和字符串一样，所有张量都是不可变的：永远无法更新张量的内容，只能创建新的张量。 

张量有形状。下面是几个相关术语：

- **形状**：张量的每个维度的长度（元素数量）。
- **秩**：张量的维度数量。标量的秩为 0，向量的秩为 1，矩阵的秩为 2。
- **轴**或**维度**：张量的一个特殊维度。
- **大小**：张量的总项数，即乘积形状向量

##### AdamOptimizer

## Pytorch-v1.7