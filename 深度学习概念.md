## 基础概念

### 通用函数

**卷积**

理解是如果一个系统输入是不稳定的，输出是不稳定的，那么我们就可以用卷积来求系统的存量，在图像中，卷积操作就是周围像素点对中间像素点的影响，在特征中，卷积操作就是提取适合的特征产生特征量来给神经网络来处理。



卷积公式
$$
\int_{-\infin}^{\infin}f(\alpha)g(x-\alpha)d\alpha
$$
相加积分公式
$$
r(x)=\int_{0}^{t}e(x)d(t-x)dx
$$


##### **logistic**函数

$$
\hat{y}=\sigma(w^Tx+b) \ where\ \sigma(z)=\frac{1}{1+e^{-z}}
$$

$$
\delta(\hat{y},y)=-(y*log\hat{y}+(1-y)log(1-\hat{y}))
$$

$$
J(w,b)=\frac{1}{m}\sum^m_{i=1}\delta(\hat{y},y)=\frac{1}{m}\sum^m_{i=1}(-(y*log\hat{y}+(1-y)log(1-\hat{y})))
$$

$$
w:=w-\alpha\frac{dJ(w)}{dw}
$$

$$
b:=b-\alpha\frac{dJ(w,b)}{db}
$$

##### softmax函数

softmax函数使变量按照大小分配概率，解决a>b的绝对概率问题。
$$
softmax(likelihood):s_i=\frac{e^{v_j}}{\sum_{0}^{j}e^{vj}}
$$

<img src=".\image\softmax_example.png" alt="softmax_example" style="zoom: 67%;" />

##### ReLU函数

Sigmoid函数或者Tanh函数只有其输入指定范围的值才能获得良好的线性梯度，一旦输入变大，梯度就会降为0，故在神经网络的反向传播中多使用ReLU函数

在卷积神经网络中，ReLU函数为网络增加了非线性，为正输入提供非饱和梯度

##### **Mish**

<img src=".\image\mish.png" alt="mish" style="zoom: 67%;" />

相比于RELU，Mish的梯度更加平滑，更平滑的梯度使信息能够流入更深的网络，使用后准确率会提升，但是它比RELU更加笨重一点，会轻微增加运算量

##### 交叉熵损失函数(cross entropy error function)

$$
L=-[ylog\hat{p}+(1-y)log(1-\hat{p})]二分类\\
L=\frac{1}{N} \sum_{i} L_{i}=\frac{1}{N} \sum_{i}-\sum_{c=1}^{M} y_{i c} \log \left(p_{i c}\right)多分类\\P(y|x)=\hat{p}^y*(1-\hat{p}^{1-y})\ (0<\hat{p}<1)
$$

y为label的值，p为样本为真的概率(softmax)，M为类别的数量

在用梯度下降法做参数更新的时候，模型学习的速度取决于两个值：一、**学习率**；二、**偏导值**。所以使用逻辑函数得到概率，并结合交叉熵当损失函数时，在模型效果差的时候学习速度比较快，在模型效果好的时候学习速度变慢。

在分类问题中，需要定义一个classification error来判定网络错误的离谱程度，而由于错误的离谱方式不同，选用何种error function来体现量化离谱程度就非常重要，这样，当离谱值大时梯度也要大，当离谱值小时梯度也要小，才能动态调整达到加速训练的效果。

但缺点就是原理比较简单，导致大量分类时出现准确率不足的问题，原因即是1.分类层线性变化矩阵参数也变大2.所学特征没有足够的区分性

### 权重(Weight)

##### 初始化

[权重/参数初始化 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/72374385)

神经网络中节点权值W不能初始化为0，否则没法训练，也不能单纯用随机数初始化

<img src=".\image\屏幕截图 2020-12-28 153257.png" alt="屏幕截图 2020-12-28 153257" style="zoom:50%;" />

因为假如随机分布选择不当，就会导致网络优化陷入困境

假如把权重初始化为较小的值，如均值为0，方差为0.01的高斯分布

![梯度消失](.\image\梯度消失.jpg)

则在不过10层的网络就梯度消失的只剩0了

但是如果把**权重初始成一个比较大的值**，如均值为0，方差为1的高斯分布

![梯度爆炸](.\image\梯度爆炸.jpg)

则在不过10层的网络梯度爆炸的只剩1和-1

**Xavier初始化：**

Xavier初始化可以帮助减少梯度消失的问题，它的**基本思想**是保持输入和输出的方差一致（服从相同的分布），这样就避免了所有输出值都趋向于0，但是Xavier初始化只适用于tanh激活函数。[具体运算方法在链接中]

**MSRA：**

MSRA的思想是在ReLU网络中，假定每一层有一半的神经元被激活，另一半为0（x负半轴中是不激活的），所以要保持variance不变，只需要在Xavier的基础上再除以2，效果是比Xavier initialization好很多。现在神经网络中，隐藏层常使用ReLU，权重初始化常用He initialization这种方法。

**BN**：

BN虽然牛逼，但是在合适的初始化可以胜任任务时还是尽量减少运算量吧

当然，为了压根儿不去想权重初始化的问题，我们可以用**Pre-train模型**，这又牵涉到一个非常复杂的方面了，一个便于理解的例子是先使用greedy layerwise auto-encoder做unsupervised pre-training，然后再做fine-tuning。

pre-training阶段：将神经网络中的每一层取出，构造一个auto-encoder做训练，使得输入层和输出层保持一致。在这一过程中，参数得以更新，形成初始值。

fine-tuning阶段：将pre-train过的每一层放回神经网络，利用pre-train阶段得到的参数初始值和训练数据对模型进行整体调整。在这一过程中，参数进一步被更新，形成最终模型。

随着数据量的增加以及activation function 的发展，pre-training的概念已经渐渐发生变化。目前，从零开始训练神经网络时我们也很少采用auto-encoder进行pre-training，而是直奔主题做模型训练。**如果不想从零开始训练神经网络时，我们往往选择一个已经在任务A上训练好的模型（称为pre-trained model），将其放在任务B上做模型调整（称为fine-tuning）**。

#### 正则化 (regularization)

* 如果网络对数据过拟合，可以增加训练数据，或使用正则化方法

* 可直接作为损失函数使用

##### L2正则化

$$
J(w,b)=\frac{1}{m}\sum_{i=1}^{n}\sigma(\hat{y}^i,y^i)+\frac{\lambda}{2m}||w||^2_2\\||w||^2_2=\sum_i^{l-1}\sum_j^{l}w_{ij}^2=w^Tw
$$

L2正则化最主要的用处即是防止过拟合，由于二次函数的性质，ta具有非常良好的权重衰减功能，在使用时要进行初始化

<img src=".\image\l2.png" alt="l2" style="zoom:67%;" />

##### droupout正则化(inverted dropout)

对一层神经元设置超参数keep-prob，随机失活神经元

<img src=".\image\drop_out.png" alt="drop_out" style="zoom: 50%;" />

drop-out在计算机视觉网络中应用非常广泛，作用有些类似于L2正则化

缺点：可能让本来拟合的模型又拟合了、在任务比较小的时候尽量不要用dropout

##### L1正则化

$$
J(w,b)=\frac{1}{m}\sum_i^{l-1}\sum_j^{l}\sigma(\hat{y}^i,y^i)+\frac{\lambda}{m}||w||_1\\||w||_1=\sum_i^{l-1}\sum_j^{l}|w|
$$

因为一次函数特性，l1函数使模型明显稀疏。有一些防止过拟合作用

![l1](.\image\l1.png)

##### 其它正则化方法

early stopping方法

在网络训练过程中，网络的值逐渐变大

<img src=".\image\early-stopping.png" alt="early-stopping" style="zoom:50%;" />

使用early-stopping方法比较方便，但是该方法在取到了合适的w的同时忽视了网络的训练成熟度，即损失函数还未达到最优就停止训练了

#### 梯度消失和梯度爆炸

当网络深度很高时，随着网络层数增加，权重W很可能以指数变大，或以指数减小，造成训练梯度过大或过小

参考解决方法，对每一层网络的权重随机初始化值进行处理

<img src=".\image\梯度消失和梯度爆炸.png" alt="梯度消失和梯度爆炸" style="zoom:50%;" />

#### 梯度检验

手动实现的反向传播很有可能出现问题，通过单独计算损失函数J(x)在x上的双向误差积分，与反向传播的dx相减求欧式距离再除以梯度和积分的范数，由结果评估反向传播的效果。

<img src=".\image\梯度检验.png" alt="梯度检验" style="zoom:50%;" />

#### 规范化(normalization)

##### 批规范化

对隐藏层输入进行均值值为0，方差为1的重新分布处理

**BN的优点：**

- ①极大提升了训练速度，收敛过程大大加快；
- ②增加了分类效果，一种解释是这是类似于Dropout的一种防止过拟合的正则化表达方式，所以不用Dropout或正则化也能达到相当的效果；
- ③简化了调参过程，对于初始化参数权重不太敏感，允许使用较大的学习率

对①，BN使输入回到梯度线性区域，梯度增大，虽然增大了梯度爆炸的风险，但加快了训练速度

对②，大概没法解释，反正很好就是了

对③，BN层使权重对输入结果的影响减弱，故使学习率调整更加简单

### 优化算法(optimizer alogrithms)

指数加权平均数

通过取不同权重的β，做一段数据的平均值近似。

<img src=".\image\指数加权平均.png" alt="指数加权平均" style="zoom:50%;" />

##### MomentumOptimizer

动量梯度下降可以减少梯度在纵轴上的大小，增大在横轴上的大小

<img src=".\image\momentum.png" alt="momentum" style="zoom:50%;" />

使用momentum算法使梯度下降的更快，可以轻易的离开local minimal

##### RMSprop

均方根梯度下降可以减少梯度在各个方向上的变化幅度

<img src=".\image\RMSprop.png" alt="RMSprop" style="zoom:50%;" />

使用RMSprop算法使梯度下降的更快，并允许使用更大的学习率

##### AdamOptimizer

Adaptive Moment estimation是Momentum算法和RMSprop算法的结合，具有两者的优点，具有良好的模型泛化能力

参数：α：自动衰减、β1：0.9、β2：0.999、ε：一般不用调

<img src=".\image\Adam.png" alt="Adam" style="zoom:50%;" />

### 模型学习方式

#### 迁移学习

* 任务AB输入输出格式相同
* A的低级特征可以帮助任务B的进行(AB共享一些低级特征)
* 迁移学习与多任务学习所需的样本数量不一致

<img src=".\image\迁移学习.png" alt="迁移学习" style="zoom: 67%;" />

在前几个卷积层中，训练得到的都是比较简单通用的特征

当训练集与预训练模型训练集性质差异不大时，只有全连接层与输出层的权重需要重新训练

当训练集与预训练模型训练集性质差异很大时，可以先冻结网络前几层的权重，对后部分进行重新训练

### 模型加速(modeling)

##### 模型压缩与剪枝

##### 模型量化

##### 使用推理软件

##### 模型优化

<img src=".\image\模型推理侧优化.png" alt="模型推理侧优化" style="zoom:50%;" />

### 数据集(Dataset)

**验证集与测试集**

 测试集用于估计模型对样本的泛化误差，验证集用于“训练”模型的超参数。前者在用于模型时关注于accuracy准确度，后者关注于不同超参数下loss的变化，二者在数据上的区别就是验证集不需要测试集那么大的数据量

#### 验证指标

#### 验证方法

**少样本学习(Few-shot leaning)**

[Understanding few-shot learning in machine learning | by Dr. Michael J. Garbade | Quick Code | Medium](https://medium.com/quick-code/understanding-few-shot-learning-in-machine-learning-bede251a0f67)

在很多金融场景，大批量的数据难以进行标注，此时能够使用的有效样本数量有限，**少样本学习**能够在稀疏的特征空间中找到最优。

在文章中提到

1. 少量未标注的样本就可能使模型性能下降很多
2. 大量标注数据集可能花费很多

## 卷积神经网络(convolution neural network)

#### 卷积层(convolution layer)

卷积层越深，网络就能学到越复杂的特征，从边缘线到几何形状到动物的眼睛等等。

##### 卷积核、滤波器：通常为奇数

<img src=".\image\1608452634683.png" alt="1608452634683" style="zoom:50%;" />

卷积核大小(size)表示感受域大小，一般为3、5、7

不同的滤波器内核尺寸会基于滤波器感受野大小进行提取不同粒度级别的特征信息，3x3会比5x5提取到更细粒度级别的特征信息

##### 鲁棒性(robust)

 算法对数据变化的容忍度 ，向算法内添加噪声（如对抗训练），可以测试算法的「鲁棒性」。 

##### pedding

通过计算步长带来的损失，用0填充外围像素

<img src=".\image\屏幕截图 2020-12-25 140637.png" alt="屏幕截图 2020-12-25 140637" style="zoom:50%;" />

##### 卷积步长(stride)

 卷积核的步长度代表提取的精度 ，Size为3的卷积核，如果step为1，那么相邻步感受野之间就会有重复区域；如果step为2，那么相邻感受野不会重复，也不会有覆盖不到的地方；如果step为3，那么相邻步感受野之间会有一道大小为1的缝隙，从某种程度来说，这样就遗漏了原图的信息，直观上理解是不好的。

当卷积核step为1时，输出矩阵Size会和输入矩阵Size保持一致；而卷积核step为2时，由于跨步会导致输出矩阵Size降低为输入矩阵Size的一半。由此会产生一种类似“池化”的效果，利用这一特性可以代替池化层。 

 步长小，提取的特征会更全面，不会遗漏太多信息。但同时可能造成计算量增大，甚至过拟合等问题。步长大，计算量会下降，但很有可能错失一些有用的特征。 

##### 三维卷积(通道passage)

<img src=".\image\屏幕截图 2020-12-26 155323.png" alt="屏幕截图 2020-12-26 155323" style="zoom:50%;" />

##### 单层卷积神经网络

<img src=".\image\屏幕截图 2020-12-26 164638.png" alt="屏幕截图 2020-12-26 164638" style="zoom:50%;" />

#### 池化层(pooling layer)

图片的相邻像素具有相似的值，因此卷基层中很多信息是冗余的。通过池化来减少这个影响，用于缩减模型的大小，提高计算速度，提高所取特征的鲁棒性，同时减少过拟合，拥有invariance(不变性)，这种不变性包括translation(平移)，rotation(旋转)，scale(尺度)。 **[**池化层没有权重**]**

##### 最大池化(max pooling) 

能很好的保留图片纹理特征

##### 平均池化(average pooling)

 能很好的保留背景，但容易使得图片变模糊

##### 全连接层(full contact)

就是经典隐藏层，因为与输入特征全部相连，所以称为全连接层。

<img src=".\image\屏幕截图 2020-12-26 185026.png" alt="屏幕截图 2020-12-26 185026" style="zoom:50%;" />

##### 经典网络

LeNet-5

<img src=".\image\LeNet-5.png" alt="LeNet-5" style="zoom:50%;" />

AlexNet

<img src=".\image\Alexnet.png" alt="Alexnet" style="zoom:50%;" />

VGG-16

<img src=".\image\VGG-16.png" alt="VGG-16" style="zoom:50%;" />

#### ResNet

当网络过于深时，训练集loss反而上升，这称为网络退化，当网络退化时，浅层网络能够达到比深层网络更好的训练效果，这时如果我们把低层的特征传到高层，那么效果应该至少不比浅层的网络效果差，或者说如果一个VGG-100网络在第98层使用的是和VGG-16第14层一模一样的特征，那么VGG-100的效果应该会和VGG-16的效果相同。所以，我们可以在VGG-100的98层和14层之间添加一条直接映射（Identity Mapping）来达到此效果。

从信息论的角度讲，由于DPI（数据处理不等式）的存在，在前向传输的过程中，随着层数的加深，Feature Map包含的图像信息会逐层减少，而ResNet的直接映射的加入，保证了 ![[公式]](https://www.zhihu.com/equation?tex=l%2B1) 层的网络一定比 ![[公式]](https://www.zhihu.com/equation?tex=l) 层包含更多的图像信息。

基于这种使用直接映射来连接网络不同层直接的思想，残差网络(Resnet)应运而生。

#### InceptionNet

#### EfficientNet

### 为什么卷积网络如此有用？

* 卷积神经网络一开始被设计出来即是为了解决传统神经网络处理图像的偏移和形变所造成的巨大误差。
* 卷积层用于特征提取，池化层用于特征组合
* 特征映射是分类的关键
* 单纯从学习参数的角度

## 循环神经网络(Recurrent Neural Network)

 **序列模型**

对带有时间或空间的关系的一系列数据处理的模型称为序列模型，而RNN擅长处理此类数据，即RNN能处理前一个输入与后一个输入有关系的序列数据，如：

语音转文字，生成音乐，评价分析，DNA序列分析，机器翻译，视频分析，判断句中成分

##### 多对多单向循环神经网络

![单向循环](.\image\单向循环.png)

图中还有W_aa, W_ax, W_ya这些权重未画出

**前向传播**
$$
a^{<t>}=g_1(W_{aa}*a^{<t-1>}+W_{ax}*x^{<t>}+b_a)\\y^{<t>}=g_2(W_{ya}*a^{<t>}+b_y)
$$
一般的g1为 tanh/relu g2为softmax

**反向传播**

对于每一个y<t>, 都有一个对应类别的概率，我们假设为是否为名字，方便做二值化。此时y非1即0，于是定义交叉熵损失函数为损失函数。通过上式，我们可以很方便的求出反向梯度

##### 多对一、一对多、多对少和少对多单向循环神经网络

大体与上相同，但在x与y的计算上有些许不同

<img src=".\image\多种循环神经网络.png" alt="多种循环神经网络" style="zoom: 33%;" />

##### 双向循环神经网络

<img src=".\image\双向循环.png" alt="双向循环" style="zoom: 33%;" />

有时在模型的建立中，结果的预测需要序列前后的信息，此时需要构筑双向循环神经网络

##### 深层循环神经网络



**梯度消失**

假如给定一个时间为t的RNN单元，如果要训练它的话，那么就是让损失函数对W1、W2、W3和b1、b2、b3求偏导，在假设简单的情况，偏导的情况可能如下

<img src=".\image\偏导.png" alt="偏导" style="zoom: 50%;" />

​																								（这里加号应该是等号）

可知，深度对W1没什么影响，但是会使W2和W3出现梯度消失和梯度爆炸的问题，其中梯度爆炸表现在结果上十分明显，并且可以用设定阈值来解决，但梯度消失更加难以发觉

当出现梯度消失时，浅层单元与深层单元的连接几乎终止，后端单元无法得到训练，表现为预测结果混乱等等

##### LSTM(long short term memory)

<img src=".\image\LSTM.png" alt="LSTM" style="zoom:50%;" />

为解决RNN“长期记忆力不足”的情况，LSTM于1997年被提出，简单来说，相比普通的RNN，LSTM能够在更长的序列中有更好的表现。 

如图，看上去LSTM比普通RNN复杂了很多，不要忘记，LSTM于RNN来说，要解决核心问题即是深度带来的梯度消失问题。于是引入ct和忘记门、记忆门以及输出门机制。

各种公式为
$$
\begin{array}{l}
\sigma_{1,2,3}=sigmoid(W^{1,2,3}_{x}*x^t+W^{1,2,3}_{h}*h_{t-1})\\
z=\tanh(W_x*x_t+W_h*h_{t-1})\\
c^{t}=\sigma_{1}^{f} \odot c^{t-1}+\sigma_{2}^{i} \odot z \\
h^{t}=\sigma_{3}^{o} \odot \tanh \left(c^{t}\right) \\
y^{t}=g\left(W^{\prime} h^{t}\right)
\end{array}
$$
根据图和这些公式，我们可以对LSTM中各单元作用进行分析：

- **输出门** ![[公式]](https://www.zhihu.com/equation?tex=o_%7Bt-1%7D)：输出门的目的是从细胞状态 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_%7Bt-1%7D) 产生隐层单元 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+h_%7Bt-1%7D) 。并不是 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_%7Bt-1%7D) 中的全部信息都和隐层单元 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+h_%7Bt-1%7D) 有关， ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_%7Bt-1%7D) 可能包含了很多对 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+h_%7Bt-1%7D) 无用的信息。因此， ![[公式]](https://www.zhihu.com/equation?tex=o_t) 的作用就是判断 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_%7Bt-1%7D) 中哪些部分是对 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+h_%7Bt-1%7D) 有用的，哪些部分是无用的。
- **输入门** ![[公式]](https://www.zhihu.com/equation?tex=i_t)。![[公式]](https://www.zhihu.com/equation?tex=i_t) 控制当前词 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+x_t) 的信息融入细胞状态 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_t) 。在理解一句话时，当前词 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+x_t) 可能对整句话的意思很重要，也可能并不重要。输入门的目的就是判断当前词 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+x_t) 对全局的重要性。当 ![[公式]](https://www.zhihu.com/equation?tex=i_t) 开关打开的时候，网络将不考虑当前输入 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+x_t) 。
- **遗忘门** ![[公式]](https://www.zhihu.com/equation?tex=f_t): ![[公式]](https://www.zhihu.com/equation?tex=f_t) 控制上一时刻细胞状态 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_%7Bt-1%7D) 的信息融入细胞状态 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_t) 。在理解一句话时，当前词 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+x_t) 可能继续延续上文的意思继续描述，也可能从当前词 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+x_t) 开始描述新的内容，与上文无关。和输入门 ![[公式]](https://www.zhihu.com/equation?tex=i_t) 相反， ![[公式]](https://www.zhihu.com/equation?tex=f_t) 不对当前词 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+x_t) 的重要性作判断，而判断的是上一时刻的细胞状态 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_%7Bt-1%7D) 对计算当前细胞状态 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_t) 的重要性。当 ![[公式]](https://www.zhihu.com/equation?tex=f_t) 开关打开的时候，网络将不考虑上一时刻的细胞状态 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_%7Bt-1%7D) 。
- **细胞状态** ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_t) ： ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_t) 综合了当前词 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+x_t) 和前一时刻细胞状态 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_%7Bt-1%7D) 的信息。这和ResNet中的残差逼近思想十分相似，通过从 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_%7Bt-1%7D) 到 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_t) 的“短路连接”，梯度得已有效地反向传播。当 ![[公式]](https://www.zhihu.com/equation?tex=f_t) 处于闭合状态时， ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_t) 的梯度可以直接沿着最下面这条短路线传递到 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+c_%7Bt-1%7D) ，不受参数 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+W_%7Bxh%7D) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol+W_%7Bhh%7D) 的影响，这是LSTM能有效地缓解梯度消失现象的关键所在。

##### GRU单元(Gate Recurrent Unit)

GRU和LSTM在很多情况下实际表现上相差无几，但是更易于计算，于是相比之下更容易进行训练，能够很大程度上提高训练效率，因此很多时候会更倾向于使用GRU。

<img src=".\image\GRU.png" alt="GRU" style="zoom: 33%;" />

如图，各种公式为
$$
\begin{array}{1}
z,r=sigmoid(W^{z,r}_x*x^t+W^{z,r}_h*h^{t-1})\\
h^{t-1'}=h^{t-1}\odot{r}\\
h'=sigmoid(W^h_x*x^t+W^h_h*h^{t-1'})\\
h^t=(1-z)\odot{h^{t-1}}+z\odot{h'}\\
\end{array}
$$
可知，GRU只有两个门，ht的更新公式同时起到了记忆和遗忘的功能，那么这里的**h'**实际上可以看成对应于LSTM中的hidden state；上一个节点传下来的 ![[公式]](https://www.zhihu.com/equation?tex=h%5E%7Bt-1%7D) 则对应于LSTM中的cell state。1-z对应的则是LSTM中的 ![[公式]](https://www.zhihu.com/equation?tex=z%5Ef) forget gate，那么 z我们似乎就可以看成是选择门 ![[公式]](https://www.zhihu.com/equation?tex=z%5Ei) 了。

### 为什么循环神经网络如此有用？

## 注意力机制(Attention)

