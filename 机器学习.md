# 机器学习

#### 监督学习-supervised learning

*  **监督学习需要有明确的目标，很清楚自己想要什么结果** ，即训练集具有已知明显特征

* ##### 监督学习的2个任务：回归、分类

  回归： **预测连续的、具体的数值** ，例如预测房价，预测 支付宝里的芝麻信用分数 

  分类： **对各种事物分门别类，用于离散型（[什么是离散？](https://baike.baidu.com/item/离散变量/8443404?fr=aladdin)）预测** ，判断‘是否’，例如认识猫、预测离婚

* ##### 主流的监督学习算法

  | 算法                                          | 类型      | 简介                                                         |
  | :-------------------------------------------- | :-------- | :----------------------------------------------------------- |
  | 朴素贝叶斯                                    | 分类      | 贝叶斯分类法是基于贝叶斯定定理的统计学分类方法。它通过预测一个给定的元组属于一个特定类的概率，来进行分类。朴素贝叶斯分类法假定一个属性值在给定类的影响独立于其他属性的 —— 类条件独立性。 |
  | 决策树                                        | 分类      | 决策树是一种简单但广泛使用的分类器，它通过训练数据构建决策树，对未知的数据进行分类。 |
  | [SVM](https://easyai.tech/ai-definition/svm/) | 分类      | 支持向量机把分类问题转化为寻找分类平面的问题，并通过最大化分类边界点距离分类平面的距离来实现分类。 |
  | 逻辑回归                                      | 分类      | 逻辑回归是用于处理因变量为分类变量的回归问题，常见的是二分类或二项分布问题，也可以处理多分类问题，它实际上是属于一种分类方法。 |
  | 线性回归                                      | 回归      | 线性回归是处理回归任务最常用的算法之一。该算法的形式十分简单，它期望使用一个超平面拟合数据集（只有两个变量的时候就是一条直线）。 |
  | 回归树                                        | 回归      | 回归树（决策树的一种）通过将数据集重复分割为不同的分支而实现分层学习，分割的标准是最大化每一次分离的信息增益。这种分支结构让回归树很自然地学习到非线性关系。 |
  | K邻近                                         | 分类+回归 | 通过搜索K个最相似的实例（邻居）的整个训练集并总结那些K个实例的输出变量，对新数据点进行预测。 |
  | Adaboosting                                   | 分类+回归 | [Adaboost](https://easyai.tech/ai-definition/adaboost/)目的就是从训练数据中学习一系列的弱分类器或基本分类器，然后将这些弱分类器组合成一个强分类器。 |
  | 神经网络                                      | 分类+回归 | 它从信息处理角度对人脑神经元网络进行抽象， 建立某种简单模型，按不同的连接方式组成不同的网络。 |

#### 无监督学习-unsupervised learning

*  **无监督学习则是没有明确目的的训练方式，你无法提前知道结果是什么** ，大概结果的也不知道， **无监督学习不需要给数据打标签** ， **几乎无法明确学习效果如何** 

*  **无监督学习是一种机器学习的训练方式，它本质上是一个统计手段，在没有标签的数据里可以发现潜在的一些结构的一种训练方式。** 

* ##### 无监督学习的一些运用

   **案例1：发现异常** ：有很多违法行为都需要”洗钱”，这些洗钱行为跟普通用户的行为是不一样的，到底哪里不一样？如果通过人为去分析是一件成本很高很复杂的事情，我们可以通过这些行为的特征对用户进行分类，就更容易找到那些行为异常的用户，然后再深入分析他们的行为到底哪里不一样，是否属于违法洗钱的范畴。即可以快速分类出行为有异常的用户。

  **案例2：用户细分**

  这个对于广告平台很有意义，我们不仅把用户按照性别、年龄、地理位置等维度进行用户细分，还可以通过用户行为对用户进行分类。通过很多维度的用户细分，广告投放可以更有针对性，效果也会更好。即自动将用户分开为一群一群

  **案例3：推荐系统**

  大家都听过”啤酒+尿不湿”的故事，这个故事就是根据用户的购买行为来推荐相关的商品的一个例子。比如大家在淘宝、天猫、京东上逛的时候，总会根据你的浏览行为推荐一些相关的商品，有些商品就是无监督学习通过聚类来推荐出来的。系统会发现一些购买行为相似的用户，推荐这类用户最”喜欢”的商品。

####  常见的2类算法是：聚类、降维 

*  聚类：简单说就是一种自动分类的方法，在监督学习中，你很清楚每一个分类是什么，但是聚类则不是，你并不清楚聚类后的几个分类每个代表什么意思。 
*  降维：降维看上去很像压缩。这是为了在尽可能保存相关的结构的同时降低数据的复杂度。 

##### 实例算法(看不懂，以后记)

#### 模型(监督学习)

* 符号：用m表示训练样本数量，x表示输入特征，y表示输出变量，用（x，y）表示样本m。h表示假设函数，用θ表示实数参数

### 文本数据处理

### 图像数据处理

### 线性回归(linear regression)

### 决策树(Decision tree)

泛化公式
$$
p_i=P(X=x_i)
$$

$$
p_{j|i}=P_{Y|X}(y_j|x_i)
$$

$$
H(x)=-\sum_{i=1}^np_ilogp_i
$$

$$
H(Y|X)=\sum_{i=1}^np_x(X=x_i)H(Y|X=x_i)=\sum_{i=1}^n(-p_i\sum_{j=1}^mP_{j|i}logp_{j|i})
$$

$$
g(D,A)=H(D)-H(D|A)
$$

$$
g_{ratio}(D,A)=\frac{g(D,A)}{H_A(D)}
$$

$$
p_i=\frac{|D_i|}{|D|}\ and\ Gini(D)=\sum_{i=1}^kp_i(1-p_k)=1-\sum_{i=1}^kp_i^2
$$

$$
Gini(D,A)=\frac{|D_1|}{D}Gini(D_1)+\frac{|D_2|}{D}Gini(D_2)
$$

### 朴素贝叶斯(Naive bayes)

### K近邻(KNN)

### 神经网络(neural network)

#### 基本概念

* 神经元与网络：
* 输入层、隐藏层和输出层：
* 权重、参数：
* 激活项：

#### 反向传播算法：

* 反向传播的概念：
* 权值更新：

#### 神经网络python实现：

### 模型与文件：

#### 模型保存与调用：

#### 1.pickle与joblib：

* 原理：通过函数保存对象在文件中，保存对象即指将传入类的**self**当前状态保存在文件中，以便于在另一个文件将该状态赋予另一个对象中

* 保存模型

  >   joblib.dump(object, "modelname.m")  or   pickle.dump(obj, file)

* 调用本地模型

  >  object = joblib.load("modelname.m")   or   pickle.load(file)    

​       tip: file means the file object and the way to open it

#### 源代码文件书写规范：

![1606917567006](C:\Users\86151\AppData\Roaming\Typora\typora-user-images\1606917567006.png)

#### 模型测试：

##### 回归:

1.  **MSE**: Mean Squared Error , 参数估计值与参数真值之差平方的期望值 ， MSE的值越小，说明预测模型描述实验数据具有更好的精确度。 
   $$
   MSE=\frac{1}{N}\sum_{t=1}^N(y-y_i)^2
   $$

2.  **MAE** :Mean Absolute Error , 平均绝对误差是绝对误差的平均值平均绝对误差能更好地反映预测值误差的实际情况. 
   $$
   MAE=\frac{1}{N}\sum_{i=1}^N|(y-y_i)|
   $$

##### 分类:







