现有的执行优化方案在启发式方法上都差不多，至少在非定制的开源数据库之中，启发式方法已经被证明其作用是有限的。在这样的背景下，基于代价的优化方法是成为当前研究的主流，基于代价优化（CBO）原理是计算所有执行路径的代价，并挑选代价最小的执行。

# 基数估计

计算代价的重要指标是基数，基数估计（cardinality estimation）就是估计基数的一类方法的统称。

基数估计的作用主要可概括成如下两个方面：

 （1）基数估计是数据库查询优化的重要一环。由于子查询的基数代表了中间结果的大小，因此现有的大多数基于代价的查询优化技术对子查询计划代价的估计**强烈依赖对子查询基数的估计**。一个优秀的基数估计模型能帮助优化器更好地选择合适的执行计划。

 （2）基数估计可以帮助回答一些只需要知道结果数量的查询。例如某个用户可能想知道全中国有多少大学生学习计算机专业，这个查询只关心结果的数目而不需要知道每条结果的具体值。对这样的查询我们可以通过基数估计的方法**快速返回结果大小的近似值**。

基数估计是一个重要且较为困难的问题，几十年来一直有研究者尝试用各种方法和技巧提升估计的准确性和稳健性，现有的方法可以分为如下几类：

**目前方法的分类**

基数估计是一个重要且较为困难的问题，几十年来一直有研究者尝试用各种方法和技巧提升估计的准确性和稳健性，现有的方法可以分为如下几类：

![img](https://img-blog.csdnimg.cn/img_convert/06b4323d3db4c9f82aaaac3606b36a2f.png)

总体上可以分类传统型方法和基于[机器学习](https://so.csdn.net/so/search?q=机器学习&spm=1001.2101.3001.7020)的方法。下面会选取其中具有代表性的几个工作简要介绍。

**传统型基数估计方法**

传统型基数估计方法大体上可以分为基于概要(synopsis-based)和采样(sampling)的两大类方法。

基于概要的基数估计方法会预先**收集数据库的一些统计信息**，并**基于独立性等简单假设**，能够方便**快速地**求解查询基数。

例如基于直方图(histogram)的方法，其对数据表中**每一列总结成一张等宽(equal width)或等深(equal depth)的直方图**，最后根据查询条件，基于列与列之间相互独立的假设，估计出查询结果的大小。当然有时这样的独立性假设效果不好，便可根据多维直方图(multi-dimensional histogram)的方法求解。

数据画像(sketching)也属于基于概要的基数估计方法，其使用**bitmap或哈希的方法**，估计数据库中互不相同的元素个数，比较适合流式数据的场景。更新的一类数据画像的方法如loglog、hyperloglog等，更加注重节约内存和提升估计的稳健性。

基于采样的方法会从原始数据表中**随机抽取一定比例或者一定数量的元组**，最终根据在采样集上执行查询后的结果大小除以相应的缩放比例便可得到查询在原数据库的基数估计。基于采样的方法在采样策略较好、**能反映大部分数据分布的情况下效果较好**，但也很容易由于采样的随机性无法产生采样集上匹配结果的情况，便有研究者提出了一种基于索引的采样方法(index-based sampling)，其在多表连接时，会根据查询的连接条件选择要采样的范围，**能减少采样出现零结果的情况**，具体图示如下：


![img](https://img-blog.csdnimg.cn/img_convert/89fd4edddadae640699dac88b146b2b0.png)

**查询驱动的学习型基数估计方法**

由于基数估计可以视为“查询”到”基数值”的回归问题(当数据库没有更新时)，故我们可以将监督学习的技巧迁移到本问题上，直接**学习查询到基数的对应关系**。一般的查询驱动方法的工作流程如下所示：

![img](https://img-blog.csdnimg.cn/img_convert/718f4dde24997a80ca96d5f93a983ba6.png)

其难点主要在于训练数据的获取和查询语句的表示两方面。

通常而言，当数据库给定时，我们可以假设查询框架(schema)是已知的（例如，某两个表会根据哪些列做连接等），由此我们可以随机产生一些查询语句，将这些语句真实运行之后，我们便得到了这些查询的真实基数，这就构成了训练模型需要的训练数据。当然也可以根据数据库的查询日志直接获取查询。需要注意的是，训练数据一定要有代表性，否则模型不能学习到工作环境中常见的查询模式到基数的对应，会影响到预测的准确性。

查询语句的表示主要由查询表、谓词条件、连接条件等决定，更复杂的情况包括含正则表示式之类的“like”查询。

这是一个很新的领域，因此可参考的方法不多。

**数据驱动的学习型基数估计方法**

与查询驱动类的方法不同，数据驱动的基数估计方法为无监督的一类方法，直接从数据表中学习数据的分布，因此训练较为简单。

在工作 [5] 中，作者使用核密度估计(kernel density estimation, KDE)的方法估计查询基数。其想法为在数据表中随机抽取若干元组，这样查询点和采样元组越靠近，那该查询有结果的可能性就会较大。作者使用了高斯核，并用可学习的参数控制概率密度的分散程度。对于数据库更新场景，作者使用蓄水池抽样(reservoir sampling)应对数据插入；通过监测去除某个采样元组对估计效果的影响应对数据删除（即如果删去某个元组能提升估计效果，则将其删去）。由于计算只需要对每个采样元组计算概率再相加，故可以通过GPU加速计算。作者也在之后的研究中，将核密度估计的方法拓展到多表连接的基数估计 [6]。下图展示了核密度估计的主要思想：

![img](https://img-blog.csdnimg.cn/img_convert/9f91341a047c3fa1f37a104173bf1d93.png)

还有一类使用自回归模型估计基数的方法。在工作[7]中，作者提出了Naru模型，在训练过程中还融合了采样和Mask技巧，能够提升模型的准确性。作者其后也将该方法拓展到多表连接的情形 [8]。

另外一类数据驱动的方法使用了SPN(sum product network)。主要思想为通过对数据表做横向划分（例如使用KMeans等聚类方法），使元组各列在分割后的数据上近似独立，这样便可通过在每一列上构建直方图或线性回归模型估计基数。DeepDB [9] 是一个使用SPN的基数估计方法，近年来也有对SPN的改进，如FLAT [10]，其将数据表的各类分为强相关和弱相关两类，再进一步使用SPN方法划分数据表。下图展示了DeepDB是如何计算“年龄小于30岁的欧洲顾客人数”的：

![img](https://img-blog.csdnimg.cn/img_convert/0ef35001d5ad5dc8c089a1d8650b1274.png)

**目前的困难和未来研究方向**

第一点，尽管目前的方法能比传统方法更准确，但也耗费了大量时间训练模型，如何在准确率和效率之间取舍是应用这些方法的数据库使用者需要考虑的问题。

第二点，大部分的模型都很难适应更新环境。当有大量数据插入或删除是，这些模型很难追踪这些改动，因此需要重新训练，这在OLTP环境中是需要改进的。

第三点，这些模型在多表连接的场景下估计效果仍然很差。多表连接时，不同表之间的依赖性难以捕捉，给基数估计带来了较大的困难。

# 代价估计

**Introduction**

查询优化器会对一个查询生成多个执行计划，并对每个执行计划预测其执行时间，选出执行时间最短的计划。因此，预测执行计划的执行时间是非常重要。一般优化器会构建代价模型来估算计划的执行时间。但由于数据库的复杂性，代价模型有可能会偏离实际的执行情况。研究者尝试用一些基于学习的方法来实现更好的代价估算。其中一种基于学习的方法是采用机器学习方式来构建代价模型。而另一种方法是用回归的方式校准传统代价模型中的参数。

**Apply Machine Learning To Cost Estimation**

机器学习有很强的建模能力，因此我们考虑将其应用于代价估算。大体上来说，我们会先从执行计划中抽取特征，然后输入到机器学习模型中，让模型预测计划的执行时间。你可以在 TiDB 上执行 `EXPLAIN ANALYZE` 语句获得执行计划的相关信息。

![](http://pic.netpunk.top/images/2022/11/24/20221124173031.png)

整个执行计划按照树状组织，每一行对应一个算子，主要的信息有：

1. id: 对应的算子名称;
2. estRows: 优化器对该算子返回行数的估算值;
3. actRows: 该算子实际的返回行数;
4. estCost: 优化器对该计划的估计值;
5. execution: 实际的执行情况，包含执行时间;

NOTICE: 训练和测试用的执行计划是通过特殊模式获取的，可以保证 `estRows` 和 `actRows` 一定相同，以避免基数估算误差对代价模型的影响。

你可以从计划当中提取特征，用来训练你的模型；不过请不要使用 `execution info`, `memory`, `disk` 和 `actRows` 字段的信息作为特征，因为这些信息需要在实际执行计划后才能获得，不应该在估算计划代价时被使用。

提取特征的方式可以有多种:

1. 最简单的方式是从各个节点抽取特征然后求和，以这种方式我们可以得到如下特征：

   a. 节点总数；

   b. 每种算子出现的次数；

   c. 每种算子对应的 `estRows` 之和。

   然而，这种抽取特征的方式将执行计划的树形结构丢弃了。

2. 第二种方式是从各个节点抽取特征以后，按照 DFS 前序遍历执行计划的顺序将各个节点的特征拼接起来。

![](http://pic.netpunk.top/images/2022/11/24/20221124172851.png)

NOTICE1: 在 DFS 前序遍历执行计划时，当我们遍历完该节点的所有孩子之后，我们会插入一个空节点表示所有孩子已经遍历完。这样执行计划和 DFS 前序遍历得到的序列构成了一对一的映射。 NOTICE2: 不同执行计划有不同数量的节点，因此得到的特征向量长度也不同。我们需要对特征向量做 padding，让它们的长度一致。

你可以使用 MLP、LSTM 等神经网络来对代价模型进行建模。

我们将估算的代价直接和实际执行时间对齐，在计算损失函数时，你可以用如下的平均相对误差：

```
1/N \sum_{i=1}^N |act_time_i - est_cost_i| / act_time_i
```

采用以上方法进行代价估算时，我们需要从计划中提取特征和执行时间，生成训练数据集来训练对应的机器学习模型。

**Calibrate Cost Factors By Regression Model**

在这个小节我们将介绍一个更加 "白盒" 的方法来建立我们的 Cost Model。

在传统的数据库系统中，会为每个算子设计不同的代价公式，比如：

1. 对于 Scan 算子，可能采用的代价公式为 `input_rows * row_size * scan_factor`，input_rows 和 row_size 分别对于扫描的行数和行宽，scan_factor 则是扫描代价因子，对应扫描 1 byte 数据的代价；
2. 对于 Sort 算子，可能采用的代价公式为 `input_rows * log2(input_rows) * cpu_factor`，其中 input_rows * log2(input_rows) 对应排序的复杂度，cpu_factor 则是 CPU 代价因子，对应 CPU 处理 1 行数据的代价；

而整个 Plan 的计划则是该 Plan 所有算子代价的累计，可见对于 Plan Cost 而言，关键点有下面两个：

1. 合理的代价公式：代价公式是对算子执行的抽象，需要对算子的物理实现有所了解才能设计出合理的代价公式；
2. 合理的代价因子：代价因子是对物理环境的刻画，比如 cpu_factor 和 scan_factor 的比值可以反应环境中 CPU 和 I/O 的速率；

合理的代价公式需要由人来保证，设计好了代价公式后，可以用回归的方式来对代价因子进行校准。

对于任意的算子，我们可以将其代价转换为代价因子向量(fv)和代价因子权重向量(wv)的乘积，如假设我们代价公式只考虑 scan，cpu，对于 Sort 算子，其代价就是 `[0, input_rows*log2(input_rows)] * [scan_factor, cpu_factor]`。

同理，对于任意 Plan，也可以将其代价转换成上述代价的形式，如下面这个 Plan:

```
+----------+---------+---------------+
| id       | estRows | operator info |
+------------------------------------+
| Sort     | 100.00  |               |
| └─Scan   | 100.00  | row_size: 12  |
+----------+---------+---------------+
```

其代价可以转换成 `[100*12, 100*log2(100)] * [scan_factor, cpu_factor]` 的形式。

如果在我们的数据集上生成了 N 个 Plan，则我们可以得到 N 个权重向量(wv)，及这 N 个 Plan 的执行时间 t，然后通过回归的方式，找到一组代价因子向量(fv)，使得这组代价因子向量能让代价尽量的与时间对齐，如让 `avg(|t-wv*fv|)` 最小。

# 零零碎碎

## 直方图

> 直方图非常重要，至少目前这是很多数据库基数估计的best practice

**简介：** 与统计学中的直方图类似，直方图是一种按数据出现的频率来进行分类存储的方法。在oracle中直方图是用来描述表中列数据的分布情况。每一个sql在被执行前都要经过优化这一步骤那么在优化器给出一个最优执行计划之优化器应该要知道sql语句中所引用的底层对象的详细信息。

在 TiDB 中，会对每个表**具体的列**构建一个**等深直方图**，**区间查询**的估算便是借助该直方图来进行。等深直方图，就是让落入每个桶里的值数量尽量相等。举个例子，比方说对于给定的集合 {1.6, 1.9, 1.9, 2.0, 2.4, 2.6, 2.7, 2.7, 2.8, 2.9, 3.4, 3.5}，并且生成 **4 个桶**，那么最终的**等深直方图**就会如下图所示，包含四个桶 [1.6, 1.9]，[2.0, 2.6]，[2.7, 2.8]，[2.9, 3.5]，其**桶深均为 3**。

![等深直方图示例](https://download.pingcap.com/images/docs-cn/statistics-1.png)

当桶数量越多，直方图的估算精度就越高，不过也会同时增大统计信息的内存使用，可以视具体情况来做调整。

在TIDB中，一个数据库的所有桶可以用下表表示：

![](http://pic.netpunk.top/images/2022/11/23/20221123222120.png)

如果将直方图导出为json，就会如下图所示

![](http://pic.netpunk.top/images/2022/11/23/20221123222515.png)

根据表的大小不同，这样的桶数量动辄数十万也是正常的

## Count-Min sketch

Count-min Sketch算法是一个可以用来计数的算法，在数据大小非常大时，一种高效的计数算法，通过牺牲准确性提高的效率。在TIDB中，当查询中出现诸如 `a = 1` 或者 `IN` 查询（如 `a in (1, 2, 3)`）这样的等值查询时，TiDB 便会使用这个数据结构来进行估算。

原理看这个博客：[Count-min Sketch 算法 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/369981005)

如果导出TIDB中的CM Sketch数据，可以发现会如下图所示

![](http://pic.netpunk.top/images/2022/11/23/20221123223649.png)

统计了每个出现过的数据的上限数







