# 监督学习-supervised learning

* **监督学习需要有明确的目标，很清楚自己想要什么结果** ，即训练集具有已知明显特征

* ##### 监督学习的2个任务：回归、分类

  回归： **预测连续的、具体的数值** ，例如预测房价，预测 支付宝里的芝麻信用分数 

  分类： **对各种事物分门别类，用于离散型（[什么是离散？](https://baike.baidu.com/item/离散变量/8443404?fr=aladdin)）预测** ，判断‘是否’，例如认识猫、预测离婚

* ##### 主流的监督学习算法

  | 算法                                          | 类型      | 简介                                                         |
  | :-------------------------------------------- | :-------- | :----------------------------------------------------------- |
  | 朴素贝叶斯                                    | 分类      | 贝叶斯分类法是基于贝叶斯定定理的统计学分类方法。它通过预测一个给定的元组属于一个特定类的概率，来进行分类。朴素贝叶斯分类法假定一个属性值在给定类的影响独立于其他属性的 —— 类条件独立性。 |
  | 决策树                                        | 分类      | 决策树是一种简单但广泛使用的分类器，它通过训练数据构建决策树，对未知的数据进行分类。 |
  | [SVM](https://easyai.tech/ai-definition/svm/) | 分类      | 支持向量机把分类问题转化为寻找分类平面的问题，并通过最大化分类边界点距离分类平面的距离来实现分类。 |
  | 逻辑回归                                      | 分类      | 逻辑回归是用于处理因变量为分类变量的回归问题，常见的是二分类或二项分布问题，也可以处理多分类问题，它实际上是属于一种分类方法。 |
  | 线性回归                                      | 回归      | 线性回归是处理回归任务最常用的算法之一。该算法的形式十分简单，它期望使用一个超平面拟合数据集（只有两个变量的时候就是一条直线）。 |
  | 回归树                                        | 回归      | 回归树（决策树的一种）通过将数据集重复分割为不同的分支而实现分层学习，分割的标准是最大化每一次分离的信息增益。这种分支结构让回归树很自然地学习到非线性关系。 |
  | K邻近                                         | 分类+回归 | 通过搜索K个最相似的实例（邻居）的整个训练集并总结那些K个实例的输出变量，对新数据点进行预测。 |
  | Adaboosting                                   | 分类+回归 | [Adaboost](https://easyai.tech/ai-definition/adaboost/)目的就是从训练数据中学习一系列的弱分类器或基本分类器，然后将这些弱分类器组合成一个强分类器。 |
  | 神经网络                                      | 分类+回归 | 它从信息处理角度对人脑神经元网络进行抽象， 建立某种简单模型，按不同的连接方式组成不同的网络。 |

## 线性模型(linear model)

线性模型试图学得一个通过属性的线性组合来进行预测的函数，即
$$
f(x)=w_{1}x_{1}+w_{2}x_{2}+...+w_{t}x_{t}+b,
$$
线性模型形式简单，但是其思想十分重要。许多强大的非线性模型可以在线性模型的基础上通过引入层级结构或者高维映射而得。

并且由于w直观的表达了各属性在预测中的重要性，所以线性模型有很好的可解释性。

线性回归模型存在一种假设，噪声服从0均值的正态分布

**最小二乘回归(OLS Regression)**

以均方误差为优化函数的线性回归模型就是最小二乘回归模型

均方误差有非常好的几何意义，在线性回归中，最小二乘法就是识图找到一条曲线，使得所有样本到曲线上的距离最小。

最小二乘法用途很广，不仅限于线性回归

最小二乘法对线性回归的矩阵解释：

可利用最小二乘法来对w和b进行估计，若数据集为满秩矩阵或正定矩阵，则有标准线性回归公式
$$
\hat{\boldsymbol{w}}^{*}=\left(\mathbf{X}^{\mathrm{T}} \mathbf{X}\right)^{-1} \mathbf{X}^{\mathrm{T}} \boldsymbol{y}
$$
然而，现实任务中数据集往往不是满秩矩阵。例如在许多任务中我们会遇到大量的变量，其数目甚至超过样例数，导致数据集的列数多于行数，数据集显然不满秩。此时可解岀多个最优权重，它们都能使均方误差最小化。选择哪一个解作为输出将由学习算法的归纳偏好决定，常见的做法是引入正则化（regularization）项

（解线性方程组时，若因变量过多，则会解出多组解）

**广义线性模型**

线性模型虽然简单，但是有丰富的变化，如对数线性回归，在形式上仍是线性回归，但实质上已是在求取输入空间到输出空间的非线性函数映射。

更一般的，考虑单调可微函数G(point)，令
$$
y=g^{-1}(w^{T}x+b)
$$
这样得到的模型称为“广义线性模型”( generalized linear model),其中函数g()称为“联系函数”( link function).显然,对数线性回归是广义线性模型在g()=ln()时的特例

广义线性回归模型的参数估计常通过加权最小二乘法或极大似然法进行

**对数概率回归(logistic regression)**

逻辑回归虽然称为回归，但却用于二分类问题，与最小二乘回归相比，优化任务从回归变为分类表示逻辑回归算法需要在两个方面上进行变化

联系函数：考虑二分类任务，其输出标记y∈{0,1}，我们需要将实值z转换为0/1值，最理想的联系函数是“单位阶跃函数”，就是sigmoid函数

优化函数：考虑到问题的性质发生改变，我们需要改变优化函数，例如极大似然函数或者交叉熵损失函数，它们都是高阶连续可导凸函数

**岭回归(Ridge Regression)**

[机器学习算法实践-岭回归和LASSO - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/30535220)

由标准线性回归公式，我们知道，当特征矩阵为奇异矩阵时，无法对其进行求解，而

* 特征矩阵本身存在线性相关关系(多重共线性), 即非满秩矩阵。如果数据的特征中存在两个相关的变量，即使并不是完全线性相关，但是也会造成矩阵求逆的时候造成求解不稳定。
* 当数据特征比数据量还要多的时候, 即 ![[公式]](https://www.zhihu.com/equation?tex=m+%3C+n) , 这时候矩阵 ![[公式]](https://www.zhihu.com/equation?tex=X) 是一个矮胖型的矩阵，非满秩。

我们可以用标准线性回归做一定的变化使原先无法求逆的矩阵变得非奇异

例如，我们在最小二乘损失函数上添加一个L2正则化惩罚项，公式为
$$
f(w)=\sum_{i=1}^{m}\left(y_{i}-x_{i}^{T} w\right)^{2}+\lambda \sum_{i=1}^{n} w_{i}^{2}
$$
此时，岭回归系数用矩阵表示为
$$
\hat{w}=\left(X^{T} X+\lambda I\right)^{-1} X^{T} y
$$
可以看到，就是通过奖特征矩阵加上一个单位举证变成非奇异矩阵并可以进行求逆运算

岭回归限定了所有回归系数的平方和不大于 ![[公式]](https://www.zhihu.com/equation?tex=t) , 在使用普通最小二乘法回归的时候当两个变量具有相关性的时候，可能会使得其中一个系数是个很大正数，另一个系数是很大的负数。通过岭回归的限制，可以避免这个问题

岭回归的几何意义更加直观，以两个变量为例, 残差平方和可以表示为 ![[公式]](https://www.zhihu.com/equation?tex=w_1%2C+w_2) 的一个二次函数，是一个在三维空间中的抛物面，可以用等值线来表示。而限制条件 ![[公式]](https://www.zhihu.com/equation?tex=w_1%5E2+%2B+w_2%5E2+%3C+t) ， 相当于在二维平面的一个圆。这个时候等值线与圆相切的点便是在约束条件下的最优点。

注意，当参数λ=0时，得到的是最小二乘解，当参数λ变大时，岭回归系数趋于0，约束项很小

岭迹图：不同的λ得到不同的岭参数，我们可以通过画岭迹图来观察较佳的λ取值，观察变量是否由多重共线性

 <img src="D:\Coder\Github\学习笔记\image\岭图.png" alt="岭图" style="zoom:50%;" />λ增大，系数趋于0，由不稳定趋于稳定

**Lasso回归(Lasso Regression)**

Lasso在很多地方与岭回归相似，不同的是，Lasso正则化惩罚项是L1正则化。这一变化带来的区别在几何层面上解释更为直观。

<img src="D:\Coder\Github\学习笔记\image\两种回归的区别.png" alt="两种回归的区别" style="zoom:50%;" />相比圆，方形的顶点更容易与抛物面相交，顶点就意味着对应更多的系数为0，即Lasso回归的正则化方法起到了很好的筛选变量的作用。两者在效果上的区别即是，Lasso回归针对某些问题十分有效，也就是说大部分时候岭回归更加有效一些。

两种回归均是在标准线性回归的基础上加上正则项来减小模型的方差。这里其实便涉及到了权衡偏差(Bias)和方差(Variance)的问题。方差针对的是模型之间的差异，即不同的训练数据得到模型的区别越大说明模型的方差越大。而偏差指的是模型预测值与样本数据之间的差异。所以为了在过拟合和欠拟合之前进行权衡，我们需要确定适当的模型复杂度来使得总误差最小。(废话)

**弹性网回归(Elastic-Net Regression)**

[机器学习笔记-Ridge回归、Lasso回归和弹性网回归 - WarningMessage - 博客园 (cnblogs.com)](https://www.cnblogs.com/dataanaly/p/12972435.html)

Lasso回归虽然大大降低了预测方差，达到了系数收缩和变量选择的目的，但是也有一定的局限性

* 在Lasso回归求解路径中，对于m×d的设计矩阵来说，最多只能选出min(m,d)min(m,d)个变量。当d>m的时候，最多只能选出m个预测变量。因此，对于d∼m的情况，Lasso方法不能够很好的选出真实的模型。
* 如果预测变量具有**群组效应**，则用Lasso回归时，只能选出其中的一个预测变量。
* 对于通常的m>dm>d的情形，如果预测变量中存在很强的共线性，Lasso的预测表现受控于岭回归。

弹性网回归则解决了这些问题，解决方法即是将岭回归和Lasso回归的正则化惩罚项凸线性组合，公式为
$$
E(w)=\min _{w} \sum_{i=1}^{m}\left(y_{i}-\sum_{j=1}^{d} x_{i j} w_{j}\right)^{2}+\lambda \sum_{j=1}^{d}\left|w_{j}\right|+\lambda \sum_{j=1}^{d} w_{j}^{2}
$$
当α=0时，弹性网回归即为岭回归；当α=1时，弹性网回归即为Lasso回归。因此，弹性网回归兼有Lasso回归和岭回归的优点，既能达到变量选择的目的，又具有很好的群组效应。(此处α的含义不细究)

**多项式回归(Polynomial Regression)**

直线回归研究的是一个依变量与一个自变量之间的回归问题。研究一个因变量与一个或多个自变量间多项式的回归分析方法，称为多项式回归（Polynomial Regression）多项式回归模型是线性回归模型的一种。多项式回归问题可以通过变量转换化为多元线性回归问题来解决。如上，多项式回归没啥好说的

## 决策树(Decision tree)

不同于逻辑回归，决策树属于**非线性模型**，可以用于分类，也可用于回归，可以认为是if-then规则的集合，是以实例为基础的归纳学习。决策树的分支需要一定的标准，怎么准确的定量选择这些标准是算法性能高低的关键

分类决策树模型是一种描述对实例进行分类的树形结构，决策树由结点和有向边组成，节点有两种类型，内部节点和叶节点，内部节点表示一个特征或属性，叶节点表示一个类

<img src=".\image\决策树的几种表现形式.png" alt="决策树的几种表现形式" style="zoom:50%;" />

决策树表示的是给定特征条件下的条件概率分布，这一条件概率分布定义在特征空间的一个划分（partition）上，将特征空间划分为互不相交的单元（cell）或区域（region）。

**决策树ID3算法**

决策树的前置概念有信息熵、联合熵、条件熵、信息增益

信息熵：用来定量表达一个概率系统不确定程度的方法，越不确定的事物，它的熵就越大。在决策树模型中，我们需要通过所有样本的离散值来计算样本熵，公式为：
$$
H(x)=-\sum_{i=1}^np_ilogp_i
$$
联合熵：联合熵是一种变量之间不确定性的衡量手段，公式为：
$$
H(X)=-\sum_{i=1}^{n} p\left(x_{i}, y_{i}\right) \log p\left(x_{i}, y_{i}\right)
$$
条件熵：有了联合熵就可以得到条件熵的表达式，公式为：
$$
H(X \mid Y)=-\sum_{i=1}^{n} p\left(x_{i}, y_{i}\right) \log p\left(x_{i} \mid y_{i}\right)=\sum_{j=1}^{n} p\left(y_{j}\right) H\left(X \mid y_{i}\right)
$$
在ID3算法中，条件熵用于度量在某个特征已知的条件下，系统还保留的不确定性

信息增益：我们需要用一个量去衡量某种特征对样本来说，信息量最大，那么它就最合适分类，为此我们引入信息增益的概念，公式为：
$$
I(D,A)=H(D)-H(D|A)
$$
最后，算法的具体过程为：

1. 初始化信息增益的阈值ϵ
2. 判断样本是否为同一类输出 Di，如果是则返回单节点树T。标记类别为 Di
3. 判断特征是否为空，如果是则返回单节点树T，标记类别为样本中输出类别D实例数最多的类别
4. 计算A中的各个特征（一共n个）对输出D的信息增益，选择信息增益最大的特征 Ag
5. 如果 Ag 的信息增益小于阈值ϵ，则返回单节点树T，标记类别为样本中输出类别D实例数最多的类别
6. 否则，按特征 Ag 的不同取值 Agi 将对应的样本输出D分成不同的类别Di。每个类别产生一个子节点。对应特征值为 Agi 。返回增加了节点的数T
7. 对于所有的子节点，令D=Di,A=A−{Ag}递归调用2-6步，得到子树Ti并返回

**决策树C4.5算法**

相对于ID3的方法，C4.5算法改进了四个问题，一是不能处理连续特征，第二个就是用信息增益作为标准容易偏向于取值较多的特征。提出了连续特征、特征熵、信息增益率的概念

对于连续特征，C4.5的思路是将连续的特征离散化。比如m个样本的连续特征A有m个，从小到大排列a1,a2,...,am,则C4.5取相邻两样本值的平均数，一共取得m-1个划分点，对于这m-1个点，分别计算以该点作为二元分类点时的信息增益。选择信息增益最大的点作为该连续特征的二元离散分类点

特征熵：特征数越多的特征对应的特征熵越大，公式为
$$
H_{A}(D)=-\sum_{i=1}^{n} \frac{|D i|}{|D|} \log _{2} \frac{|D i|}{|D|}
$$
信息增益率：我们需要将取值较多的特征的信息增益缩小，将取值较少的特征的信息增益相对增大，故引入信息增益率这个概念，公式为：
$$
I_{R}(D, A)=\frac{I(A, D)}{H_{A}(D)}
$$
经证明，在决定连续特征的分界点时采用增益这个指标，而选择属性的时候才使用增益率这个指标能选择出最佳分类特征，

![C4.5](D:\Coder\Github\学习笔记\image\C4.5.png)

对于缺失值处理

**CART决策树**

CART假设决策树是二叉树，内部节点特征的取值为是和否。在决策树算法中，寻找最优决策树是一个NPC问题，即无法用计算机在多项式时间内，找出全局最优解，因此，大多数决策树算法都采用启发式的思想，在每一个节点上寻找局部最优解，而在全局上，决策树无法保证全局最优解

* NPC问题指的是计算机对于能否在多项式时间内求出问题的解是未知的，但是可以确定在多项式时间内验证这个解

CART算法解决了C4.5算法的几个问题：

1. 决策树算法非常容易过拟合，因此对于生成的决策树必须要进行剪枝。剪枝的算法思路主要是两种，一种是预剪枝，即在生成决策树的时候就决定是否剪枝。另一个是后剪枝，即先生成决策树，再通过交叉验证来剪枝。CART中主要用的是后剪枝
2. 很多时候，在计算机中二叉树模型会比多叉树运算效率高。CART采用完全二叉树作为，提高了计算效率
3. C4.5只能用于分类，CART能将决策树用于回归
4. C4.5由于使用了熵模型，里面有大量的耗时的对数运算，如果是连续值还有大量的排序运算。CART采用基尼系数来代替信息增益进行特征选择，能够简化模型，而不至于完全丢失熵模型的优点

**分类树的生成**

基尼指数(Gini不纯度)表示在样本集合中一个随机选中的样本被分错的概率

Gini指数越小表示集合中被选中的样本被分错的概率越小，也就是说集合的纯度越高，反之，集合越不纯。当集合中所有样本为一个类时，基尼指数为0

对多个样本，计算不同特征的加权基尼指数和来作为样本集合的基尼不纯度

分类问题中，假设有K类，样本点属于第K类的概率是Pk，则概率分布的基尼指数定义为：
$$
Gini(p)=\sum_{k=1}^{K}p_{k}(1-p_{k})=1-\sum_{k=1}^{K}p_{k}^{2}
$$
对于给定的样本集合D，其基尼指数为：
$$
Gini(D)=1-\sum_{k=1}^{K}(\frac{|C_{k}|}{|D|})^2
$$
在特征A的条件下，集合D的基尼指数定义为：
$$
Gini(D,A)=\frac{|D_1|}{D}Gini(D_1)+\frac{|D_2|}{D}Gini(D_2)
$$
CART分类树生成算法流程为：

输入：训练集D，基尼知数阈值

输出：CART分类决策树T

**回归树的生成**

回归树( regression tree),就是用树模型做回归问题，每片叶子都输出一个预测值。预测值一般是叶子节点所含训练集元素输出的均值，但是在叶节点包含数据集较多的时候，也可以使用其它回归方法如线性回归来预测数值

在分类树中最佳划分点的判别标准是熵或者基尼系数，都是用纯度来衡量的，但是在回归树中的样本标签是连续数值，所以需要用**平方误差**来评判拟合程度

CART回归树树生成算法流程为：

输入：训练集D，误差（变化系数）阈值

选择第j个变量和它的取值s，作为切分变量和切分点，对每一个固定输入变量j可以找到最优切分点s，遍历所有输入变量，依次将输入空间划分为两个区域，接着重复上述过程，知道满足停止条件为止  
$$
\min _{j, s}\left[\min _{c_{1}} \sum_{x_{i} \in R_{1}(j, s)}\left(y_{i}-c_{1}\right)^{2}+\min _{c_{2}} \sum_{x_{i} \in R_{2}(j, s)}\left(y_{i}-c_{2}\right)^{2}\right]
$$
![回归树节点划分公式](D:\Coder\Github\学习笔记\image\回归树节点划分公式.png)

输出：CART回归决策树

## 支持向量机(Support Vector Machine)

[看了这篇文章你还不懂SVM你就来打我 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/49331510)

**间隔与支持向量**

从简单来讲，SVM的定义就是一种二元分类器，它的基本模型就是定义在特征空间上的与**支持向量间隔最大**的线性分类器

<img src="D:\Coder\Github\学习笔记\image\SVM基础直观.png" alt="SVM基础直观" style="zoom:50%;" />

如上图，在p维的特征空间上，位于p-1维的超平面H将样本分为两类，其中，我们评价H3比H2要分的更好，直观的看是H3的鲁棒性比较好，量化来看。

支持向量：在分类范围内，距离超平面最近的几个样本点使得约束条件取等值的点就称为支持向量

间隔：两个异类支持向量到超平面的距离之和称为间隔

取间隔最大的超平面作为最后的分类器，间隔的公式为
$$
margin=p=\frac{2}{||W||} \\
使margin最大，有等价关系\\
\max _{W, b} \rho \Longleftrightarrow \max _{W, b} \rho^{2} \Longleftrightarrow \min _{W, b} \frac{1}{2}\|W\|^{2}\\即，间隔最大化问题的数学表达就是\\
\begin{aligned}
&\min _{W, b} J(W)=\min _{W, b} \frac{1}{2}\|W\|^{2} \\
&\text { s.t. } \quad y_{i}\left(X_{i}^{T} W+b\right) \geq 1, i=1,2, \ldots n .
\end{aligned}
$$
这里问题貌似与b无关，但是b的取值隐式的影响了w的取值，故b的取值也十分重要，具体下面讲

**对偶问题**

前面已经得到了优化目标，可以直接优化w来得到最优解。但是现实中我们一般应用拉格朗日乘子法构造其对偶拉格朗日函数，求解该函数也能够得到最优解，好处就是

- 对偶问题更易求解，由下文知对偶问题只需优化一个变量α且约束条件更简单；
- 能更加自然地引入核函数，进而推广到非线性问题；

转化步骤如下
$$
首先构建拉格朗日函数。为此需要引进拉格朗日乘子(Lagrange multiplier) \alpha_{i} \geq 0, i=1,2, \ldots n 。则拉格朗日函数为:\\
L(W, b, \alpha)=\frac{1}{2}\|W\|^{2}-\sum_{i=1}^{n} \alpha_{i}\left[y_{i}\left(X_{i}^{T} W+b\right)-1\right]
\\因此，给定一个 W 和 b, 若不满足约束条件, 那么有\\
\max _{\alpha} L(W, b, \alpha)=+\infty\\
否则，若满足约束条件, 有\\
\max _{\alpha} L(W, b, \alpha)=J(W)=\frac{1}{2}\|W\|^{2}\\
结合可知, 优化问题通过对偶性转化为了\\
\max _{\alpha} \min _{W,b} L(W, b, \alpha)
$$
此时问题转为了两个子问题，即对L中W和b求极小，对α求极大

(1)将上面两式代入 L(W,b,α) ，有
$$
\min _{W, b} L(W, b, \alpha)=min(-\frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i} \alpha_{j} y_{i} y_{j} X_{i}^{T} X_{j}+\sum_{i=1}^{n} \alpha_{i})
$$
(2)求L(W,b,α)对α的极大:
等价于上式 对α求极大, 也等价于上式取负数后对α求极小，即
$$
\min _{\alpha} \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i} \alpha_{j} y_{i} y_{j} X_{i}^{T} X_{j}-\sum_{i=1}^{n} \alpha_{i}
$$
同时满足约束条件:
$$
\begin{aligned}
&\sum_{i=1}^{n} \alpha_{i} y_{i}=0 \\
&\alpha_{i} \geq 0, i=1,2, \ldots, n
\end{aligned}
$$
至此，我们得到了原始最优化问题和新的对偶最优化问题

由slater条件知，因为原始优化问题的目标函数和不等式约束条件都是凸函数，并且该不等式约束是严格可行的(因为数据是线性可分的), 所以存在W，b，α同时使得原始最优化问题和对偶最优化问题得到最优解

我们通过求解对偶最优化问题（更简单），得到最优解α

求解α是一个二次规划问题，但是使用通用算法太耗时，在实际应用中需要寻求更加高效的算法，例如序列最小优化(Sequential Minimal Optimization, SMO)算法。

SMO算法：《机器学习-第六章-支持向量机》

再通过约束条件得到最优W和最优b。因为
$$
最优W=\sum_{i=1}^{n}最优\alpha_{i}y_{i}X_{i}
$$
显然因为至少存在两个对立的α>0，若不存在，则margin=∞，这样是不行的，再根据KTT条件

KTT：![KTT](D:\Coder\Github\学习笔记\image\KTT.png)

所以至少存在一个j，使得
$$
y_{j}(X^{T}_{j}\hat{W}+\hat{b})-1=0
$$
于是直接推可以知道
$$
\begin{aligned}
\hat{b}=y_{j}-\sum_{i=1}^{n} \hat{\alpha}_{i} y_{i} X_{j}^{T} X_{i}
\end{aligned}
$$
至此，我们得到了整个线性可分样本的SVM的解。求得的超平面为：
$$
\sum_{i=1}^{n} \hat{\alpha}_{i} y_{i} X^{T} X_{i}+\hat{b}=0
$$
则分类的决策函数为
$$
f(X)=\operatorname{sign}\left(\sum_{i=1}^{n} \hat{\alpha}_{i} y_{i} X^{T} X_{i}+\hat{b}\right)
$$
再分析KKT条件里的互补条件，知道，若样本α=0，则该样本点不是支持向量，可有可无。若样本α>0，则该样本位于间隔边界上，是一个支持向量，这就是支持向量机这个名字的由来，模型的有效性和复杂性全部来自于支持向量，其它的样本不起作用

**软间隔**

在前面的讨论中，我们一直假定训练数据是严格线性可分的，即存在一个超平面能完全将两类数据分开。但是现实任务这个假设往往不成立，于是我们引入“软间隔”的概念。即允许少量样本不满足约束

但是为了使不满足的样本点尽可能少，我们需要在优化目标里新增一个惩罚项。如hinge损失：
$$
L_{hinge}(x)=\max(0,1-z)
$$
即若样本点满足约束条件损失就是0, 否则损失就是1-z，应用在原优化目标上就是以下式子：
$$
\min _{W, b} \frac{1}{2}\|W\|^{2}+C \sum_{i=1}^{n} \max \left(0,1-y_{i}\left(X_{i}^{T} W+b\right)\right)
$$
上式表示软间隔依然是一个凸二次规划问题，和之前类似，我们可以通过可以通过拉格朗日乘子法将其转换为对偶问题进行求解。
$$
L(W, b, \xi, \alpha, \beta)=\frac{1}{2}\|W\|^{2}+C \sum_{i=1}^{n} \xi_{i}-\sum_{i=1}^{n} \alpha_{i}\left[y_{i}\left(X_{i}^{T} W+b\right)-1+\xi_{i}\right]-\sum_{i=1}^{n} \beta_{i} \xi_{i}
$$
为了求得对偶问题的解，我们需要先求得![[公式]](https://www.zhihu.com/equation?tex=L%28W%2Cb%2C%5Cxi%2C%5Calpha%2C%5Cbeta%29)对![[公式]](https://www.zhihu.com/equation?tex=W)、![[公式]](https://www.zhihu.com/equation?tex=b)和![[公式]](https://www.zhihu.com/equation?tex=%5Cxi)的极小再求对![[公式]](https://www.zhihu.com/equation?tex=%5Calpha)和![[公式]](https://www.zhihu.com/equation?tex=%5Cbeta)的极大。依然是那个过程
$$
(1) 求 \min _{W, b, \xi} L(W, b, \xi, \alpha, \beta): 将 L(W, b, \xi, \alpha, \beta) 分别对 W 、 b 和 \xi 求偏导并令为0可得\\
\begin{gathered}
W=\sum_{i=1}^{n} \alpha_{i} y_{i} X_{i} \\
\sum_{i=1}^{n} \alpha_{i} y_{i}=0 \\
C=\alpha_{i}+\beta_{i}
\end{gathered}\\
将上面三个式子代入上面总的式子 并进行推导即得\\
\min _{W, b, \xi} L(W, b, \xi, \alpha, \beta)=-\frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i} \alpha_{j} y_{i} y_{j} X_{i}^{T} X_{j}+\sum_{i=1}^{n} \alpha_{i}\\
注意其中的 \beta 被消去了。\\
(2) 求 \min _{W, b, \xi} L(W, b, \xi, \alpha, \beta) 对 \alpha 的极大:
总拉格朗日函数对 \alpha 求极大, 也等价于总拉格朗日函数取负数后对 \alpha 求极小, 即\\
\min _{\alpha} \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i} \alpha_{j} y_{i} y_{j} X_{i}^{T} X_{j}-\sum_{i=1}^{n} \alpha_{i}
$$
至此，我们得到了原始最优化问题和对偶最优化问题，也是一样的，我们求到了最优α之后，就可以求得最优W。再根据KTT条件

可求得整个软间隔SVM的解，即:
$$
\begin{gathered}
\hat{W}=\sum_{i \in S V} \hat{\alpha}_{i} y_{i} X_{i} \\
\hat{b}=y_{j}-\sum_{i \in S V} \hat{\alpha}_{i} y_{i} X_{j}^{T} X_{i}
\end{gathered}
$$
可以看到形式上和上面是一样的，但是此时的支持向量有了更多区分

其中![[公式]](https://www.zhihu.com/equation?tex=j)需满足![[公式]](https://www.zhihu.com/equation?tex=0+%3C+%5Chat%7B%5Calpha%7D_j+%3C+C)。

对于任意样本![[公式]](https://www.zhihu.com/equation?tex=%28X_i%2C+y_i%29)，*若![[公式]](https://www.zhihu.com/equation?tex=%5Calpha_i%3D0)*，此样本点不是支持向量，该样本对模型没有任何的作用；若![[公式]](https://www.zhihu.com/equation?tex=%5Calpha_i%3E0)，此样本是一个支持向量。

若满足![[公式]](https://www.zhihu.com/equation?tex=%5Calpha_i%3E0)，进一步地，若![[公式]](https://www.zhihu.com/equation?tex=0+%3C+%5Calpha_i+%3C+C), 由式![[公式]](https://www.zhihu.com/equation?tex=%283.2.4%29)得![[公式]](https://www.zhihu.com/equation?tex=%5Cbeta_i+%3D+0)，即刚好![[公式]](https://www.zhihu.com/equation?tex=y_i%28X_i%5ETW%2Bb%29+%3D1)，样本恰好在最大间隔边界上；若![[公式]](https://www.zhihu.com/equation?tex=%5Calpha_i+%3D+C)，有![[公式]](https://www.zhihu.com/equation?tex=%5Cbeta_i+%3E+0)，此时若![[公式]](https://www.zhihu.com/equation?tex=%5Cbeta_i+%3C+1)则该样本落在最大间隔内部，若![[公式]](https://www.zhihu.com/equation?tex=%5Cbeta_i+%3E+1)则该样本落在最大间隔内部即被错误分类。

如下图：

<img src="D:\Coder\Github\学习笔记\image\软间隔支持向量.png" alt="软间隔支持向量" style="zoom: 33%;" />

有关于惩罚参数C，我们有相对直观的认识

我们原始的目标函数是：
$$
\min_{W,b,\epsilon} \frac{1}{2}||W||^{2}+C\sum_{i=1}^{n}\epsilon_{i}
$$
与一般问题不同，在这里左边是结构风险，但不是正则化项，右边是经验，作为模型的正则化项

参数![[公式]](https://www.zhihu.com/equation?tex=C)就是用于对二者的折中,即我们一方面要求模型要满足某种性质另一方面又想使模型与训练数据很契合。![[公式]](https://www.zhihu.com/equation?tex=C)越大即对误分类的惩罚越大(要求模型对训练模型更契合)，这可能会存在过拟合；![[公式]](https://www.zhihu.com/equation?tex=C)越小即相对更加看重正则化项，此时可能存在欠拟合。

**核技巧**

前面介绍的都是线性问题，但是我们经常会遇到非线性的问题(例如异或问题)，此时就需要用到核技巧(kernel trick)将线性支持向量机推广到非线性支持向量机。需要注意的是，不仅仅是SVM，很多线性模型都可以用核技巧推广到非线性模型，例如核线性判别分析(KLDA)。

异或问题：

核函数：既然数据在特征空间中呈非线性，那么我们就用映射函数来将数据映射到更加高维的空间上，再那个空间数据是线性可分的

<img src="D:\Coder\Github\学习笔记\image\高维映射.png" alt="高维映射" style="zoom:50%;" />

幸运的是，数学已经证明了，假如原始空间是有限维，那么一定存在一个高维特征空间使样本可分（废话）

具体的，设核函数为K，事实上，只要一个对称函数所对应的核矩阵半正定，那么就能作为核函数使用。但因为一般核函数的推导太过复杂，我们用已有的核函数直接套用，例如多项式核函数、高斯核函数、线性核函数等。这里有一些经验，对文本数据一般用线性核，情况不明一般用高斯核

只需将线性支持向量机中的內积换成核函数即可
$$
首先选取适当的核函数 K(x, z) 和适当的参数 C, 构造最优化问题\\
\begin{array}{ll}
\min _{\alpha} & \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i} \alpha_{j} y_{i} y_{j} K\left(X_{i}, X_{j}\right)-\sum_{i=1}^{n} \alpha_{i} \\
\text { s.t. } & \sum_{i=1}^{n} \alpha_{i} y_{i}=0 \\
& 0 \leq \alpha_{i} \leq C, i=1,2, \ldots, n
\end{array}\\
再利用现成的二次规划问题求解算法或者SMO算法求得最优解 \hat{\alpha} 。\\
选择 \hat{\alpha} 的一个满足 0<\hat{\alpha}_{j}<C 的分量 \hat{\alpha}_{j}, 计算\\
\hat{b}=y_{j}-\sum_{i \in S V} \hat{\alpha}_{i} y_{i} K\left(X_{j}, X_{i}\right)\\
构造决策函数:\\
f(x)=\operatorname{sign}\left(\sum_{i \in S V} \hat{\alpha}_{i} y_{i} K\left(X_{j}, X_{i}\right)+\hat{b}\right)\\
$$
**支持向量回归SVR**

SVR方法产生预测值的方法本质上讲和单个感知机相同，只是在W与b的更新方式上，或者说损失的计算上有所不同，抛去损失计算的话，SVR方法就是用了核技巧的感知机了

一般来说，回归模型通常直接基于模型输出f(x)与真实输出y之间的差别来计算损失，并且通过损失来更新W和b，以达到拟合数据的效果

而SVR假设我们能够容忍最多epsilon的误差，超出这一边界的样本才被计算误差，用以调整拟合平面

于是, SVR 问题可形式化为
$$
\min _{\boldsymbol{w}, b} \frac{1}{2}\|\boldsymbol{w}\|^{2}+C \sum_{i=1}^{m} \ell_{(}\left(f\left(\boldsymbol{x}_{i}\right)-y_{i}\right),
$$
其中C 为正则化常数, ε是**ε-不敏感损失函数**
$$
\ell_{\epsilon}(z)= \begin{cases}0, & \text { if }|z| \leqslant \epsilon \\ |z|-\epsilon, & \text { otherwise }\end{cases}
$$
引入松弛变量 xi，hat(xi), 可有，SVR两侧松弛变量大小可不同
$$
\min _{\boldsymbol{w}, b, \xi_{i} \hat{\xi}_{i}} \frac{1}{2}\|\boldsymbol{w}\|^{2}+C \sum_{i=1}^{m}\left(\xi_{i}+\hat{\xi}_{i}\right)
$$
接下来的过程与上面类似，求导，得到对偶函数，根据KTT条件，得到隔带内样本特点：

只有在隔带内的样本满足α1和α2至少有一个为0，在隔带上的样本α1和α2都为0

SVR的解和b的取值公式为：
$$
f(\boldsymbol{x})=\sum_{i=1}^{m}\left(\hat{\alpha}_{i}-\alpha_{i}\right) \boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{x}+b .\\
b=y_{i}+\epsilon-\sum_{i=1}^{m}\left(\hat{\alpha}_{i}-\alpha_{i}\right) \boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{x} .
$$
**核方法**

若不考虑偏移项b，发现无论SVM还是SVR，学得的模型总能表示成核函数的线性组合

这里涉及到真正的magic，过于硬核，详情在《机器学习-第6章-支持向量机-6.6核方法》

**SVM优缺点**

SVM的优点是：

* 由于SVM是一个凸优化问题，所以求得的解一定是全局最优而不是局部最优。
* 不仅适用于线性线性问题还适用于非线性问题(用核技巧)。
* 拥有高维样本空间的数据也能用SVM，这是因为数据集的复杂度只取决于支持向量而不是数据集的维度，这在某种意义上避免了“维数灾难”。
* 理论基础比较完善，解释性较强

SVM的缺点是：

* 二次规划问题求解将涉及m阶矩阵的计算(m为样本的个数), 因此SVM不适用于超大数据集。(SMO算法可以缓解这个问题)
* 只适用于二分类问题。(SVM的推广SVR也适用于回归问题；可以通过多个SVM的组合来解决多分类问题)

支持向量机的背后是深厚的凸优化与泛函分析知识，对SVM的理解和使用需要多实践才能更好的理解

## K近邻(KNN)

**原理**

通过计算测试点与训练集的距离来对其进行分类或回归，分类用投票原则，回归则求平均值。

![1607436948206](D:/Coder/Github/学习笔记/image/1607436948206.png)

**关键**

* 样本特征都要有可比较的量化
* 样本特征要做归一化处理
* 需要选取合适的距离公式，比如最大边缘近邻法或近邻成分分析法或加权最近邻法
* 需要选取合适的K值，可以交叉验证确定K值

**作用**

适合对有数值量化的样本的分类或回归，特别适合于文本分类。

**优缺点**

**优点**：

* 无需训练，属于惰性学习，易于实现，特别适合处理多分类问题和类别比较弱的样本分类。

**缺点**：

* 比较吃训练集的性能，计算量比较大，但可以KDtree算法来解决。

**一些公式**

欧式距离(其实和曼哈顿距离差不多一样的)
$$
EuclideanDistance(d)=\sqrt{(x_2-x_1)^2+(y_2-y_1)^2}
$$
曼哈顿距离
$$
ManhattanDistance(d)=|x_2-x_1|+|y_2-y_1|
$$

**KDtree的python实现**

## 神经网络(neural network)

[“神经网络”是什么？如何直观理解它的能力极限？它是如何无限逼近真理的？ - YouTube](https://www.youtube.com/watch?v=MU03tMR1-CU)

**公式：**
$$
z(x)=\sum^n_{i=1}w_ix_i
$$

$$
\delta(z)=\frac{1}{1+e^-z}
$$

$$
E_d(w)=\frac{1}{2}\sum_{k\in{outputs}}(y_k-o_k)^2
$$

$$
w_{ji}:=w_{ji}+\delta{w_{ji}}\ \delta{w_{ji}}=\frac{\partial{E_i}}{\partial{w}}
$$

**分类偏差的直观理解**

<img src="D:/Coder/Github/学习笔记/image/分配偏差的直观理解.png" alt="分配偏差的直观理解" style="zoom:50%;" />

**拓展：深入了解激活函数，sigmoid与softmax的和与同**

[softmax是为了解决归一问题凑出来的吗？和最大熵是什么关系？最大熵对机器学习为什么非常重要？_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1cP4y1t7cP?spm_id_from=333.851.dynamic.content.click)

1. 在输出层，softmax就是sigmoid的拓展，一种增强概率版本的sigmoid。其转化思路就是，为了满足转化后输出概率的条件，即

   * 输出大于都大于0，小于1
   * 输出值相加为1

2. 为此，我们将输出做指数、均值处理，在softmax为输出节点的情况下，损失函数为
   $$
   J=-\sum_{k=1}^{m}\left(y_{1}^{(k)} \cdot \log \boldsymbol{p}_{1}^{(k)}+y_{2}^{(k)} \cdot \log \boldsymbol{p}_{2}^{(k)}\right)
   $$

3. 此时我们引出问题，为什么构造归一函数有那么多方法，为什么要以e为底来作为softmax的函数形式呢？

4. 

5. 

# 无监督学习-unsupervised learning

* **无监督学习则是没有明确目的的训练方式，你无法提前知道结果是什么** ，大概结果的也不知道， **无监督学习不需要给数据打标签** ， **几乎无法明确学习效果如何** 

* **无监督学习是一种机器学习的训练方式，它本质上是一个统计手段，在没有标签的数据里可以发现潜在的一些结构的一种训练方式。** 

* ##### 无监督学习的一些运用

  **案例1：发现异常** ：有很多违法行为都需要”洗钱”，这些洗钱行为跟普通用户的行为是不一样的，到底哪里不一样？如果通过人为去分析是一件成本很高很复杂的事情，我们可以通过这些行为的特征对用户进行分类，就更容易找到那些行为异常的用户，然后再深入分析他们的行为到底哪里不一样，是否属于违法洗钱的范畴。即可以快速分类出行为有异常的用户。

  **案例2：用户细分**

  这个对于广告平台很有意义，我们不仅把用户按照性别、年龄、地理位置等维度进行用户细分，还可以通过用户行为对用户进行分类。通过很多维度的用户细分，广告投放可以更有针对性，效果也会更好。即自动将用户分开为一群一群

  **案例3：推荐系统**

  大家都听过”啤酒+尿不湿”的故事，这个故事就是根据用户的购买行为来推荐相关的商品的一个例子。比如大家在淘宝、天猫、京东上逛的时候，总会根据你的浏览行为推荐一些相关的商品，有些商品就是无监督学习通过聚类来推荐出来的。系统会发现一些购买行为相似的用户，推荐这类用户最”喜欢”的商品。 

## K-means

## 最大期望

