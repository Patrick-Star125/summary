算法学习途径：

1. bilibili
2. 油管
3. 知乎
4. 微信
5. 资料书(最花时间)

# 监督学习-supervised learning

*  **监督学习需要有明确的目标，很清楚自己想要什么结果** ，即训练集具有已知明显特征

* ##### 监督学习的2个任务：回归、分类

  回归： **预测连续的、具体的数值** ，例如预测房价，预测 支付宝里的芝麻信用分数 

  分类： **对各种事物分门别类，用于离散型（[什么是离散？](https://baike.baidu.com/item/离散变量/8443404?fr=aladdin)）预测** ，判断‘是否’，例如认识猫、预测离婚

* ##### 主流的监督学习算法

  | 算法                                          | 类型      | 简介                                                         |
  | :-------------------------------------------- | :-------- | :----------------------------------------------------------- |
  | 朴素贝叶斯                                    | 分类      | 贝叶斯分类法是基于贝叶斯定定理的统计学分类方法。它通过预测一个给定的元组属于一个特定类的概率，来进行分类。朴素贝叶斯分类法假定一个属性值在给定类的影响独立于其他属性的 —— 类条件独立性。 |
  | 决策树                                        | 分类      | 决策树是一种简单但广泛使用的分类器，它通过训练数据构建决策树，对未知的数据进行分类。 |
  | [SVM](https://easyai.tech/ai-definition/svm/) | 分类      | 支持向量机把分类问题转化为寻找分类平面的问题，并通过最大化分类边界点距离分类平面的距离来实现分类。 |
  | 逻辑回归                                      | 分类      | 逻辑回归是用于处理因变量为分类变量的回归问题，常见的是二分类或二项分布问题，也可以处理多分类问题，它实际上是属于一种分类方法。 |
  | 线性回归                                      | 回归      | 线性回归是处理回归任务最常用的算法之一。该算法的形式十分简单，它期望使用一个超平面拟合数据集（只有两个变量的时候就是一条直线）。 |
  | 回归树                                        | 回归      | 回归树（决策树的一种）通过将数据集重复分割为不同的分支而实现分层学习，分割的标准是最大化每一次分离的信息增益。这种分支结构让回归树很自然地学习到非线性关系。 |
  | K邻近                                         | 分类+回归 | 通过搜索K个最相似的实例（邻居）的整个训练集并总结那些K个实例的输出变量，对新数据点进行预测。 |
  | Adaboosting                                   | 分类+回归 | [Adaboost](https://easyai.tech/ai-definition/adaboost/)目的就是从训练数据中学习一系列的弱分类器或基本分类器，然后将这些弱分类器组合成一个强分类器。 |
  | 神经网络                                      | 分类+回归 | 它从信息处理角度对人脑神经元网络进行抽象， 建立某种简单模型，按不同的连接方式组成不同的网络。 |

## 线性回归(linear regression)

**原理**

对训练集特征进行拟合，根据不同的**损失函数**，利用不同公式算出合适的回归系数W

**关键**

损失函数的选择

**优缺点**

比较简单，缺点也是比较简单

## 决策树(Decision tree)

不同于逻辑回归，决策树属于**非线性模型**，可以用于分类，也可用于回归，可以认为是if-then规则的集合，是以实例为基础的归纳学习。决策树的分支需要一定的标准，怎么准确的定量选择这些标准是算法性能高低的关键

分类决策树模型是一种描述对实例进行分类的树形结构，决策树由结点和有向边组成，节点有两种类型，内部节点和叶节点，内部节点表示一个特征或属性，叶节点表示一个类

<img src=".\image\决策树的几种表现形式.png" alt="决策树的几种表现形式" style="zoom:50%;" />

决策树表示的是给定特征条件下的条件概率分布，这一条件概率分布定义在特征空间的一个划分（partition）上，将特征空间划分为互不相交的单元（cell）或区域（region）。

**决策树ID3算法**

决策树的前置概念有信息熵、联合熵、条件熵、信息增益

信息熵：用来定量表达一个概率系统不确定程度的方法，越不确定的事物，它的熵就越大。在决策树模型中，我们需要通过所有样本的离散值来计算样本熵，公式为：
$$
H(x)=-\sum_{i=1}^np_ilogp_i
$$
联合熵：联合熵是一种变量之间不确定性的衡量手段，公式为：
$$
H(X)=-\sum_{i=1}^{n} p\left(x_{i}, y_{i}\right) \log p\left(x_{i}, y_{i}\right)
$$
条件熵：有了联合熵就可以得到条件熵的表达式，公式为：
$$
H(X \mid Y)=-\sum_{i=1}^{n} p\left(x_{i}, y_{i}\right) \log p\left(x_{i} \mid y_{i}\right)=\sum_{j=1}^{n} p\left(y_{j}\right) H\left(X \mid y_{i}\right)
$$
在ID3算法中，条件熵用于度量在某个特征已知的条件下，系统还保留的不确定性

信息增益：我们需要用一个量去衡量某种特征对样本来说，信息量最大，那么它就最合适分类，为此我们引入信息增益的概念，公式为：
$$
I(D,A)=H(D)-H(D|A)
$$
最后，算法的具体过程为：

1. 初始化信息增益的阈值ϵ
2. 判断样本是否为同一类输出 Di，如果是则返回单节点树T。标记类别为 Di
3. 判断特征是否为空，如果是则返回单节点树T，标记类别为样本中输出类别D实例数最多的类别
4. 计算A中的各个特征（一共n个）对输出D的信息增益，选择信息增益最大的特征 Ag
5. 如果 Ag 的信息增益小于阈值ϵ，则返回单节点树T，标记类别为样本中输出类别D实例数最多的类别
6. 否则，按特征 Ag 的不同取值 Agi 将对应的样本输出D分成不同的类别Di。每个类别产生一个子节点。对应特征值为 Agi 。返回增加了节点的数T
7. 对于所有的子节点，令D=Di,A=A−{Ag}递归调用2-6步，得到子树Ti并返回

**决策树C4.5算法**

相对于ID3的方法，C4.5算法改进了四个问题，一是不能处理连续特征，第二个就是用信息增益作为标准容易偏向于取值较多的特征。提出了连续特征、特征熵、信息增益率的概念

对于连续特征，C4.5的思路是将连续的特征离散化。比如m个样本的连续特征A有m个，从小到大排列a1,a2,...,am,则C4.5取相邻两样本值的平均数，一共取得m-1个划分点，对于这m-1个点，分别计算以该点作为二元分类点时的信息增益。选择信息增益最大的点作为该连续特征的二元离散分类点

特征熵：特征数越多的特征对应的特征熵越大，公式为
$$
H_{A}(D)=-\sum_{i=1}^{n} \frac{|D i|}{|D|} \log _{2} \frac{|D i|}{|D|}
$$
信息增益率：我们需要将取值较多的特征的信息增益缩小，将取值较少的特征的信息增益相对增大，故引入信息增益率这个概念，公式为：
$$
I_{R}(D, A)=\frac{I(A, D)}{H_{A}(D)}
$$
经证明，在决定连续特征的分界点时采用增益这个指标，而选择属性的时候才使用增益率这个指标能选择出最佳分类特征，

![C4.5](D:\Coder\Github\学习笔记\image\C4.5.png)

对于缺失值处理

**CART决策树**

CART算法解决了C4.5算法的几个问题：

1. 决策树算法非常容易过拟合，因此对于生成的决策树必须要进行剪枝。剪枝的算法思路主要是两种，一种是预剪枝，即在生成决策树的时候就决定是否剪枝。另一个是后剪枝，即先生成决策树，再通过交叉验证来剪枝。CART中主要用的是后剪枝
2. 很多时候，在计算机中二叉树模型会比多叉树运算效率高。CART采用完全二叉树作为，提高了计算效率
3. C4.5只能用于分类，CART能将决策树用于回归
4. C4.5由于使用了熵模型，里面有大量的耗时的对数运算，如果是连续值还有大量的排序运算。CART采用基尼系数来代替信息增益进行特征选择，能够简化模型，而不至于完全丢失熵模型的优点

**回归树的生成**

**分类树的生成**

分类问题中，假设有K类，样本点属于第K类的概率是Pk，则概率分布的基尼知数定义为：
$$
Gini(p)=\sum_{k=1}^{K}p_{k}(1-p_{k})=1-\sum_{k=1}^{K}p_{k}^{2}
$$
对于给定的样本集合D，其基尼指数为：
$$
Gini(D)=1-\sum_{k=1}^{K}(\frac{|C_{k}|}{|D|})^2
$$
在特征A的条件下，集合D的基尼指数定义为：
$$
Gini(D,A)=\frac{|D_1|}{D}Gini(D_1)+\frac{|D_2|}{D}Gini(D_2)
$$
CART生成算法流程为：

输入：训练集D，基尼知数阈值

输出：CART分类决策树T

**CART剪枝算法**

没太看懂，多看看文章

## 朴素贝叶斯(Naive bayes)

## 支持向量机(Support Vector Machine)

## K近邻(KNN)

**原理**

通过计算测试点与训练集的距离来对其进行分类或回归，分类用投票原则，回归则求平均值。

![1607436948206](.\image\1607436948206.png)

**关键**

* 样本特征都要有可比较的量化
* 样本特征要做归一化处理
* 需要选取合适的距离公式，比如最大边缘近邻法或近邻成分分析法或加权最近邻法
* 需要选取合适的K值，可以交叉验证确定K值

**作用**

适合对有数值量化的样本的分类或回归，特别适合于文本分类。

**优缺点**

**优点**：

* 无需训练，属于惰性学习，易于实现，特别适合处理多分类问题和类别比较弱的样本分类。

**缺点**：

* 比较吃训练集的性能，计算量比较大，但可以KDtree算法来解决。

**一些公式**

欧式距离(其实和曼哈顿距离差不多一样的)
$$
EuclideanDistance(d)=\sqrt{(x_2-x_1)^2+(y_2-y_1)^2}
$$
曼哈顿距离
$$
ManhattanDistance(d)=|x_2-x_1|+|y_2-y_1|
$$

**KDtree的python实现**

## 神经网络(neural network)

[“神经网络”是什么？如何直观理解它的能力极限？它是如何无限逼近真理的？ - YouTube](https://www.youtube.com/watch?v=MU03tMR1-CU)

**公式：**
$$
z(x)=\sum^n_{i=1}w_ix_i
$$

$$
\delta(z)=\frac{1}{1+e^-z}
$$

$$
E_d(w)=\frac{1}{2}\sum_{k\in{outputs}}(y_k-o_k)^2
$$

$$
w_{ji}:=w_{ji}+\delta{w_{ji}}\ \delta{w_{ji}}=\frac{\partial{E_i}}{\partial{w}}
$$

**分类偏差的直观理解**

<img src=".\image\分配偏差的直观理解.png" alt="分配偏差的直观理解" style="zoom:50%;" />

# 无监督学习-unsupervised learning

* **无监督学习则是没有明确目的的训练方式，你无法提前知道结果是什么** ，大概结果的也不知道， **无监督学习不需要给数据打标签** ， **几乎无法明确学习效果如何** 

* **无监督学习是一种机器学习的训练方式，它本质上是一个统计手段，在没有标签的数据里可以发现潜在的一些结构的一种训练方式。** 

* ##### 无监督学习的一些运用

  **案例1：发现异常** ：有很多违法行为都需要”洗钱”，这些洗钱行为跟普通用户的行为是不一样的，到底哪里不一样？如果通过人为去分析是一件成本很高很复杂的事情，我们可以通过这些行为的特征对用户进行分类，就更容易找到那些行为异常的用户，然后再深入分析他们的行为到底哪里不一样，是否属于违法洗钱的范畴。即可以快速分类出行为有异常的用户。

  **案例2：用户细分**

  这个对于广告平台很有意义，我们不仅把用户按照性别、年龄、地理位置等维度进行用户细分，还可以通过用户行为对用户进行分类。通过很多维度的用户细分，广告投放可以更有针对性，效果也会更好。即自动将用户分开为一群一群

  **案例3：推荐系统**

  大家都听过”啤酒+尿不湿”的故事，这个故事就是根据用户的购买行为来推荐相关的商品的一个例子。比如大家在淘宝、天猫、京东上逛的时候，总会根据你的浏览行为推荐一些相关的商品，有些商品就是无监督学习通过聚类来推荐出来的。系统会发现一些购买行为相似的用户，推荐这类用户最”喜欢”的商品。 

## K-means

## 最大期望(Expectation Maximization)

# 集成学习-ensemble learning

**集成学习**（**ensemble learning**）通过构建并结合多个学习器来完成学习任务。如下图所示，其工作原理是先产生一组“个体学习器”，再用某种策略将它们结合起来得到最终输出

集成学习通过将多个学习器进行结合，常常可获得比单一学习器显著优越的泛化性能。这对“弱学习器”尤为明显，因此集成学习的很多理论研究都是针对弱学习器进行的。但是实践中处于种种考虑，例如希望使用较少的个体学习器，或是重用关于常见学习器的一些经验等，人们往往会使用比较强的学习器

关于集成学习效果为什么那么好，可以做个简单的分析，对于**二分类**问题，假如基分类器的错误率为ε，则有
$$
P(h_{i}(x)\neq{f(x)})=ε
$$

$$
H(x)=sign(\sum_{i=1}^{T}h_{i}(x))
$$

$$
P(H(x)\neq{f(x)})=\sum^{[T/2]}_{k=0}\tbinom{T}{k}(1-ε)^{k}ε^{T-k}\leq{exp(-\frac{1}{2}T(1-2ε)^{2})}
$$

可知，随着分类器数量T的增大，集成的错误率将知指数下降，最终趋向于0

如何产生并结合“好而不同”的个体学习器，是集成学习研究的核心

集成学习结合多个学习器的效果直观展示

<img src="D:\Coder\Github\学习笔记\image\ensemble learning example 1.png" alt="ensemble learning example 1" style="zoom: 33%;" />

<img src="D:\Coder\Github\学习笔记\image\ensemble learning example 2.png" alt="ensemble learning example 2" style="zoom:33%;" />

### Stacking

Stacking是一种更强大的结合策略，即通过使用一个学习器来进行结合。在stacking算法中，我们把个体学习器叫做初级学习器，用于结合的学习器叫做**次级学习器**或**元学习器**

<img src=".\image\stacking.png" alt="stacking" style="zoom:50%;" />

stacking是一种trick，一种可以用于各种模型之间融合来提升最后predict performance的方法

原理：

### Bagging

Bagging就是通过Bootstrap aggregating来生成众多不同的分类器的一种集成学习方法

Bootstrap：是一种有放回的统计学上的采样方法，因为该方法充分利用了给定的观测信息，不需要模型其他的假设和增加新的观测，并且具有稳健性和效率高的特点0通过重采样（re-sample）避免了Cross-Validation造成的样本减少问题，Bootstrap也可以用于创造数据的随机性，以此来训练同一类型模型的不同分类器。

#### Random Forest

总体上，就是生成大量不同的决策树，如果是分类树，就以voting来产生结果，如果是回归树，则以average来产生结果。（前置概念：决策树，bagging方法，voting、average combiner方法）

优点：1.本身采用bootstrap方法，不需要交叉验证2.精度很高，但是参数却很少3.训练特征是随机的，不需要做特征属性选择

### Boosting

Boosting是一族可以将弱学习器提升为强学习器的算法，这族算法的工作机制类似：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本进行调整，使得先前基小学期做错的训练样本在后续受到更多关注，然后基于调整后的样本分布来训练下一个基学习器；如此反复进行，直至学习器数目达到了事前指定的值T

#### Adaboost

Boosting中比较有名的是Adaboost，接下来我们结合boosting的概念来得出Adaboost算法流程与细节











