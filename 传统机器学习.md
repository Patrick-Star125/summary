# 机器学习

#### 监督学习-supervised learning

*  **监督学习需要有明确的目标，很清楚自己想要什么结果** ，即训练集具有已知明显特征

* ##### 监督学习的2个任务：回归、分类

  回归： **预测连续的、具体的数值** ，例如预测房价，预测 支付宝里的芝麻信用分数 

  分类： **对各种事物分门别类，用于离散型（[什么是离散？](https://baike.baidu.com/item/离散变量/8443404?fr=aladdin)）预测** ，判断‘是否’，例如认识猫、预测离婚

* ##### 主流的监督学习算法

  | 算法                                          | 类型      | 简介                                                         |
  | :-------------------------------------------- | :-------- | :----------------------------------------------------------- |
  | 朴素贝叶斯                                    | 分类      | 贝叶斯分类法是基于贝叶斯定定理的统计学分类方法。它通过预测一个给定的元组属于一个特定类的概率，来进行分类。朴素贝叶斯分类法假定一个属性值在给定类的影响独立于其他属性的 —— 类条件独立性。 |
  | 决策树                                        | 分类      | 决策树是一种简单但广泛使用的分类器，它通过训练数据构建决策树，对未知的数据进行分类。 |
  | [SVM](https://easyai.tech/ai-definition/svm/) | 分类      | 支持向量机把分类问题转化为寻找分类平面的问题，并通过最大化分类边界点距离分类平面的距离来实现分类。 |
  | 逻辑回归                                      | 分类      | 逻辑回归是用于处理因变量为分类变量的回归问题，常见的是二分类或二项分布问题，也可以处理多分类问题，它实际上是属于一种分类方法。 |
  | 线性回归                                      | 回归      | 线性回归是处理回归任务最常用的算法之一。该算法的形式十分简单，它期望使用一个超平面拟合数据集（只有两个变量的时候就是一条直线）。 |
  | 回归树                                        | 回归      | 回归树（决策树的一种）通过将数据集重复分割为不同的分支而实现分层学习，分割的标准是最大化每一次分离的信息增益。这种分支结构让回归树很自然地学习到非线性关系。 |
  | K邻近                                         | 分类+回归 | 通过搜索K个最相似的实例（邻居）的整个训练集并总结那些K个实例的输出变量，对新数据点进行预测。 |
  | Adaboosting                                   | 分类+回归 | [Adaboost](https://easyai.tech/ai-definition/adaboost/)目的就是从训练数据中学习一系列的弱分类器或基本分类器，然后将这些弱分类器组合成一个强分类器。 |
  | 神经网络                                      | 分类+回归 | 它从信息处理角度对人脑神经元网络进行抽象， 建立某种简单模型，按不同的连接方式组成不同的网络。 |

#### 无监督学习-unsupervised learning

*  **无监督学习则是没有明确目的的训练方式，你无法提前知道结果是什么** ，大概结果的也不知道， **无监督学习不需要给数据打标签** ， **几乎无法明确学习效果如何** 

*  **无监督学习是一种机器学习的训练方式，它本质上是一个统计手段，在没有标签的数据里可以发现潜在的一些结构的一种训练方式。** 

* ##### 无监督学习的一些运用

   **案例1：发现异常** ：有很多违法行为都需要”洗钱”，这些洗钱行为跟普通用户的行为是不一样的，到底哪里不一样？如果通过人为去分析是一件成本很高很复杂的事情，我们可以通过这些行为的特征对用户进行分类，就更容易找到那些行为异常的用户，然后再深入分析他们的行为到底哪里不一样，是否属于违法洗钱的范畴。即可以快速分类出行为有异常的用户。

  **案例2：用户细分**

  这个对于广告平台很有意义，我们不仅把用户按照性别、年龄、地理位置等维度进行用户细分，还可以通过用户行为对用户进行分类。通过很多维度的用户细分，广告投放可以更有针对性，效果也会更好。即自动将用户分开为一群一群

  **案例3：推荐系统**

  大家都听过”啤酒+尿不湿”的故事，这个故事就是根据用户的购买行为来推荐相关的商品的一个例子。比如大家在淘宝、天猫、京东上逛的时候，总会根据你的浏览行为推荐一些相关的商品，有些商品就是无监督学习通过聚类来推荐出来的。系统会发现一些购买行为相似的用户，推荐这类用户最”喜欢”的商品。

####  常见的2类算法是：聚类、降维 

*  聚类：简单说就是一种自动分类的方法，在监督学习中，你很清楚每一个分类是什么，但是聚类则不是，你并不清楚聚类后的几个分类每个代表什么意思。 
*  降维：降维看上去很像压缩。这是为了在尽可能保存相关的结构的同时降低数据的复杂度。 

##### 实例算法(看不懂，以后记)

#### 模型(监督学习)

* 符号：用m表示训练样本数量，x表示输入特征，y表示输出变量，用（x，y）表示样本m。h表示假设函数，用θ表示实数参数

### 文本数据处理

### 图像数据处理

### 线性回归(linear regression)

#### 原理

对训练集特征进行拟合，根据不同的**损失函数**，利用不同公式算出合适的回归系数W。

#### 关键

损失函数的选择

#### 优缺点

比较简单，缺点也是比较简单

#### 线性回归python实现

### 决策树(Decision tree)

#### 原理

模拟人决策的过程，将训练集的特征一个个分开，组成一颗具有强泛化能力的分类或回归树

![1607435019873](.\image\1607435019873.png)

##### 可泛化公式

$$
p_i=P(X=x_i)\ 前置
$$

$$
p_{j|i}=P_{Y|X}(y_j|x_i)\ 前置
$$

$$
H(x)=-\sum_{i=1}^np_ilogp_i
$$

$$
H(Y|X)=\sum_{i=1}^np_x(X=x_i)H(Y|X=x_i)=\sum_{i=1}^n(-p_i\sum_{j=1}^mP_{j|i}logp_{j|i})
$$

$$
g(D,A)=H(D)-H(D|A)
$$

$$
g_{ratio}(D,A)=\frac{g(D,A)}{H_A(D)}
$$

$$
p_i=\frac{|D_i|}{|D|}\ and\ Gini(D)=\sum_{i=1}^kp_i(1-p_k)=1-\sum_{i=1}^kp_i^2
$$

$$
Gini(D,A)=\frac{|D_1|}{D}Gini(D_1)+\frac{|D_2|}{D}Gini(D_2)
$$

#### 关键

**分割特征**(怎样分割？以什么凭据分割？)的选择决定了决策树的性能， 这很容易理解，例如我们如果依靠学生的身高预测他是否聪明，显然得不到好的结果。 原则是，尽可能的将同类数据分到同一子叶下。

#### 模型作用

决策树可以依据分割方法、特征类型的不同分为分类树和决策树。

#### 优缺点

##### **优点**:

* 计算复杂度不高：不进行大量计算而训练模型
* 对中间值缺失不敏感：对特征的分割前后相对独立
* 解释性强：甚至可以画出图来
* 决策树的预测准确率比一般方法低，但是可以用集成学习方法组合大量优质决策树

#### 决策树python实现

### 朴素贝叶斯(Naive bayes)

原理

关键

能做什么：能用什么达到什么效果

优缺点及其原理

一些重要公式和图片

#### 朴素贝叶斯python实现

### 支持向量机(Support Vector Machine)

#### **支持向量机python实现**

### K近邻(KNN)

#### 原理

通过计算测试点与训练集的距离来对其进行分类或回归，分类用投票原则，回归则求平均值。

![1607436948206](.\image\1607436948206.png)

#### 关键

* 样本特征都要有可比较的量化
* 样本特征要做归一化处理
* 需要选取合适的距离公式，比如最大边缘近邻法或近邻成分分析法或加权最近邻法
* 需要选取合适的K值，可以交叉验证确定K值

#### 作用

适合对有数值量化的样本的分类或回归，特别适合于文本分类。

#### 优缺点

**优点**：

* 无需训练，属于惰性学习，易于实现，特别适合处理多分类问题和类别比较弱的样本分类。

**缺点**：

* 比较吃训练集的性能，计算量比较大，但可以KDtree算法来解决。

#### 一些公式

欧式距离(其实和曼哈顿距离差不多一样的)
$$
EuclideanDistance(d)=\sqrt{(x_2-x_1)^2+(y_2-y_1)^2}
$$
曼哈顿距离
$$
ManhattanDistance(d)=|x_2-x_1|+|y_2-y_1|
$$


#### KDtree的python实现

### 神经网络(neural network)

#### 基本概念

* 神经元与网络：
* 输入层、隐藏层和输出层：
* 权重、参数：
* 激活项：

#### 反向传播算法：

* 反向传播的概念：
* 权值更新：

#### 神经网络特点：

#### 神经网络python实现：

#### 公式：

$$
z(x)=\sum^n_{i=1}w_ix_i
$$

$$
\delta(z)=\frac{1}{1+e^-z}
$$

$$
E_d(w)=\frac{1}{2}\sum_{k\in{outputs}}(y_k-o_k)^2
$$

$$
w_{ji}:=w_{ji}+\delta{w_{ji}}\ \delta{w_{ji}}=\frac{\partial{E_i}}{\partial{w}}
$$

### 模型与文件：

#### 模型保存与调用：

#### 1.pickle与joblib：

* 原理：通过函数保存对象在文件中，保存对象即指将传入类的**self**当前状态保存在文件中，以便于在另一个文件将该状态赋予另一个对象中

* 保存模型

  >   joblib.dump(object, "modelname.m")  or   pickle.dump(obj, file)

* 调用本地模型

  >  object = joblib.load("modelname.m")   or   pickle.load(file)    

​       tip: file means the file object and the way to open it

#### 源代码文件书写规范：

![屏幕截图 2020-12-02 215908](.\image\屏幕截图 2020-12-02 215908.png)

* 一个model文件中只有类，其它的东西不要写，总操作要在创建的终端文件中完成
* model源文件中有关联的函数写的近一些，并加以充分但不过分的注释

#### 模型测试：

##### 回归:

1.  **MSE**: Mean Squared Error , 参数估计值与参数真值之差平方的期望值 ， MSE的值越小，说明预测模型描述实验数据具有更好的精确度。 
   $$
   MSE=\frac{1}{N}\sum_{t=1}^N(y-y_i)^2
   $$

2.  **MAE** :Mean Absolute Error , 平均绝对误差是绝对误差的平均值平均绝对误差能更好地反映预测值误差的实际情况. 
   $$
   MAE=\frac{1}{N}\sum_{i=1}^N|(y-y_i)|
   $$

##### 分类:







